{"title":"Random forests: a tutorial with forestry data","markdown":{"yaml":{"title":"Random forests: a tutorial with forestry data","author":"Matt Russell","date":"2021-09-27","slug":[],"categories":["Analytics"],"tags":["analytics","Data science","random forests","statistics"]},"headingText":"The Parresol tree biomass data","containsRefs":false,"markdown":"\n\n<center>\n![](bootstrap_rf.png){width=500px}\n</center>\n\n\nRandom forests have quickly become [one of the most popular analytical techniques](https://arbor-analytics.com/post/random-forests-in-a-nutshell/) used in forestry today. Random forests (RF) are a machine learning technique that differ in many ways to traditional prediction models such as regression. Random forests can handle a lot of data, can be applied to classification or regression problems, and rank the relative importance of many variables that are related to a response variable of interest.\n\nI've written about the [theory behind random forests](https://arbor-analytics.com/post/random-forests-in-a-nutshell/). This post will present a tutorial of using random forests in R. \n\n\n\nAs an example, we'll use a data set of 40 slash pine trees from Louisiana USA presented in Parresol's 2001 paper [*Additivity of nonlinear biomass equations*](https://cdnsciencepub.com/doi/10.1139/x00-202). The data are presented in Table 1 of the paper, which is replicated in [this Google Sheet](https://docs.google.com/spreadsheets/d/1TPutUVyZLWr7XopKguT5Nvh9lo1EOG4wvOZ6_lD1F_M/edit?usp=sharing).\n\nWe'll read in the data using the `read_sheet()` function from the **googlesheets4** package. We will also load the **tidyverse** package to use some of its plotting features:\n\n```{r, warning = F, message = F}\nlibrary(tidyverse)\nlibrary(googlesheets4)\n\ntree <- read_sheet(\"https://docs.google.com/spreadsheets/d/1TPutUVyZLWr7XopKguT5Nvh9lo1EOG4wvOZ6_lD1F_M/edit?usp=sharing\")\n```\n\nThe data contain the following variables:\n\n* `TreeID`: Tree observation record,\n* `DBH`: Tree diameter at breast height, cm,\n* `HT`: Tree height, m,\n* `LCL`: Tree live crown length, m,\n* `Age`: Age of the tree, years,\n* `Mass_wood`: Green mass of the wood in the tree, kg,\n* `Mass_bark`: Green mass of the bark in the tree, kg,\n* `Mass_crown`: Green mass of the crown of the tree, kg, and\n* `Mass_tree`: Green mass of all tree components, kg.\n\nOur ultimate interest is in predicting the mass all tree components using common tree measurements such as tree diameter, height, live crown length, and age. Before we start modeling with the data, it is a good practice to first visualize the variables. The `ggpairs()` function from the **GGally** package is a useful tool that visualizes the distribution and correlation between variables:\n\n```{r, warning = F, message = F}\nlibrary(GGally)\n\nggpairs(tree, columns = c(2:5, 9))\n```\n\nYou can see a few variables have strong positive correlations with the mass of the tree (e.g., height and diameter) and some more moderate positive correlations (e.g., age).\n\n## The randomForest R package\n\nR and Python both have numerous packages that implement random forests. In R alone, there are nearly 400 packages with the word \"tree\" or \"forest\" in their name. (Sidebar: This is not ideal if you're a forest analyst of biometrician because [only 31 of them](https://arbor-analytics.com/post/31-r-packages-for-forest-analysts/) are actually about forestry.)\n\nBreiman [wrote about random forests in 2001](https://link.springer.com/article/10.1023/A:1010933404324) and a year later [Liaw and Wiener](https://cogns.northwestern.edu/cbmg/LiawAndWiener2002.pdf) created an R package that implements the technique. To date, the **randomForest** R package remains one of the most popular ones in machine learning. \n \nWe can install and load the **randomForest** package:\n\n```{r, warning = F, message = F}\n# install.packages(\"randomForest\")\nlibrary(randomForest)\n```\n\nWe will use the `randomForest()` function to predict total tree mass using several variables in the **tree** data set. A few other key statements to use in the `randomForest()` function are:\n\n* `keep.forest = T`: This will save the random forest output, which will be helpful in summarizing the results.\n* `importance = TRUE`: This will assess the importance of each of the predictors, essential output in random forests!\n* `mtry = 1`: This tells the function to randomly sample one variable at each split in the random forest. For applications in regression, the default value is the number of predictor variables divided by three (and rounded down). In the modeling, several small samples of the entire data set are taken. Any observations that are not taken are called “out-of-bag” samples.\n* `ntree = 500`. This tells the function to grow 500 trees. Generally, a larger number of trees will produce more stable estimates. However, increasing the number of trees needs to be done with consideration of time and memory issues when dealing with large data sets. \n\nOur response variable in the random forests model is `Mass_tree` and predictors are `DBH`, `HT`, `LCL`, and `Age`.\n\n```{r, warning = F, message = F}\ntree.rf <- randomForest(Mass_tree ~ DBH + HT + LCL + Age,\n                        data = tree,\n                        keep.forest = T,\n                        importance = TRUE, \n                        mtry = 1,\n                        ntree = 500)\ntree.rf\n```\n\nNote the mean of squared residuals and the percent variation explained (analogous to R-squared) provided in the output. (We'll revisit them later.)\n\nAnother way to visualize the out-of-bag error rates of the random forests models is to use the `plot()` function. In this application, although we specified 500 trees, the out-of-bag error generally stabilizes after 100 trees:\n\n```{r}\nplot(tree.rf)\n```\n\nSome of the most helpful output in random forests is the importance of each of the predictor variables. The importance score is calculated by evaluating the regression tree with and without that variable. When evaluating the regression tree, the mean square error (MSE) will go up, down, or stay the same.\n\nIf the percent increase in MSE after removing the variable is large, it indicates an important variable. If the percent increase in MSE after removing the variable is small, it’s less important.\n\nThe `importance()` function prints the importance scores for each variable and the `varImpPlot()` function plots them:\n\n```{r}\nimportance(tree.rf)\nvarImpPlot(tree.rf,type=1)\n```\n\nThe output indicates that `DBH` is the most important variable for predicting `Mass_tree` and age the least important. \n\n## Comparing random forests and regression models\n\nForest analysts are often compare multiple models and determine which one has a better predictive ability. In this case, we can fit a multiple linear regression model to the data and compare to the random forests model.\n\nThe `lm()` function can be used to develop a parametric model for `Mass_tree`:\n\n```{r}\ntree.reg <- lm(Mass_tree ~ DBH + HT + LCL + Age, data = tree)\nsummary(tree.reg)\n```\n\nNote the residual standard error of 82.33 kg and the adjusted R-squared of 0.91. The residual standard error is slightly lower and the R-squared value slightly higher for the multiple regression model compared to the random forest output. In addition, further work may be conducted on the multiple regression model by removing the non-significant variables and refitting the model.\n\nAnother aspect of model evaluation is comparing predictions. Although random forests models are often considered a “black box” method because their results are not easily interpreted, the `predict()` function provides predictions of total tree mass:\n\n```{r}\nMass_pred_rf <- predict(tree.rf, tree, predict.all = F)\nMass_pred_reg <- predict(tree.reg, tree, predict.all = F)\n```\n\nIn an ideal setting we might test our model on an independent data set not used in model fitting. However, we can combine the predicted tree weights from both models to the **tree** data set:\n\n```{r}\ntree2 <- as.data.frame(cbind(tree, Mass_pred_rf, Mass_pred_reg))\n```\n\nNote that some predictions from the linear regression model on the 40 trees provide negative values for predicted total tree mass, an undesirable feature that may need to be addressed before implementing the model:\n\n```{r}\ntree2 %>% \n  summarize(Mass_tree, Mass_pred_rf, Mass_pred_reg)\n```\n\nWe may also be interested in plotting residual values from both model types to compare their performance: \n\n```{r}\np.rf <- ggplot(tree2, (aes(x = Mass_pred_rf, y = Mass_tree - Mass_pred_rf))) +\n  geom_point() + \n  scale_y_continuous(limits = c(-200, 200)) +\n  labs(x = \"Predicted tree mass (kg)\",\n       y = \"Residual (kg)\",\n       subtitle = \"Random forests model\") \n\np.reg <- ggplot(tree2, (aes(x = Mass_pred_reg, y = Mass_tree - Mass_pred_reg))) +\n  geom_point() + \n  scale_y_continuous(limits = c(-200, 200)) +\n  labs(x = \"Predicted tree mass (kg)\",\n       y = \"Residual (kg)\",\n       subtitle = \"Regression model\") \n\nlibrary(patchwork)\n\np.rf + p.reg\n```\n\nWith the heteroscedastic residuals in the models, we'd likely want to explore transforming the data prior to model fitting, or to explore other modeling techniques. \n\n## Summary\n\nRandom forests techniques are flexible and can perform comparably with other regression or classification methods. Random forests can handle all types of data (e.g., categorical, continuous) and are advantageous because they work well with data sets containing a large number of predictor variables. The **randomForest** package has seen a lot of development and can be used to help solve modeling problems in your future forest analytics work. \n\n--\n\n*By Matt Russell. [Email Matt](mailto:matt@arbor-analytics.com) with any questions or comments. Sign up for my [monthly newsletter](https://mailchi.mp/d96897dc0f46/arbor-analytics) for in-depth analysis on data and analytics in the forest products industry.*","srcMarkdownNoYaml":"\n\n<center>\n![](bootstrap_rf.png){width=500px}\n</center>\n\n#  \n# \n\nRandom forests have quickly become [one of the most popular analytical techniques](https://arbor-analytics.com/post/random-forests-in-a-nutshell/) used in forestry today. Random forests (RF) are a machine learning technique that differ in many ways to traditional prediction models such as regression. Random forests can handle a lot of data, can be applied to classification or regression problems, and rank the relative importance of many variables that are related to a response variable of interest.\n\nI've written about the [theory behind random forests](https://arbor-analytics.com/post/random-forests-in-a-nutshell/). This post will present a tutorial of using random forests in R. \n\n\n## The Parresol tree biomass data\n\nAs an example, we'll use a data set of 40 slash pine trees from Louisiana USA presented in Parresol's 2001 paper [*Additivity of nonlinear biomass equations*](https://cdnsciencepub.com/doi/10.1139/x00-202). The data are presented in Table 1 of the paper, which is replicated in [this Google Sheet](https://docs.google.com/spreadsheets/d/1TPutUVyZLWr7XopKguT5Nvh9lo1EOG4wvOZ6_lD1F_M/edit?usp=sharing).\n\nWe'll read in the data using the `read_sheet()` function from the **googlesheets4** package. We will also load the **tidyverse** package to use some of its plotting features:\n\n```{r, warning = F, message = F}\nlibrary(tidyverse)\nlibrary(googlesheets4)\n\ntree <- read_sheet(\"https://docs.google.com/spreadsheets/d/1TPutUVyZLWr7XopKguT5Nvh9lo1EOG4wvOZ6_lD1F_M/edit?usp=sharing\")\n```\n\nThe data contain the following variables:\n\n* `TreeID`: Tree observation record,\n* `DBH`: Tree diameter at breast height, cm,\n* `HT`: Tree height, m,\n* `LCL`: Tree live crown length, m,\n* `Age`: Age of the tree, years,\n* `Mass_wood`: Green mass of the wood in the tree, kg,\n* `Mass_bark`: Green mass of the bark in the tree, kg,\n* `Mass_crown`: Green mass of the crown of the tree, kg, and\n* `Mass_tree`: Green mass of all tree components, kg.\n\nOur ultimate interest is in predicting the mass all tree components using common tree measurements such as tree diameter, height, live crown length, and age. Before we start modeling with the data, it is a good practice to first visualize the variables. The `ggpairs()` function from the **GGally** package is a useful tool that visualizes the distribution and correlation between variables:\n\n```{r, warning = F, message = F}\nlibrary(GGally)\n\nggpairs(tree, columns = c(2:5, 9))\n```\n\nYou can see a few variables have strong positive correlations with the mass of the tree (e.g., height and diameter) and some more moderate positive correlations (e.g., age).\n\n## The randomForest R package\n\nR and Python both have numerous packages that implement random forests. In R alone, there are nearly 400 packages with the word \"tree\" or \"forest\" in their name. (Sidebar: This is not ideal if you're a forest analyst of biometrician because [only 31 of them](https://arbor-analytics.com/post/31-r-packages-for-forest-analysts/) are actually about forestry.)\n\nBreiman [wrote about random forests in 2001](https://link.springer.com/article/10.1023/A:1010933404324) and a year later [Liaw and Wiener](https://cogns.northwestern.edu/cbmg/LiawAndWiener2002.pdf) created an R package that implements the technique. To date, the **randomForest** R package remains one of the most popular ones in machine learning. \n \nWe can install and load the **randomForest** package:\n\n```{r, warning = F, message = F}\n# install.packages(\"randomForest\")\nlibrary(randomForest)\n```\n\nWe will use the `randomForest()` function to predict total tree mass using several variables in the **tree** data set. A few other key statements to use in the `randomForest()` function are:\n\n* `keep.forest = T`: This will save the random forest output, which will be helpful in summarizing the results.\n* `importance = TRUE`: This will assess the importance of each of the predictors, essential output in random forests!\n* `mtry = 1`: This tells the function to randomly sample one variable at each split in the random forest. For applications in regression, the default value is the number of predictor variables divided by three (and rounded down). In the modeling, several small samples of the entire data set are taken. Any observations that are not taken are called “out-of-bag” samples.\n* `ntree = 500`. This tells the function to grow 500 trees. Generally, a larger number of trees will produce more stable estimates. However, increasing the number of trees needs to be done with consideration of time and memory issues when dealing with large data sets. \n\nOur response variable in the random forests model is `Mass_tree` and predictors are `DBH`, `HT`, `LCL`, and `Age`.\n\n```{r, warning = F, message = F}\ntree.rf <- randomForest(Mass_tree ~ DBH + HT + LCL + Age,\n                        data = tree,\n                        keep.forest = T,\n                        importance = TRUE, \n                        mtry = 1,\n                        ntree = 500)\ntree.rf\n```\n\nNote the mean of squared residuals and the percent variation explained (analogous to R-squared) provided in the output. (We'll revisit them later.)\n\nAnother way to visualize the out-of-bag error rates of the random forests models is to use the `plot()` function. In this application, although we specified 500 trees, the out-of-bag error generally stabilizes after 100 trees:\n\n```{r}\nplot(tree.rf)\n```\n\nSome of the most helpful output in random forests is the importance of each of the predictor variables. The importance score is calculated by evaluating the regression tree with and without that variable. When evaluating the regression tree, the mean square error (MSE) will go up, down, or stay the same.\n\nIf the percent increase in MSE after removing the variable is large, it indicates an important variable. If the percent increase in MSE after removing the variable is small, it’s less important.\n\nThe `importance()` function prints the importance scores for each variable and the `varImpPlot()` function plots them:\n\n```{r}\nimportance(tree.rf)\nvarImpPlot(tree.rf,type=1)\n```\n\nThe output indicates that `DBH` is the most important variable for predicting `Mass_tree` and age the least important. \n\n## Comparing random forests and regression models\n\nForest analysts are often compare multiple models and determine which one has a better predictive ability. In this case, we can fit a multiple linear regression model to the data and compare to the random forests model.\n\nThe `lm()` function can be used to develop a parametric model for `Mass_tree`:\n\n```{r}\ntree.reg <- lm(Mass_tree ~ DBH + HT + LCL + Age, data = tree)\nsummary(tree.reg)\n```\n\nNote the residual standard error of 82.33 kg and the adjusted R-squared of 0.91. The residual standard error is slightly lower and the R-squared value slightly higher for the multiple regression model compared to the random forest output. In addition, further work may be conducted on the multiple regression model by removing the non-significant variables and refitting the model.\n\nAnother aspect of model evaluation is comparing predictions. Although random forests models are often considered a “black box” method because their results are not easily interpreted, the `predict()` function provides predictions of total tree mass:\n\n```{r}\nMass_pred_rf <- predict(tree.rf, tree, predict.all = F)\nMass_pred_reg <- predict(tree.reg, tree, predict.all = F)\n```\n\nIn an ideal setting we might test our model on an independent data set not used in model fitting. However, we can combine the predicted tree weights from both models to the **tree** data set:\n\n```{r}\ntree2 <- as.data.frame(cbind(tree, Mass_pred_rf, Mass_pred_reg))\n```\n\nNote that some predictions from the linear regression model on the 40 trees provide negative values for predicted total tree mass, an undesirable feature that may need to be addressed before implementing the model:\n\n```{r}\ntree2 %>% \n  summarize(Mass_tree, Mass_pred_rf, Mass_pred_reg)\n```\n\nWe may also be interested in plotting residual values from both model types to compare their performance: \n\n```{r}\np.rf <- ggplot(tree2, (aes(x = Mass_pred_rf, y = Mass_tree - Mass_pred_rf))) +\n  geom_point() + \n  scale_y_continuous(limits = c(-200, 200)) +\n  labs(x = \"Predicted tree mass (kg)\",\n       y = \"Residual (kg)\",\n       subtitle = \"Random forests model\") \n\np.reg <- ggplot(tree2, (aes(x = Mass_pred_reg, y = Mass_tree - Mass_pred_reg))) +\n  geom_point() + \n  scale_y_continuous(limits = c(-200, 200)) +\n  labs(x = \"Predicted tree mass (kg)\",\n       y = \"Residual (kg)\",\n       subtitle = \"Regression model\") \n\nlibrary(patchwork)\n\np.rf + p.reg\n```\n\nWith the heteroscedastic residuals in the models, we'd likely want to explore transforming the data prior to model fitting, or to explore other modeling techniques. \n\n## Summary\n\nRandom forests techniques are flexible and can perform comparably with other regression or classification methods. Random forests can handle all types of data (e.g., categorical, continuous) and are advantageous because they work well with data sets containing a large number of predictor variables. The **randomForest** package has seen a lot of development and can be used to help solve modeling problems in your future forest analytics work. \n\n--\n\n*By Matt Russell. [Email Matt](mailto:matt@arbor-analytics.com) with any questions or comments. Sign up for my [monthly newsletter](https://mailchi.mp/d96897dc0f46/arbor-analytics) for in-depth analysis on data and analytics in the forest products industry.*"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.553","theme":{"light":"minty","dark":"slate"},"title-block-banner":true,"title":"Random forests: a tutorial with forestry data","author":"Matt Russell","date":"2021-09-27","slug":[],"categories":["Analytics"],"tags":["analytics","Data science","random forests","statistics"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}