{
  "hash": "853f36e58faa6b81723d7add345ffbba",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Linear quantile mixed models: applications in forestry'\ndescription: \"LQMM models provide a flexible structure to account for random effects\"\nauthor: Matt Russell\ndate: '2023-08-21'\nslug: []\ncategories:\n  - Statistics\ntags:\n  - analytics\n  - forest measurements\n  - data science\n  - mixed models\n  - quantile regression\nimage: lqmm_example.png\n---\n\n\n# \n<center>\n\n![*Example output comparing predictions from three models fit with linear quantile mixed models and one fit using ordinary least squares.*](lqmm_example.png){width=500px}\n</center>\n# \n\nMixed models have been used for decades by forest biometricians and statisticians. [They have become popular](https://arbor-analytics.com/post/mixed-models-a-primer/) because forestry and natural resources data are often nested, allowing the analyst to account for spatial and temporal correlation among measurements. Forest plots are often measured within stands, stands are located within ownerships, and a collection of ownerships comprise a landscape.\n\nWith mixed models, fixed effects are considered population-averaged values and are similar to the parameters found in “traditional” regression techniques like ordinary least squares. Random effects can be determined for each parameter, typically for each hierarchical level in a data set.\n\nQuantile regression has also become a widely used technique in forestry. [Quantile regression](https://www.aeaweb.org/articles?id=10.1257/jep.15.4.143) methods allow estimation of response variables for any quantile of the data. While ordinary least squares and other regression techniques fit regression lines through the mean or median (i.e., 50th quantile), quantile regression can perform regression through any quantile of the data.  \n\n## Linear quantile mixed models\n\nAs an extension to quantile regression, [linear quantile mixed models (LQMM)](https://link.springer.com/article/10.1007/s11222-013-9381-9) provide a flexible structure to account for multilevel data through the incorporation of random effects. These LQMM models may be specified with random effects similar to a mixed models framework. \n\nThe general model form for an LQMM model follows, adapted from a simple linear regression model form with a random effect placed on the intercept:\n\n$$Y=\\beta_0+b_i+\\beta_1X+\\epsilon$$\n\nwhere $\\beta_0$ and $\\beta_1$ are fixed effects and $b_i$ is a random effect for subject $i$. The random effect can be thought of as each subject's deviation from the fixed intercept parameter. The $\\beta_0$ and $\\beta_1$ values are fit using LQMM procedures to any desired quantile.\n\n## The lqmm package in R\n\nIn R, [the **lqmm** package](https://rdrr.io/cran/lqmm/man/lqmm.html) fits linear quantile mixed models for hierarchical data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages(\"lqmm\")\n\nlibrary(lqmm)\n```\n:::\n\n\nAs an application, we can fit an LQMM to predict tree height (`HT`) based on its diameter at breast height (`DBH`). Data are from 450 red pine trees collected at the Cloquet Forestry Center in Cloquet, Minnesota in 2014 with `DBH` measured in inches and `HT` measured in feet. The data are found in the *redpine* data set available in the **stats4nr** package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"devtools\")\n\n#devtools::install_github(\"mbrussell/stats4nr\")\n\nlibrary(stats4nr)\n\ndata(redpine)\n```\n:::\n\n\nNow, most applications of tree height-diameter models will employ nonlinear model forms, but for the red pine data here, we can see a general linear trend with the data. Plus, it will make our application of LQMM techniques easy to compare to other linear regression techniques:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nggplot(redpine, aes(x = DBH, y = HT)) +\n  geom_point() +\n  labs(x = \"DBH (inches)\",\n       y = \"Height (feet)\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nWe might believe that specifying the inventory plot as a random effect can improve our model performance. That is, we might believe that the inventory plot in which a tree was measured can reduce the variability in tree height estimates. To fit an LQMM model with a random intercept term in R, we specify the `lqmm()` function. The key arguments are specifying the model form in the `fixed =` statement and the random effect variable with the `group =` statement. The `tau =` statement allows you to specify the quantile you wish to run the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nht.lqmm50 <- lqmm(fixed = HT ~ DBH, \n                random = ~1, \n                group = PlotNum,\n                tau = 0.50,\n                data = redpine)\n\nsummary(ht.lqmm50)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall: lqmm(fixed = HT ~ DBH, random = ~1, group = PlotNum, tau = 0.5, \n    data = redpine)\n\nQuantile 0.5 \n\nFixed effects:\n               Value Std. Error lower bound upper bound  Pr(>|t|)    \n(Intercept) 31.51788    3.83488    23.81139     39.2244 8.863e-11 ***\nDBH          2.76287    0.24128     2.27800      3.2477 1.857e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAIC:\n[1] 3441 (df = 4)\n```\n\n\n:::\n:::\n\n\nWe can also extract the random effects for each inventory plot using the `ranef()` function. We can see that the random effects are centered around zero and range from about -20 to 20: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nranef <- ranef(ht.lqmm50)\n\nggplot(ranef, aes(`(Intercept)`)) +\n  geom_density()  \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nWe can compare the LQMM model output to a simple linear regression fit the the mean value and without any random effect:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nht.slr <- lm(HT~DBH,\n             data = redpine)\nsummary(ht.slr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = HT ~ DBH, data = redpine)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-55.449  -9.250  -0.894   8.878  43.415 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  31.1552     1.8357   16.97   <2e-16 ***\nDBH           3.0493     0.1201   25.39   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.31 on 448 degrees of freedom\nMultiple R-squared:   0.59,\tAdjusted R-squared:  0.5891 \nF-statistic: 644.7 on 1 and 448 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nYou can see that the intercept values are quite similar (31.5179 and 31.1552), however the slope for the LQMM model (2.7629) is lower than that of the simple linear regression (3.0493).\n\nWe can specify multiple quantiles in the `tau =` statement to run multiple regressions at various quantiles of the data. We might seek to do this because trees at higher quantiles may be in better crown positions, i.e., they are dominant or co-dominant trees and might need to estimate site index of the forest stand using these trees. Similarly, trees at lower quantiles may be suppressed or intermediate in crown position and we might wish to implement management strategies that improve their growth. As we fit the LQMM models, note the increases in the slope term associated with `DBH` as the quantiles increase from 0.25 to 0.75:  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nht.lqmm <- lqmm(fixed = HT ~ DBH, \n                random = ~1, \n                group = PlotNum,\n                tau = c(0.25, 0.50, 0.75),\n                data = redpine)\n\nsummary(ht.lqmm)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in errorHandling(OPTIMIZATION$low_loop, \"low\", control$LP_max_iter, : Lower loop did not converge in: lqmm. Try increasing max number of iterations (500) or tolerance (1e-05)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall: lqmm(fixed = HT ~ DBH, random = ~1, group = PlotNum, tau = c(0.25, \n    0.5, 0.75), data = redpine)\n\ntau = 0.25\n\nFixed effects:\n               Value Std. Error lower bound upper bound  Pr(>|t|)    \n(Intercept) 28.26885    3.88807    20.45547     36.0822 2.529e-09 ***\nDBH          2.69508    0.21929     2.25439      3.1358 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ntau = 0.5\n\nFixed effects:\n               Value Std. Error lower bound upper bound  Pr(>|t|)    \n(Intercept) 31.51788    3.77772    23.92626     39.1095 5.736e-11 ***\nDBH          2.76287    0.21189     2.33706      3.1887 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ntau = 0.75\n\nFixed effects:\n               Value Std. Error lower bound upper bound  Pr(>|t|)    \n(Intercept) 32.15125    3.78961    24.53574     39.7668 3.509e-11 ***\nDBH          3.19633    0.25434     2.68522      3.7074 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAIC:\n[1] 3483 (df = 4) 3441 (df = 4) 3473 (df = 4)\n```\n\n\n:::\n:::\n\n\nLet's make a small data set that contains the predictions of these models, stored in the **tree** data set. Then, we'll visualize the output along with the ordinary least squares line: \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree <- tibble(DBH = c(seq(0,30,1))) %>% \n  mutate(`Ordinary least squares` = 31.1552 + 3.0493*DBH,\n         `LQMM, quantile = 0.25` = 28.26885 + 2.69508*DBH,\n         `LQMM, quantile = 0.50` = 31.51788 + 2.76287*DBH,\n         `LQMM, quantile = 0.75` = 32.15125 + 3.19633*DBH) %>% \n  pivot_longer(`Ordinary least squares`:`LQMM, quantile = 0.75`,\n               names_to = \"Model\",\n               values_to = \"HT\")\n\nggplot(tree, aes(DBH, HT, col = Model)) +\n  geom_line(linewidth = 1.25) +\n  labs(x = \"DBH (inches)\",\n       y = \"Height (feet)\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nOne thing I've noticed in fitting LQMM models is that the standard errors for parameter estimates tend to always be higher for compared to ordinary least squares models. They also tend to be high compared to models fit with linear mixed models through functions available in packages like **lme4**. This is likely due to the quantile regression techniques. \n\n## Conclusion\nLinear quantile mixed models are robust and combine the strengths of both mixed models and quantile regression techniques. These models have a number of potential applications with forestry and natural resources data where data are often nested and we're not always concerned with what's happening through the mean value of the data, but instead at some other quantile. There is also an R package that fits [quantile regressions for nonlinear mixed-effects models](https://cran.r-project.org/web/packages/qrNLMM/index.html), providing more tools to estimate trends through any quantile with data found in a variety of shapes and sizes.\n\n--\n\n*By Matt Russell. [Email Matt](mailto:matt@arbor-analytics.com) with any questions or comments.*",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}