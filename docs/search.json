[
  {
    "objectID": "post.html",
    "href": "post.html",
    "title": "Insights",
    "section": "",
    "text": "Arbor Custom Analytics website now runs on Quarto\n\n\n\nBehind the curtain\n\n\n\nThe Arbor Custom Analytics website now runs on Quarto, an open source publishing system for sharing information.\n\n\n\nMatt Russell\n\n\nMay 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA primer on carbon sequestration rates in US forests\n\n\n\nForest carbon\n\n\n\nHere’s a primer on forest carbon sequestration values and how much carbon US forests grow on an annual basis.\n\n\n\nMatt Russell\n\n\nApr 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecapping the Russell 2024 maple syrup season\n\n\n\nFor fun\n\n\n\nThe data behind our amateur maple syrup operations.\n\n\n\nMatt Russell\n\n\nApr 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPreliminary thoughts on using the new National Scale Volume and Biomass Estimators\n\n\n\nForest carbon\n\n\nNSVB\n\n\n\nA few initial thoughts on using the new NSVB equations.\n\n\n\nMatt Russell\n\n\nFeb 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA quick and easy way to estimate forestland area change across US states\n\n\n\nForest analytics\n\n\nFIA\n\n\n\nHow to estimate forestland area change.\n\n\n\nMatt Russell\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAre winter temperatures correlated with stumpage prices in Maine?\n\n\n\nForest products\n\n\nClimate\n\n\n\nLooking at the correlation between stumpage prices and Maine winters.\n\n\n\nMatt Russell\n\n\nJan 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew web app shows pandemic assistance to forest products industry by state\n\n\n\nForest products\n\n\nCovid\n\n\n\nForest products companies and their use of PPP loans and PATHH payments.\n\n\n\nMatt Russell\n\n\nJan 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMost popular blog posts and podcasts in 2023\n\n\n\nBehind the curtain\n\n\n\nA rundown of the most popular content from this year.\n\n\n\nMatt Russell\n\n\nDec 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTree measurements on the Rockefeller Center Christmas Tree\n\n\n\nData visualization\n\n\n\nData, measurements, and fun facts about the Rockefeller Center Christmas Tree.\n\n\n\nMatt Russell\n\n\nDec 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA list of R packages for forestry applications\n\n\n\nAnalytics\n\n\n\nA handy list of packages for R users in the forestry community.\n\n\n\nMatt Russell\n\n\nNov 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA handy R function for getting ecological division from FIA data\n\n\n\n\n\nSome functions from the R’s stringr package to obtain ecological division.\n\n\n\nMatt Russell\n\n\nNov 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow much does adding previous diameter and height growth change FVS predictions?\n\n\n\nForest growth and yield\n\n\n\nComparing non-calibrated out-of-the-box FVS predictions with a calibrated version.\n\n\n\nMatt Russell\n\n\nOct 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing R and Python to get forest resource data through the EVALIDator API\n\n\n\nAnalytics\n\n\n\nNew ways to access FIA estimates through R and Python.\n\n\n\nMatt Russell\n\n\nOct 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStratifying the number of plots to measure in a forest carbon inventory\n\n\n\nSampling\n\n\n\nHow to allocate field plots to measure forest carbon under a stratified random sampling framework.\n\n\n\nMatt Russell\n\n\nSep 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecent updates to tidyverse functions\n\n\n\nAnalytics\n\n\n\nA few key updates from Teaching the tidyverse in 2023\n\n\n\nMatt Russell\n\n\nSep 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing climate adaptive silviculture scenarios in the Forest Vegetation Simulator\n\n\n\nForest growth and yield\n\n\n\nA tutorial on adaptive silviculture treatments in FVS.\n\n\n\nMatt Russell\n\n\nAug 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear quantile mixed models: applications in forestry\n\n\n\nStatistics\n\n\n\nLQMM models provide a flexible structure to account for random effects\n\n\n\nMatt Russell\n\n\nAug 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s okay to look at the solutions for statistics and data science exercises\n\n\n\nStatistics\n\n\n\nI had mixed feelings about making the solutions for exercises available to learners.\n\n\n\nMatt Russell\n\n\nJul 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaking R graphs more accessible using BrailleR\n\n\n\nAnalytics\n\n\nData visualization\n\n\nAccessibility\n\n\n\nAn R package to make graphs and figures more accessible\n\n\n\nMatt Russell\n\n\nJun 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantifying the shade tolerance of climate-adapted tree species in the Northeast\n\n\n\nForest measurements\n\n\n\nA look at the shade tolerance of tree species in the northeastern US that are projected to see changes in suitable habitat.\n\n\n\nMatt Russell\n\n\nMay 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLorey’s height: the remote sensing way to estimate tree height\n\n\n\nForest measurements\n\n\n\nLorey’s height is a measure of average stand height that is weighted by basal area.\n\n\n\nMatt Russell\n\n\nApr 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBootstrapped resampling to model tree biomass\n\n\n\nStatistics\n\n\n\nBootstrapping works well with messy data and with small samples.\n\n\n\nMatt Russell\n\n\nApr 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHow much of Maine’s forests are enrolled in carbon programs?\n\n\n\nCarbon\n\n\n\n\n\n\n\nMatt Russell\n\n\nMar 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nImputing missing tree heights from a forest inventory\n\n\n\nAnalytics\n\n\n\n\n\n\n\nMatt Russell\n\n\nFeb 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating carbon in standing dead trees\n\n\n\nForest carbon\n\n\n\n\n\n\n\nMatt Russell\n\n\nFeb 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nForestland on the blockchain: a new prospect for private forest owners\n\n\n\nForestry\n\n\n\n\n\n\n\nMatt Russell\n\n\nJan 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nStumpage prices for sawlogs in Maine, 2019-2020\n\n\n\nForest products\n\n\n\n\n\n\n\nMatt Russell\n\n\nDec 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nForest carbon cheat sheet: updated for 2022\n\n\n\nCarbon\n\n\n\n\n\n\n\nMatt Russell\n\n\nDec 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nDoing statistics in the tidyverse: exploring the infer package\n\n\n\nStatistics\n\n\n\n\n\n\n\nMatt Russell\n\n\nDec 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nNew updates for tools that query Forest Inventory and Analysis data\n\n\n\nAnalytics\n\n\n\n\n\n\n\nMatt Russell\n\n\nNov 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHow to spot poor data analyses\n\n\n\nAnalytics\n\n\n\n\n\n\n\nMatt Russell\n\n\nOct 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nP-ing in the woods: p-values in forest science\n\n\n\nStatistics\n\n\n\n\n\n\n\nMatt Russell\n\n\nOct 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHow many trees make a mass timber building?\n\n\n\nPrediction\n\n\n\n\n\n\n\nMatt Russell\n\n\nSep 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nComparing logging trucker wages to other industries\n\n\n\nLogging\n\n\n\n\n\n\n\nMatt Russell\n\n\nAug 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nFVS and CBM-CFS3: Comparing two forest simulation models\n\n\n\nForest measurements\n\n\n\n\n\n\n\nMatt Russell\n\n\nAug 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nSimple volume to weight conversion for US tree species\n\n\n\nForest measurements\n\n\n\n\n\n\n\nMatt Russell\n\n\nJul 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nNew book: Statistics in Natural Resources: Applications with R\n\n\n\nStatistics\n\n\n\n\n\n\n\nMatt Russell\n\n\nJul 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nA forest carbon data dashboard for Minnesota\n\n\n\nCarbon\n\n\n\n\n\n\n\nMatt Russell\n\n\nJul 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nKnown unknowns, unknown unknowns\n\n\n\nProfessional development\n\n\n\n\n\n\n\nMatt Russell\n\n\nApr 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nCarbon credits: quantity versus quality\n\n\n\nCarbon\n\n\n\n\n\n\n\nMatt Russell\n\n\nApr 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nWhat does the price of carbon have to be for landowners to enroll in carbon markets?\n\n\n\nCarbon\n\n\n\n\n\n\n\nMatt Russell\n\n\nFeb 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nForestry is a STEM discipline\n\n\n\nForestry\n\n\n\n\n\n\n\nMatt Russell\n\n\nFeb 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMost read articles in 2021\n\n\n\nBehind the curtain\n\n\n\n\n\n\n\nMatt Russell\n\n\nDec 17, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nCO2 offsets and trips to the family cabin\n\n\n\nCarbon\n\n\n\n\n\n\n\nMatt Russell\n\n\nNov 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nH-2B workers: essential to the forest products industry\n\n\n\nForestry\n\n\n\n\n\n\n\nMatt Russell\n\n\nNov 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nRandom forests: a tutorial with forestry data\n\n\n\nAnalytics\n\n\n\n\n\n\n\nMatt Russell\n\n\nSep 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nThe importance of citizen data scientists in forest analytics\n\n\n\nAnalytics\n\n\n\n\n\n\n\nMatt Russell\n\n\nSep 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nIf 50% of a tree’s biomass is carbon, what’s the other half?\n\n\n\nCarbon\n\n\n\n\n\n\n\nMatt Russell\n\n\nAug 22, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nNew eBook: Forest Carbon by the Numbers\n\n\n\nCarbon\n\n\n\n\n\n\n\nMatt Russell\n\n\nAug 12, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nForest carbon: key definitions and numbers in perspective\n\n\n\nCarbon\n\n\n\n\n\n\n\nMatt Russell\n\n\nAug 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nPeer review: Lessons learned from reviewing 100 forest science manuscripts\n\n\n\nBehind the curtain\n\n\n\n\n\n\n\nMatt Russell\n\n\nJul 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nForest carbon cheat sheet: updated for 2021\n\n\n\nCarbon\n\n\n\n\n\n\n\nMatt Russell\n\n\nJun 12, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nHarvested wood products: a growing contributor to US carbon storage\n\n\n\nCarbon\n\n\n\n\n\n\n\nMatt Russell\n\n\nMay 16, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the forest products industry using NAICS codes\n\n\n\nAnalytics\n\n\n\n\n\n\n\nMatt Russell\n\n\nMay 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nFive forest carbon markets for small landowners\n\n\n\nCarbon\n\n\n\n\n\n\n\nMatt Russell\n\n\nMar 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nPrivate forest landowner demographics compared to the US population\n\n\n\nForestry\n\n\n\n\n\n\n\nMatt Russell\n\n\nFeb 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nA #ForestProud moment after listening to the MeatEater podcast\n\n\n\nForestry\n\n\n\n\n\n\n\nMatt Russell\n\n\nFeb 12, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nForest carbon: a reading list for beginners\n\n\n\nCarbon\n\n\n\n\n\n\n\nMatt Russell\n\n\nFeb 6, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "post/2024-04-24-a-primer-on-carbon-sequestration-rates-in-us-forests/index.html",
    "href": "post/2024-04-24-a-primer-on-carbon-sequestration-rates-in-us-forests/index.html",
    "title": "A primer on carbon sequestration rates in US forests",
    "section": "",
    "text": "If you work in forests, you know how important it is to understand different growth rates. Whether the stand is young, old, managed or not, forests will grow at different rates depending on a variety of conditions.\nHistorically, foresters have talked about growth rates in terms of volume. About 10 years ago I met with a Minnesota forester in a recently cut aspen stand. I vividly remember him telling me that “You need to be getting half a cord per acre per year to have a stand worth selling at 45 years.” Given most aspen sales in Minnesota use clearcutting as a practice (or something close to it) and sell around 20 cords per acre at harvest, his math made sense to me.\nI still often think about forests in terms of volume and cords. Most of us have stacked a cord of firewood or at least can envision what a cord of wood looks like.\nBut increasingly, foresters are being asked to talk about forests in terms of the carbon they store and sequester. Part of the issue with this is the confusion around units associated with forest carbon. But another source of the confusion is that foresters have had few opportunities to learn about the different ways we can quantify forest carbon. This includes talking about growth in terms of carbon sequestration, net change, or uptake, a few different definitions we use to quantify forest carbon.\nFor starters, I’ll discuss the growth of carbon in terms of metric units (e.g., metric tonnes) because that’s the common unit used throughout the carbon market discussion in the US. For reference, one metric tonne is equivalent to 1.10 US tons. I’ll also talk about carbon in terms of \\(CO_2\\) equivalents, where one unit of carbon equals 3.6667 units of \\(CO_2\\) equivalents. Finally, and not to confuse you more, I’ll discuss the growth values on a per acre basis. Again, this is mainly for ease of comparison to programs and reported numbers that discuss carbon in the US. For example, California’s recent auction in February 2024 had a settlement price of $41.76 for 1 metric tonne of \\(CO_2\\) equivalent on a per-acre basis.\nIn the USDA’s 2021 report on greenhouse gas emissions and removals, authors report that “forest uptake averaged 0.6 metric tonnes of carbon per hectare per year, with live vegetation accounting for more than 83 percent of the uptake.” Converting to our units of interest, this equates converts to 0.24 metric tonnes C/ac/yr, or 0.89 tonnes CO2-eq/ac/yr.\nA great publication by Hoover and Smith in 2021 provides similar numbers like these by region and forest type. Their analysis calculated average annual change in aboveground live trees, with average values in the negative for the Rocky Mountains (there’s been a lot of forest disturbances occurring there). Values ranged as high as 2.58 tonnes CO2-eq/ac/yr in the western region of the Pacific Northwest (one of the most productive regions in North America).\nHere is a table showing the average sequestration values presented in Hoover and Smith 2021 (their Table 1):\n\n\n\n\nAverage annual change in aboveground live trees across forested regions in the US, from Hoover and Smith 2021.\n\n\nRegion\nAverage annual change (tonnes CO2/ac/yr)\n\n\n\n\nNortheast\n0.82\n\n\nNorthern Lake States\n0.59\n\n\nSouth Central\n1.34\n\n\nSoutheast\n1.42\n\n\nCentral States\n0.56\n\n\nGreat Plains\n0.12\n\n\nRocky Mountain, North\n-0.10\n\n\nRocky Mountain, South\n-0.27\n\n\nPacific Northwest, East\n0.67\n\n\nPacific Northwest, West\n2.58\n\n\nPacific Southwest\n0.86\n\n\n\n\n\n\n\n\nHopefully these simple values can provide more perspective when conveying forest carbon information to others. I encourage you to look at the publications below to see more detailed carbon sequestration values by forest type and stand age.\n\nReferences\nHoover, C.M., Smith, J.E. 2021. Current aboveground live tree carbon stocks and annual net change in forests of conterminous United States. Carbon Balance and Management 16, 17.\nHoover, C.M., Smith, J.E. 2023. Aboveground live tree carbon stock and change in forests of conterminous United States: influence of stand age. Carbon Balance Manage 18, 7.\n–\nBy Matt Russell. For more, subscribe to my monthly email newsletter to stay ahead on data and analytics trends in the forest products industry."
  },
  {
    "objectID": "post/2024-02-26-preliminary-thoughts-on-using-the-new-national-scale-volume-and-biomass-estimators/index.html#get-to-know-your-neighborhood-ecological-division.",
    "href": "post/2024-02-26-preliminary-thoughts-on-using-the-new-national-scale-volume-and-biomass-estimators/index.html#get-to-know-your-neighborhood-ecological-division.",
    "title": "Preliminary thoughts on using the new National Scale Volume and Biomass Estimators",
    "section": "1. Get to know your neighborhood ecological division.",
    "text": "1. Get to know your neighborhood ecological division.\nOne of the primary differences in the NSVB models is the use of ecological provinces and divisions. I may have learned about ecological provinces and divisions in my forestry schooling, but I don’t use them regularly. Species within each ecological division (provided there were enough observations in the modeling data set) have their own set of model parameters that are used in the framework. As an example, there are 10 different volume equations for Douglas-fir dependending on which ecological division the forest is located in.\nThese divisions are outlined in the US National Hierarchical Framework of Ecological Units, described in detail in a map produced by Cleland and others in 2007. Within divisions there are ecological provinces. Within those provinces there are ecological sections and subsections. At various stages in the NSVB modeling framework, the division or province will need to be known to apply the appropriate set of coefficients.\nI wrote about ecological divisions a few months ago because they’re an important part of the new modeling framework. For projects that span multiple divisions and provinces, users may need to apply the Cleland map to see where their forests lie. Ecological divisions don’t follow boundaries like state and county lines, so analysts may need to incorporate these into their own workflow."
  },
  {
    "objectID": "post/2024-02-26-preliminary-thoughts-on-using-the-new-national-scale-volume-and-biomass-estimators/index.html#be-cautious-when-dealing-with-species-codes.",
    "href": "post/2024-02-26-preliminary-thoughts-on-using-the-new-national-scale-volume-and-biomass-estimators/index.html#be-cautious-when-dealing-with-species-codes.",
    "title": "Preliminary thoughts on using the new National Scale Volume and Biomass Estimators",
    "section": "2. Be cautious when dealing with species codes.",
    "text": "2. Be cautious when dealing with species codes.\nIn addition to the multiple equations for a species depending on ecological division, species are handled delicately in the NSVB framework. Species-level coefficients may also be provided across all ecological divisions or may be grouped into species groups for estimation. These species groups follow ones presented in Jenkins et al. 2003. Some of the grouped equations use a mixed model form with a random effect, providing “quasi” species-level estimates.\nIn total, there should be an equation to use for every species in your tree list depending on where the tree is located. Keeping track of which equation to use can be cumbersome, but it’s best to closely follow the coefficients listed in the supplemental data tables provided in the report."
  },
  {
    "objectID": "post/2024-02-26-preliminary-thoughts-on-using-the-new-national-scale-volume-and-biomass-estimators/index.html#there-are-only-four-model-forms-which-are-relatively-easy-to-work-with.",
    "href": "post/2024-02-26-preliminary-thoughts-on-using-the-new-national-scale-volume-and-biomass-estimators/index.html#there-are-only-four-model-forms-which-are-relatively-easy-to-work-with.",
    "title": "Preliminary thoughts on using the new National Scale Volume and Biomass Estimators",
    "section": "3. There are only four model forms, which are (relatively) easy to work with.",
    "text": "3. There are only four model forms, which are (relatively) easy to work with.\nThe new framework provides four different model forms, with the one that fit the data best ultimately chosen as the one to represent that species. These models depend on a tree’s diameter and height. An equation to predict a tree’s height along its stem at the merchantable diameter limit is a bit tricky, but a required value to determine merchantable volume.\nUsing relatively few model forms is parsimonious and makes the analysis straightforward. Users of the former CRM method and the abundant volume equation forms that differ by region and species might be relieved."
  },
  {
    "objectID": "post/2024-02-26-preliminary-thoughts-on-using-the-new-national-scale-volume-and-biomass-estimators/index.html#models-rely-on-heightactual-height-and-cull-estimates.",
    "href": "post/2024-02-26-preliminary-thoughts-on-using-the-new-national-scale-volume-and-biomass-estimators/index.html#models-rely-on-heightactual-height-and-cull-estimates.",
    "title": "Preliminary thoughts on using the new National Scale Volume and Biomass Estimators",
    "section": "4. Models rely on height/actual height and cull estimates.",
    "text": "4. Models rely on height/actual height and cull estimates.\nThe NSVB models require estimates of actual height and cull percentage on individual trees. These variables were also required in the CRM method, but it’s worth mentioning here because these values are not always collected in forest inventories.\nActual height measures the height of the tree from ground level to the highest remaining portion of the tree. This differs from the height variable used in the FIA program which assumes a tree has all of its tops and limbs intact. Cull refers to the proportion of a live or dead tree that is rotten or missing.\nNot having a measurement of height assuming an intact stem or an visual estimate of cull can lead to likely overestimations of volume, biomass, and carbon. Users that require an accurate estimate of carbon in standing dead trees or are working in recently disturbed forests should take note, as these conditions contain trees with broken tops.\nHealthy trees will likely have an actual height equal to their true height, and users may be able to get around this by assuming the two values are equal. If the proportion of a tree that is in cull was not collected in an inventory, users could make some rough approximations if the tree is designated as unacceptable growing stock or other defects are noted in the tree. Or, use any other historical data your organization might have that allows you to best determine these attributes.\n–\nBy Matt Russell. For more, subscribe to my monthly email newsletter to stay ahead on data and analytics trends in the forest products industry."
  },
  {
    "objectID": "post/2024-02-16-a-quick-and-easy-way-to-estimate-forestland-area-change-across-us-states/index.html#obtaining-forestland-area-change-estimates",
    "href": "post/2024-02-16-a-quick-and-easy-way-to-estimate-forestland-area-change-across-us-states/index.html#obtaining-forestland-area-change-estimates",
    "title": "A quick and easy way to estimate forestland area change across US states",
    "section": "Obtaining forestland area change estimates",
    "text": "Obtaining forestland area change estimates\nThe EVALIDator program can be used to access a number of forest attributes. Importantly, it can be used to acquire estimates of change in forest conditions. It’s important to note that the amount and length of the data will vary depending on your region and state. Most states have data collected since the late 1990’s or early 2000’s when FIA implemented its annual inventory framework. This provides around 20 years of quality data to estimate changing conditions. Many states also have older “periodic” inventories where data were collected prior to this, but these data were not collected using the same nationally consistent framework that is used by FIA today.\nFor our example, we are interested in estimating how much forestland area has changed in the state of Minnesota over the last 10 years. Here are the steps to obtain those values:\n\nOpen the EVALIDator API at https://apps.fs.usda.gov/fiadb-api/evalidator.\nOn the first page, select State(s) retrieval as the Retrieval Type. Select Forest land from the land basis drop-down list. Select Area from the numerator drop-down list and select No denominator – just produce estimates from the ratio estimate drop-down list. Click Continue.\nFrom the drop-down list, select 2 – Area of forestland, in acres. Be sure that the radio buttons Use FIA definition of forest land and Limit retrieval to only most recent inventories are selected. Click Continue.\nScroll down in the list of available evaluations and select MINNESOTA as the state. Note that these estimates will use FIA data collected between 2017 and 2021. Click Continue.\nLeave the Page variable section drop-down items selected to None and Current. For the row variable, scroll down to Forest type group and leave the row temporal basis set to Current. For the column variable, scroll down to Ownership group - major and leave the column temporal basis set to Current. Click Open estimates in new window.\n\nIn the output, a Total row appears as the first column and first row. In total, current estimates show that 17.67 million acres of forestland exist in the state, measured from 2017 through 2021. More forestland area is found in public forests and the aspen/birch forest type group contains the largest acreage (Fig. 1)."
  },
  {
    "objectID": "post/2024-02-16-a-quick-and-easy-way-to-estimate-forestland-area-change-across-us-states/index.html#conclusion",
    "href": "post/2024-02-16-a-quick-and-easy-way-to-estimate-forestland-area-change-across-us-states/index.html#conclusion",
    "title": "A quick and easy way to estimate forestland area change across US states",
    "section": "Conclusion",
    "text": "Conclusion\nIn addition to understanding current forest conditions, the EVALIDator web tool can also provide estimates of changes to forests such as forestland area change. While there remains uncertainty about the information required by companies in regulations outlined in the EUDR and SFI’s Fiber Sourcing Standard, you and your organization can be prepared knowing how to access FIA data to show that wood does not originate from areas that have been deforested or seen significant forestland area change.\n–\nBy Matt Russell. For more, subscribe to my monthly email newsletter to stay ahead on data and analytics trends in the forest products industry.\nThis was previously published as a Technical Review for the Forest Resources Association."
  },
  {
    "objectID": "post/2024-01-13-new-web-app-shows-pandemic-assistance-to-forest-products-industry-by-state/index.html",
    "href": "post/2024-01-13-new-web-app-shows-pandemic-assistance-to-forest-products-industry-by-state/index.html",
    "title": "New web app shows pandemic assistance to forest products industry by state",
    "section": "",
    "text": "Screenshot of the forest products and pandemic funding application.\n\n\n\n\n\n\nForest products companies used the Paycheck Protection Program (PPP) and Pandemic Assistance for Timber Harvesters and Haulers (PATHH) programs to keep nearly 500,000 workers in the US forest industry on payroll throughout the COVID-19 pandemic. A new web application is now available that summarizes these pandemic assistance programs and funding provided to each individual state.\nYou can access the web app here.\nThe app uses data presented in the paper A summary of COVID-19 pandemic assistance to US forest products companies, published in the Forest Products Journal in late 2022. Values include:\n\nNumber of loans/payments offered to forest products companies,\nAverage loan/payment amounts,\nTotal value of loans/payment provided to state,\nAverage number of forest products jobs supported per loan,\nTotal number of forest products jobs supported in state,\nAverage gross revenue lost in 2020 per business, and\nTotal gross revenue lost by state in 2020\n\nData for the PPP and PATHH programs are presented for each US state plus the District of Columbia and Puerto Rico.\n–\nBy Matt Russell. For more, subscribe to my monthly email newsletter to stay ahead on data and analytics trends in the forest products industry."
  },
  {
    "objectID": "post/2023-12-17-rockefeller-tree/index.html",
    "href": "post/2023-12-17-rockefeller-tree/index.html",
    "title": "Tree measurements on the Rockefeller Center Christmas Tree",
    "section": "",
    "text": "The Rockefeller Center Christmas Tree in 2022."
  },
  {
    "objectID": "post/2023-12-17-rockefeller-tree/index.html#species-and-tree-height",
    "href": "post/2023-12-17-rockefeller-tree/index.html#species-and-tree-height",
    "title": "Tree measurements on the Rockefeller Center Christmas Tree",
    "section": "Species and tree height",
    "text": "Species and tree height\nSince at least 1982, the species chosen has always been a Norway spruce, a large and fast-growing conifer known for its drooping branches. Prior to that, white spruce and balsam fir were chosen more than once for the tree:\n\n\n\n\n\n\n\n\n\nFor the 92 trees which there are data, the average height of all of the Rockefeller Christmas trees was 72.3 feet. The Rockefeller tree has generally gotten taller through the years. Prior to 2000 there were several trees less than 70 feet tall. Since 2000 the average height of the tree has been 78.4 feet:"
  },
  {
    "objectID": "post/2023-11-17-a-handy-function-for-getting-ecological-division-from-fia-data/index.html",
    "href": "post/2023-11-17-a-handy-function-for-getting-ecological-division-from-fia-data/index.html",
    "title": "A handy R function for getting ecological division from FIA data",
    "section": "",
    "text": "Ecological sections and subsections in Maine, USA (from Cleland et al. 2007)."
  },
  {
    "objectID": "post/2023-11-17-a-handy-function-for-getting-ecological-division-from-fia-data/index.html#section",
    "href": "post/2023-11-17-a-handy-function-for-getting-ecological-division-from-fia-data/index.html#section",
    "title": "A handy R function for getting ecological division from FIA data",
    "section": "",
    "text": "To get a better feel for how they work, I’ve been digging into the new tree volume, biomass, and carbon models released by the USDA Forest Service last month. These new equations replace volume equations developed at the regional level and are now nationally-consistent. Importantly for anyone that works with forest carbon inventory data, these new equations replace the Component Ratio Method (CRM).\nAlthough the equations replace regional ones previously implemented, they do require a geographical variable not widely used in forest biomass and carbon equations: ecological division. These divisions are outlined in the US National Hierarchical Framework of Ecological Units, described in detail in a map produced by Cleland and others in 2007. Within divisions there are ecological provinces. Within those provinces there are ecological sections and subsections.\nIf you’re doing an analysis on an individual ownership, you can simply look up the property’s ecological division and find the equations for the tree species that you need. But if you’re doing an analysis at a larger scale, e.g., across a US state, there may exist multiple ecological divisions within your project area. Ecological divisions don’t line up with regional, state, or county boundaries, making it tricky to keep track of which volume, biomass, and carbon equation to use. For example, a state like Maine has three different ecological divisions. Hence, the same-sized eastern white pine tree located in three different locations in the state could have three different estates of tree biomass.\nThis issue will arise for analysts that do statewide assessments using Forest Inventory and Analysis (FIA) data. While the FIA database has been updated with the new estimates of tree volume, biomass, and carbon, analysts often will need to use the FIA data and write customized functions based on the data.\nFortunately, the PLOT table in the FIA database contains the ecological subsection where the plot resides, stored in the ECOSUBCD variable. As described in the FIA Database User Guide, ecological subsections are defined as areas of “similar surficial geology, lithology, geomorphic process, soil groups, subregional climate, and potential natural communities.” This represents the most detailed level within the hierarchical framework.\nYou’ll need to do some work to pull the ecological division out from this variable, and it’s messy because of the alpha-numeric characteristics and differing lengths of of the ECOSUBCD variable. For example, ECOSUBCD 211Da in central Maine is termed the Central Maine Embayment Subsection.\nI’ll use some functions from the stringr package in R to do this, which can be called from the tidyverse library:\n\nlibrary(tidyverse)\n\nFirst, I’ll make a data table called eco_codes of all of the ECOSUBCD values list in Maine’s PLOT table from FIA’s ME_PLOT.csv file using the as_tibble() function. Then I’ll rename the variables to something that makes sense. You can see that Maine has 19 unique ecological subsections:\n\neco_codes &lt;- as_tibble(table(me_plot$ECOSUBCD), .name_repair = \"unique\") |&gt; \n  rename(ECOSUBCD = ...1,\n         num_plots = n)\n\neco_codes\n\n# A tibble: 19 × 2\n   ECOSUBCD num_plots\n   &lt;chr&gt;        &lt;int&gt;\n 1 211Aa         1499\n 2 211Ab          776\n 3 211Ba          962\n 4 211Bb         2020\n 5 211Ca          788\n 6 211Cb          820\n 7 211Da         2697\n 8 211Db          434\n 9 211Dc          628\n10 221Ai          162\n11 221Ak          425\n12 221Al          785\n13 M211Aa         952\n14 M211Ab        2386\n15 M211Ac        1546\n16 M211Ad         103\n17 M211Ae         648\n18 M211Af        1088\n19 M211Ag         767\n\n\nWe can determine the ecological province (eco_province) by using the str_sub() function by “trimming off” the subsection letters at the end of the subsection name. This can be done by specifying end = -3 in the function.\nIf only it were that simple. To obtain the ecological division we need to “round down” the ecological province. For example, Ecological province 211 (Northeastern Mixed Forest Province) corresponds to ecological division 210 (Warm Continental Division). To do this you can replace the last character in the eco_province variable with a zero. This can be done with the str_replace() function and replacing the last character using the \".$\" command.\nIn the end, you can see the relationships between the ecological subsection, province, and division codes:\n\neco_codes &lt;- eco_codes |&gt; \n  mutate(eco_province = str_sub(ECOSUBCD, end = -3),\n         eco_division = str_replace(eco_province, \".$\", \"0\"))\neco_codes\n\n# A tibble: 19 × 4\n   ECOSUBCD num_plots eco_province eco_division\n   &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;       \n 1 211Aa         1499 211          210         \n 2 211Ab          776 211          210         \n 3 211Ba          962 211          210         \n 4 211Bb         2020 211          210         \n 5 211Ca          788 211          210         \n 6 211Cb          820 211          210         \n 7 211Da         2697 211          210         \n 8 211Db          434 211          210         \n 9 211Dc          628 211          210         \n10 221Ai          162 221          220         \n11 221Ak          425 221          220         \n12 221Al          785 221          220         \n13 M211Aa         952 M211         M210        \n14 M211Ab        2386 M211         M210        \n15 M211Ac        1546 M211         M210        \n16 M211Ad         103 M211         M210        \n17 M211Ae         648 M211         M210        \n18 M211Af        1088 M211         M210        \n19 M211Ag         767 M211         M210        \n\n\nYou can see that most FIA plots in Maine are found in the Division 210 (Warm Continental Division), followed by M210 (Warm Continental Division - Mountain Provinces) and 220 (Hot Continental Division):\n\neco_codes |&gt; \n  summarize(num_plots_division = sum(num_plots),\n            .by = \"eco_division\")\n\n# A tibble: 3 × 2\n  eco_division num_plots_division\n  &lt;chr&gt;                     &lt;int&gt;\n1 210                       10624\n2 220                        1372\n3 M210                       7490\n\n\nSurely, there must exist an ecological subsection lookup table that contains each subsection and its corresponding values for ecological sections, divisions, and so forth. I haven’t been able to find one yet, but if you’re aware of one, do let me know! For know I’ll likely continue use functions like this to pull out these values.\n–\nBy Matt Russell. Email Matt with any questions or comments."
  },
  {
    "objectID": "post/2023-10-25-using-r-and-python-to-get-forest-resource-data-through-the-evalidator-api/index.html#using-r-to-access-fia-data-at-the-state-level",
    "href": "post/2023-10-25-using-r-and-python-to-get-forest-resource-data-through-the-evalidator-api/index.html#using-r-to-access-fia-data-at-the-state-level",
    "title": "Using R and Python to get forest resource data through the EVALIDator API",
    "section": "Using R to access FIA data at the state level",
    "text": "Using R to access FIA data at the state level\nThis first example will access the total amount of carbon stored in the live aboveground portions of trees growing on forestland in Maine, measured in metric tonnes. I’ll ask for estimates to be provided by forest type group and ownership.\nFirst, I’ll use R and will load a few packages to access the data:\n\nlibrary(tidyverse)\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(rlist)\nlibrary(knitr)\n\nIf you look at the bottom of the documentation webpage, you’ll see example use cases for extracting FIA estimates using R and Python. There are two examples for each that use GET and POST scripts. The GET scripts allow you to enter a complete URL if you know the attributes you’re interested in. The POST script, which I copy here with the fiadb_api_POST() function, allows you to directly input the variables you’re interested in in R:\n\n# fiadb_api_POST() will accept a FIADB-API full report URL and return data frames\n# See descriptor: https://apps.fs.usda.gov/fiadb-api/\n\nfiadb_api_POST &lt;- function(argList){\n  # make request\n  resp &lt;- POST(url = \"https://apps.fs.usda.gov/fiadb-api/fullreport\", \n               body = argList, \n               encode = \"form\")\n  # parse response from JSON to R list\n  respObj &lt;- content(resp, \"parsed\", encoding = \"ISO-8859-1\")\n  # create empty output list\n  outputList &lt;- list()\n  # add estimates data frame to output list\n  outputList[['estimates']] &lt;- as.data.frame(do.call(rbind,respObj$estimates))\n\n  # if estimate includes totals and subtotals, add those data frames to output list\n  if ('subtotals' %in% names(respObj)){\n    subtotals &lt;- list()\n    # one subtotal data frame for each grouping variable\n    for (i in names(respObj$subtotals)){\n      subtotals[[i]] &lt;- as.data.frame(do.call(rbind,respObj$subtotals[[i]]))\n    }\n    outputList[['subtotals']] &lt;- subtotals\n\n    # totals data frame\n    outputList[['totals']] &lt;- as.data.frame(do.call(rbind,respObj$totals))\n  }\n\n  # add estimate metadata\n  outputList[['metadata']] &lt;- respObj$metadata\n\n  return(outputList)\n}\n\nThe first item to know before accessing FIA data is the numeric code corresponding to the variable you’re interested in. In my case, this is snum = 98, corresponding to the variable that represents “Forest carbon pool 1: live aboveground, in metric tonnes, on forest land.” A note of caution: there are thousands of variables to choose from, but I suspect the most popular ones are listed toward the top of the page.\nThe second item to know is which wc code you’re interested in. I have no idea why it’s abbreviated “wc”, but it connects the state FIPS code with the 4-digit FIA inventory year. For example, I’m interested in wc = 232021 to obtain data from Maine’s (FIPS code 23) most recent inventory, collected in 2021. You could go back to inventories collected decades ago if you were interested in looking at trends, and those codes are described here.\nFinally, you can use the rselected and cselected statements to identify the variables you’d like to group by in rows and columns. In our case this is forest type group and ownership group. I’ll obtain the data in an NJSON format, but you can also obtain the data in HTML, XML, or other formats. You can specify these parameters in arg_list and then store the data in a data frame called all_rows. I love how the data are presented in a long and “tidy” format:\n\narg_list_maine &lt;- list(snum = 98,\n               wc = 232021,\n               rselected = 'Forest type group', \n               cselected = 'Ownership group - Major',\n               outputFormat = 'NJSON')\n\n# submit list to POST request function\npost_data_maine &lt;- fiadb_api_POST(arg_list_maine)\n\n# estimate data frame\nall_rows_maine &lt;- post_data_maine[['estimates']]\n\nprint(all_rows_maine)\n\n    ESTIMATE                                    GRP1          GRP2 PLOT_COUNT\n1    3904898 `0001 White \\\\/ red \\\\/ jack pine group  `0001 Public         22\n2   34789659 `0001 White \\\\/ red \\\\/ jack pine group `0002 Private        248\n3   14310434              `0002 Spruce \\\\/ fir group  `0001 Public        110\n4  105397247              `0002 Spruce \\\\/ fir group `0002 Private       1022\n5   62478.28 `0005 Loblolly \\\\/ shortleaf pine group `0002 Private          1\n6   200517.1            `0018 Exotic softwoods group  `0001 Public          1\n7     531124            `0018 Exotic softwoods group `0002 Private          5\n8   823381.8                `0020 Oak \\\\/ pine group  `0001 Public          5\n9   10607119                `0020 Oak \\\\/ pine group `0002 Private         73\n10  992844.6             `0021 Oak \\\\/ hickory group  `0001 Public          8\n11  11742364             `0021 Oak \\\\/ hickory group `0002 Private         85\n12  409060.3  `0023 Elm \\\\/ ash \\\\/ cottonwood group  `0001 Public          7\n13   6009873  `0023 Elm \\\\/ ash \\\\/ cottonwood group `0002 Private         85\n14  14742425   `0024 Maple \\\\/ beech \\\\/ birch group  `0001 Public        111\n15 136482887   `0024 Maple \\\\/ beech \\\\/ birch group `0002 Private       1244\n16   3260952             `0025 Aspen \\\\/ birch group  `0001 Public         28\n17  29091440             `0025 Aspen \\\\/ birch group `0002 Private        330\n18  5415.465             `0030 Other hardwoods group  `0001 Public          2\n19  734850.1             `0030 Other hardwoods group `0002 Private         26\n20  2668.444                        `0034 Nonstocked  `0001 Public          1\n21  56632.09                        `0034 Nonstocked `0002 Private         10\n         SE SE_PERCENT     VARIANCE\n1  989847.4   25.34887 979797935531\n2   2438499   7.009263 5.946276e+12\n3   1346913   9.412105 1.814175e+12\n4   3093327   2.934922 9.568672e+12\n5  69535.69   111.2958   4835212844\n6  197339.8   98.41544   3.8943e+10\n7  322121.4     60.649 103762218840\n8  383501.6    46.5764 147073471553\n9   1405489   13.25043 1.975398e+12\n10 376671.9   37.93866 141881751049\n11  1392492    11.8587 1.939034e+12\n12 210167.8    51.3782  44170517675\n13 792626.1   13.18873 628256055243\n14  1316485   8.929905 1.733132e+12\n15  3598018   2.636241 1.294574e+13\n16   719201   22.05494 517250138459\n17  1931424   6.639148 3.730397e+12\n18 4321.885   79.80635     18678690\n19 191250.2   26.02575  36576657832\n20 2380.922    89.2251      5668789\n21 31032.09   54.79594    962990504\n\n\nThis makes the output easy to plot immediately. Here’s a graph of the output, where you can see the vast amount of carbon stored on private land in Maine, mostly in the spruce/fir and maple/beech/birch forest type groups:\n\nggplot(all_rows_maine, aes(x = as.character(GRP2), \n                           y = as.numeric(ESTIMATE))) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~as.character(GRP1)) +\n  labs(x = \"Ownership\",\n       y = \"Forest carbon (metric tonnes)\")\n\n\n\n\n\n\n\n\nNote you’ll need to play with the names of variables a bit to tidy them up, e.g., turning “`0001 Public” to simply “Public”. But the R functions allow you to quickly obtain the data of interest. You can also grab the subtotals of the output to sum all values within each forest type or ownership category:\n\nsubtotals_maine &lt;- post_data_maine[['subtotals']]\n\nprint(subtotals_maine)\n\n$GRP1\n    ESTIMATE                                    GRP1 PLOT_COUNT       SE\n1   38694556 `0001 White \\\\/ red \\\\/ jack pine group        269  2618535\n2  119707681              `0002 Spruce \\\\/ fir group       1127  3344993\n3   62478.28 `0005 Loblolly \\\\/ shortleaf pine group          1 69535.69\n4   731641.2            `0018 Exotic softwoods group          6 377174.6\n5   11430501                `0020 Oak \\\\/ pine group         78  1452201\n6   12735208             `0021 Oak \\\\/ hickory group         92  1432097\n7    6418933  `0023 Elm \\\\/ ash \\\\/ cottonwood group         92   815680\n8  151225312   `0024 Maple \\\\/ beech \\\\/ birch group       1348  3791540\n9   32352393             `0025 Aspen \\\\/ birch group        357  2057692\n10  740265.6             `0030 Other hardwoods group         28 191299.1\n11  59300.54                        `0034 Nonstocked         11 31123.29\n   SE_PERCENT     VARIANCE\n1    6.767193 6.856727e+12\n2    2.794301 1.118898e+13\n3    111.2958   4835212844\n4    51.55185 142260655272\n5    12.70461 2.108887e+12\n6    11.24518 2.050902e+12\n7    12.70741 665333890414\n8    2.507213 1.437578e+13\n9    6.360247 4.234097e+12\n10   25.84195  36595336521\n11     52.484    968659293\n\n$GRP2\n   ESTIMATE          GRP2 PLOT_COUNT      SE SE_PERCENT     VARIANCE\n1  38652597  `0001 Public        274 1659512   4.293404  2.75398e+12\n2 335505672 `0002 Private       2891 3373908   1.005619 1.138325e+13\n\n\nFinally, you can grab the total number that sums all values. In this case, we learn that there’s about 374 million metric tonnes of carbon stored in the live aboveground portions of trees in Maine forests:\n\ntotals_maine &lt;- post_data_maine[['totals']]\nprint(totals_maine)\n\n   ESTIMATE PLOT_COUNT      SE SE_PERCENT     VARIANCE\n1 374158269       3143 3430416  0.9168357 1.176776e+13"
  },
  {
    "objectID": "post/2023-10-25-using-r-and-python-to-get-forest-resource-data-through-the-evalidator-api/index.html#using-r-to-access-fia-data-around-a-fixed-geographic-point",
    "href": "post/2023-10-25-using-r-and-python-to-get-forest-resource-data-through-the-evalidator-api/index.html#using-r-to-access-fia-data-around-a-fixed-geographic-point",
    "title": "Using R and Python to get forest resource data through the EVALIDator API",
    "section": "Using R to access FIA data around a fixed geographic point",
    "text": "Using R to access FIA data around a fixed geographic point\nA favorite use of EVALIDator by many is the ability to query FIA data around a specific location. For example, a user can generate population estimates for a 50-mile radius around a proposed mill that uses wood.\nHere’s an example that queries FIA data in a 25-mile radius surrounding Bangor, Maine. It estimates the total forestland area (snum = 2) by stand age class in 20-year increments and stand size class (large-, medium-, or small-diameter trees). The latitude and longitude coordinates are specified in the function:\n\narg_list_bangor &lt;- list(lat = 44.809,\n                        lon = -68.771, \n                        radius = 25,\n                        wc = 232021,\n                        snum = 2,\n                        rselected = 'Stand-size class', \n                        cselected = 'Stand age 20 yr classes (0 to 100 plus)',\n                        outputFormat = 'NJSON')\n\n# submit list to POST request function\npost_data_bangor &lt;- fiadb_api_POST(arg_list_bangor)\n\n# estimate data frame\nall_rows_bangor &lt;- post_data_bangor[['estimates']]\n\nprint(all_rows_bangor)\n\n   ESTIMATE                  GRP1               GRP2 PLOT_COUNT       SE\n1   14155.6  `0001 Large diameter  `0002 21-40 years          4 7338.611\n2  45473.05  `0001 Large diameter  `0003 41-60 years         10    15372\n3  90436.76  `0001 Large diameter  `0004 61-80 years         18 22385.23\n4    107109  `0001 Large diameter `0005 81-100 years         23 24368.78\n5  41243.83  `0001 Large diameter   `0006 100+ years          7  15696.3\n6  51611.41 `0002 Medium diameter  `0002 21-40 years         12 16379.57\n7  113830.6 `0002 Medium diameter  `0003 41-60 years         25 24352.08\n8  199754.6 `0002 Medium diameter  `0004 61-80 years         37 33455.07\n9  103521.8 `0002 Medium diameter `0005 81-100 years         21 24195.48\n10   5758.7 `0002 Medium diameter   `0006 100+ years          1 6011.665\n11  19655.8  `0003 Small diameter   `0001 0-20 years          4 10104.88\n12 90910.76  `0003 Small diameter  `0002 21-40 years         16 23393.71\n13    40205  `0003 Small diameter  `0003 41-60 years          7 15304.59\n14 13209.02  `0003 Small diameter  `0004 61-80 years          3 8759.869\n15 21404.85  `0003 Small diameter `0005 81-100 years          3  11056.3\n16 5305.999  `0003 Small diameter   `0006 100+ years          1 5045.363\n   SE_PERCENT   VARIANCE\n1    51.84246   53855209\n2    33.80465  236298512\n3    24.75236  501098555\n4    22.75138  593837455\n5    38.05733  246373784\n6    31.73633  268290228\n7    21.39326  593023809\n8    16.74808 1119241412\n9    23.37235  585421221\n10   104.3927   36140117\n11   51.40919  102108689\n12   25.73261  547265614\n13   38.06637  234230329\n14   66.31735   76735306\n15   51.65323  122241669\n16    95.0879   25455691\n\n\nYou can plot these data directly to visualize the trends within the 25-mile radius:\n\nggplot(all_rows_bangor, aes(x = as.character(GRP2), \n                            y = as.numeric(ESTIMATE),\n                            fill = as.character(GRP1))) +\n  geom_bar(stat = \"identity\") +\n  labs(x = \"Stand age\",\n       y = \"Forestland area (acres)\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "post/2023-10-25-using-r-and-python-to-get-forest-resource-data-through-the-evalidator-api/index.html#conclusion",
    "href": "post/2023-10-25-using-r-and-python-to-get-forest-resource-data-through-the-evalidator-api/index.html#conclusion",
    "title": "Using R and Python to get forest resource data through the EVALIDator API",
    "section": "Conclusion",
    "text": "Conclusion\nThe new features available in the EVALIDator API that allow you to access data through R or Python can help to speed up data analysis using FIA data. You can access the entire history of FIA data and analyze forest resources data by state or a circular retrieval from a fixed geographic point. Special thanks to the USDA Forest Service staff that have made this available!\n–\nBy Matt Russell. Email Matt with any questions or comments."
  },
  {
    "objectID": "post/2023-09-13-recent-changes-in-the-tidyverse/index.html#conclusion",
    "href": "post/2023-09-13-recent-changes-in-the-tidyverse/index.html#conclusion",
    "title": "Recent updates to tidyverse functions",
    "section": "Conclusion",
    "text": "Conclusion\nCheck out these new features in the tidyverse and try them in your own analysis. These new features are particularly well adapted for new learners of R/the tidyverse given they make more intuitive sense (and speed up performance). Let me know if any of the new techniques presented here help in your own data analysis.\n–\nBy Matt Russell. Email Matt with any questions or comments."
  },
  {
    "objectID": "post/2023-07-31-linear-quantile-mixed-models-applications-in-forestry/index.html",
    "href": "post/2023-07-31-linear-quantile-mixed-models-applications-in-forestry/index.html",
    "title": "Linear quantile mixed models: applications in forestry",
    "section": "",
    "text": "Example output comparing predictions from three models fit with linear quantile mixed models and one fit using ordinary least squares."
  },
  {
    "objectID": "post/2023-07-31-linear-quantile-mixed-models-applications-in-forestry/index.html#linear-quantile-mixed-models",
    "href": "post/2023-07-31-linear-quantile-mixed-models-applications-in-forestry/index.html#linear-quantile-mixed-models",
    "title": "Linear quantile mixed models: applications in forestry",
    "section": "Linear quantile mixed models",
    "text": "Linear quantile mixed models\nAs an extension to quantile regression, linear quantile mixed models (LQMM) provide a flexible structure to account for multilevel data through the incorporation of random effects. These LQMM models may be specified with random effects similar to a mixed models framework.\nThe general model form for an LQMM model follows, adapted from a simple linear regression model form with a random effect placed on the intercept:\n\\[Y=\\beta_0+b_i+\\beta_1X+\\epsilon\\]\nwhere \\(\\beta_0\\) and \\(\\beta_1\\) are fixed effects and \\(b_i\\) is a random effect for subject \\(i\\). The random effect can be thought of as each subject’s deviation from the fixed intercept parameter. The \\(\\beta_0\\) and \\(\\beta_1\\) values are fit using LQMM procedures to any desired quantile."
  },
  {
    "objectID": "post/2023-07-31-linear-quantile-mixed-models-applications-in-forestry/index.html#the-lqmm-package-in-r",
    "href": "post/2023-07-31-linear-quantile-mixed-models-applications-in-forestry/index.html#the-lqmm-package-in-r",
    "title": "Linear quantile mixed models: applications in forestry",
    "section": "The lqmm package in R",
    "text": "The lqmm package in R\nIn R, the lqmm package fits linear quantile mixed models for hierarchical data:\n\n#install.packages(\"lqmm\")\n\nlibrary(lqmm)\n\nAs an application, we can fit an LQMM to predict tree height (HT) based on its diameter at breast height (DBH). Data are from 450 red pine trees collected at the Cloquet Forestry Center in Cloquet, Minnesota in 2014 with DBH measured in inches and HT measured in feet. The data are found in the redpine data set available in the stats4nr package:\n\n# install.packages(\"devtools\")\n\n#devtools::install_github(\"mbrussell/stats4nr\")\n\nlibrary(stats4nr)\n\ndata(redpine)\n\nNow, most applications of tree height-diameter models will employ nonlinear model forms, but for the red pine data here, we can see a general linear trend with the data. Plus, it will make our application of LQMM techniques easy to compare to other linear regression techniques:\n\nlibrary(tidyverse)\n\nggplot(redpine, aes(x = DBH, y = HT)) +\n  geom_point() +\n  labs(x = \"DBH (inches)\",\n       y = \"Height (feet)\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nWe might believe that specifying the inventory plot as a random effect can improve our model performance. That is, we might believe that the inventory plot in which a tree was measured can reduce the variability in tree height estimates. To fit an LQMM model with a random intercept term in R, we specify the lqmm() function. The key arguments are specifying the model form in the fixed = statement and the random effect variable with the group = statement. The tau = statement allows you to specify the quantile you wish to run the model:\n\nht.lqmm50 &lt;- lqmm(fixed = HT ~ DBH, \n                random = ~1, \n                group = PlotNum,\n                tau = 0.50,\n                data = redpine)\n\nsummary(ht.lqmm50)\n\nCall: lqmm(fixed = HT ~ DBH, random = ~1, group = PlotNum, tau = 0.5, \n    data = redpine)\n\nQuantile 0.5 \n\nFixed effects:\n               Value Std. Error lower bound upper bound  Pr(&gt;|t|)    \n(Intercept) 31.51788    3.83488    23.81139     39.2244 8.863e-11 ***\nDBH          2.76287    0.24128     2.27800      3.2477 1.857e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAIC:\n[1] 3441 (df = 4)\n\n\nWe can also extract the random effects for each inventory plot using the ranef() function. We can see that the random effects are centered around zero and range from about -20 to 20:\n\nranef &lt;- ranef(ht.lqmm50)\n\nggplot(ranef, aes(`(Intercept)`)) +\n  geom_density()  \n\n\n\n\n\n\n\n\nWe can compare the LQMM model output to a simple linear regression fit the the mean value and without any random effect:\n\nht.slr &lt;- lm(HT~DBH,\n             data = redpine)\nsummary(ht.slr)\n\n\nCall:\nlm(formula = HT ~ DBH, data = redpine)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-55.449  -9.250  -0.894   8.878  43.415 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  31.1552     1.8357   16.97   &lt;2e-16 ***\nDBH           3.0493     0.1201   25.39   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.31 on 448 degrees of freedom\nMultiple R-squared:   0.59, Adjusted R-squared:  0.5891 \nF-statistic: 644.7 on 1 and 448 DF,  p-value: &lt; 2.2e-16\n\n\nYou can see that the intercept values are quite similar (31.5179 and 31.1552), however the slope for the LQMM model (2.7629) is lower than that of the simple linear regression (3.0493).\nWe can specify multiple quantiles in the tau = statement to run multiple regressions at various quantiles of the data. We might seek to do this because trees at higher quantiles may be in better crown positions, i.e., they are dominant or co-dominant trees and might need to estimate site index of the forest stand using these trees. Similarly, trees at lower quantiles may be suppressed or intermediate in crown position and we might wish to implement management strategies that improve their growth. As we fit the LQMM models, note the increases in the slope term associated with DBH as the quantiles increase from 0.25 to 0.75:\n\nht.lqmm &lt;- lqmm(fixed = HT ~ DBH, \n                random = ~1, \n                group = PlotNum,\n                tau = c(0.25, 0.50, 0.75),\n                data = redpine)\n\nsummary(ht.lqmm)\n\nWarning in errorHandling(OPTIMIZATION$low_loop, \"low\", control$LP_max_iter, : Lower loop did not converge in: lqmm. Try increasing max number of iterations (500) or tolerance (1e-05)\n\n\nCall: lqmm(fixed = HT ~ DBH, random = ~1, group = PlotNum, tau = c(0.25, \n    0.5, 0.75), data = redpine)\n\ntau = 0.25\n\nFixed effects:\n               Value Std. Error lower bound upper bound  Pr(&gt;|t|)    \n(Intercept) 28.26885    3.88807    20.45547     36.0822 2.529e-09 ***\nDBH          2.69508    0.21929     2.25439      3.1358 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ntau = 0.5\n\nFixed effects:\n               Value Std. Error lower bound upper bound  Pr(&gt;|t|)    \n(Intercept) 31.51788    3.77772    23.92626     39.1095 5.736e-11 ***\nDBH          2.76287    0.21189     2.33706      3.1887 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ntau = 0.75\n\nFixed effects:\n               Value Std. Error lower bound upper bound  Pr(&gt;|t|)    \n(Intercept) 32.15125    3.78961    24.53574     39.7668 3.509e-11 ***\nDBH          3.19633    0.25434     2.68522      3.7074 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAIC:\n[1] 3483 (df = 4) 3441 (df = 4) 3473 (df = 4)\n\n\nLet’s make a small data set that contains the predictions of these models, stored in the tree data set. Then, we’ll visualize the output along with the ordinary least squares line:\n\ntree &lt;- tibble(DBH = c(seq(0,30,1))) %&gt;% \n  mutate(`Ordinary least squares` = 31.1552 + 3.0493*DBH,\n         `LQMM, quantile = 0.25` = 28.26885 + 2.69508*DBH,\n         `LQMM, quantile = 0.50` = 31.51788 + 2.76287*DBH,\n         `LQMM, quantile = 0.75` = 32.15125 + 3.19633*DBH) %&gt;% \n  pivot_longer(`Ordinary least squares`:`LQMM, quantile = 0.75`,\n               names_to = \"Model\",\n               values_to = \"HT\")\n\nggplot(tree, aes(DBH, HT, col = Model)) +\n  geom_line(linewidth = 1.25) +\n  labs(x = \"DBH (inches)\",\n       y = \"Height (feet)\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nOne thing I’ve noticed in fitting LQMM models is that the standard errors for parameter estimates tend to always be higher for compared to ordinary least squares models. They also tend to be high compared to models fit with linear mixed models through functions available in packages like lme4. This is likely due to the quantile regression techniques."
  },
  {
    "objectID": "post/2023-07-31-linear-quantile-mixed-models-applications-in-forestry/index.html#conclusion",
    "href": "post/2023-07-31-linear-quantile-mixed-models-applications-in-forestry/index.html#conclusion",
    "title": "Linear quantile mixed models: applications in forestry",
    "section": "Conclusion",
    "text": "Conclusion\nLinear quantile mixed models are robust and combine the strengths of both mixed models and quantile regression techniques. These models have a number of potential applications with forestry and natural resources data where data are often nested and we’re not always concerned with what’s happening through the mean value of the data, but instead at some other quantile. There is also an R package that fits quantile regressions for nonlinear mixed-effects models, providing more tools to estimate trends through any quantile with data found in a variety of shapes and sizes.\n–\nBy Matt Russell. Email Matt with any questions or comments."
  },
  {
    "objectID": "post/2023-06-30-making-r-graphs-more-accessible-using-brailler/index.html",
    "href": "post/2023-06-30-making-r-graphs-more-accessible-using-brailler/index.html",
    "title": "Making R graphs more accessible using BrailleR",
    "section": "",
    "text": "An example use of alt text for a scatterplot made in R, with description of the scatterplot provided by the BrailleR package below the graph.\n\n\n\n\n\nYou might have seen these prompts while you’re uploading an image to social media or anywhere else on the internet: “Add alt text here.” Alt text describes what’s contained in an image or figure on a webpage. It’s useful because it can be used by screen readers for visually impaired people.\nAdding alt text to graphics promotes accessibility and allows more readers to interpret your image or figure. Alt text will also appear on the screen if an image does not load properly on a webpage.\nAs it turns out, adding alt text to images also improves search engine optimization. So, dedicating time to writing alt text on images and figures can promote accessibility and also help to rank your webpages higher in internet searches.\nI recently read an excellent interview in Significance magazine about the importance of using alt text for visually impaired learners. In it, they describe the R package BrailleR and how it can be used to generate text from figures made in R. This post describes a few example uses of the functions found within the package.\n\n\n\nI’ve read online that R is a software program that is already friendly for the blind and visually impaired. The BrailleR package contains functions that do more to help blind and visually impaired people interpret graphs and figures produced in R.\nYou can install the R package and load it in your workspace:\n\n# install.packages(\"BrailleR\")\nlibrary(BrailleR)\n\nAs an example, I’ll show a series of graphs using the redpine data set available in the stats4nr package. Data contain information collected from 450 red pine trees in Minnesota, including their diameter at breast height (DBH) and total height (HT). Data were collected from various forest cover types (CoverType*) and forest inventory plots (PlotNum) across the forest:\n\n# install.packages(\"devtools\")\n\n# devtools::install_github(\"mbrussell/stats4nr\")\n\nlibrary(stats4nr)\n\ndata(redpine)\n\nredpine\n\n# A tibble: 450 × 5\n   PlotNum CoverType TreeNum   DBH    HT\n     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1       5 Red pine       46  13      51\n 2       5 Red pine       50   8.3    54\n 3       5 Red pine       54   8.2    48\n 4       5 Red pine       56  11.8    55\n 5       5 Red pine       63   9      54\n 6       5 Red pine       71  12.5    46\n 7      11 Cut            47  25.3    90\n 8      13 Red pine        1  16.2    85\n 9      13 Red pine        2  18.8    86\n10      13 Red pine        7  22.2    90\n# ℹ 440 more rows\n\n\nWe can make a plot of tree height and diameter using the following code from ggplot:\n\nlibrary(tidyverse)\n\np_HT &lt;- ggplot(redpine, aes(DBH, HT)) +\n  geom_point() +\n  labs(x = \"Diameter at breast height (inches)\",\n       y = \"Height (feet)\",\n       title = \"Red pine height-diameter observations\")\n\nWhen we call the p_HT object, the BrailleR package provides the graph in addition to four lines of text describing what’s contained in the plot:\n\np_HT\n\n\n\n\n\n\n\n\nThis chart has title 'Red pine height-diameter observations'.\nIt has x-axis 'Diameter at breast height (inches)' with labels 10, 20 and 30.\nIt has y-axis 'Height (feet)' with labels 25, 50, 75, 100 and 125.\nThe chart is a set of 450 big solid circle points of which about 91% can be seen.\n\n\nThe output reads us the title of the plot, the axis labels and axis titles, and the number of data points. This is all very helpful attributes for interpreting the plot and can be read on the screen.\nThe data are collected from different cover types, so we can add a different color for each tree using col = CoverType and produce the results:\n\np_HT2 &lt;- ggplot(redpine, aes(DBH, HT, col = CoverType)) +\n  geom_point() +\n  labs(x = \"Diameter at breast height (inches)\",\n       y = \"Height (feet)\",\n       title = \"Red pine height-diameter observations\")\n\np_HT2\n\n\n\n\n\n\n\n\nThis chart has title 'Red pine height-diameter observations'.\nIt has x-axis 'Diameter at breast height (inches)' with labels 10, 20 and 30.\nIt has y-axis 'Height (feet)' with labels 25, 50, 75, 100 and 125.\nThere is a legend indicating colour is used to show CoverType, with 13 levels:\nAspen shown as strong reddish orange colour, \nBalsam fir shown as strong orange colour, \nCut shown as strong yellow colour, \nGrass shown as vivid yellow green colour, \nJack pine shown as vivid yellowish green colour, \nPaper birch shown as vivid yellowish green colour, \nRed pine shown as brilliant bluish green colour, \nRight of way shown as vivid blue colour, \nScotch pine shown as brilliant blue colour, \nSwamp conifers shown as vivid violet colour, \nUpland mixed hardwoods shown as vivid violet colour, \nWhite pine shown as deep purplish pink colour and \nWhite spruce shown as vivid purplish red colour.\nThe chart is a set of 450 big solid circle points of which about 91% can be seen.\n\n\nNow, you can see the additional detail of the number of levels in each cover type and their colors.\nThe **BrailleR package also works for graphics developed in base R, which can be specified with the VI() function. Here’s a histogram of the height of red pines.\n\nVI(hist(redpine$HT))\n\n\n\n\n\n\n\n\nThis is a histogram, with the title: with the title: Histogram of redpine$HT\n\"redpine$HT\" is marked on the x-axis.\nTick marks for the x-axis are at: 20, 40, 60, 80, 100, and 120 \nThere are a total of 450 elements for this variable.\nTick marks for the y-axis are at: 0, 20, 40, 60, and 80 \nIt has 11 bins with equal widths, starting at 10 and ending at 120 .\nThe mids and counts for the bins are:\nmid = 15  count = 4 \nmid = 25  count = 7 \nmid = 35  count = 17 \nmid = 45  count = 47 \nmid = 55  count = 59 \nmid = 65  count = 61 \nmid = 75  count = 46 \nmid = 85  count = 77 \nmid = 95  count = 75 \nmid = 105  count = 50 \nmid = 115  count = 7\n\n\nThe functions available in BrailleR are a great start for generating alt text copy that can be used on a figure or image. But they won’t tell you the trends in the data or the “big message” that you want to leave your reader with. This is essential to have an effective graphic.\nFor this, Amy Cesal has any excellent post on writing alt-text images. Her template is to use:\n\n“CHART TYPE of TYPE OF DATA where REASON FOR INCUDING CHART.”\n\nFor example, for the first figure plotting tree diameter and height, one could provide the following for the alt text description:\n\n“Scatter plot of red pine tree diameters and heights where larger diameter trees are taller than shorter ones.”\n\nIn combination with the text provided by BrailleR, these steps can help improve the accessibility of your graphs and figures made in R. For more, a great resource I found for learning more about the BrailleR package is the book BrailleR In Action by A. Jonathan R. Godfrey.\n–\nBy Matt Russell. Email Matt with any questions or comments."
  },
  {
    "objectID": "post/2023-06-30-making-r-graphs-more-accessible-using-brailler/index.html#section-1",
    "href": "post/2023-06-30-making-r-graphs-more-accessible-using-brailler/index.html#section-1",
    "title": "Making R graphs more accessible using BrailleR",
    "section": "",
    "text": "You might have seen these prompts while you’re uploading an image to social media or anywhere else on the internet: “Add alt text here.” Alt text describes what’s contained in an image or figure on a webpage. It’s useful because it can be used by screen readers for visually impaired people.\nAdding alt text to graphics promotes accessibility and allows more readers to interpret your image or figure. Alt text will also appear on the screen if an image does not load properly on a webpage.\nAs it turns out, adding alt text to images also improves search engine optimization. So, dedicating time to writing alt text on images and figures can promote accessibility and also help to rank your webpages higher in internet searches.\nI recently read an excellent interview in Significance magazine about the importance of using alt text for visually impaired learners. In it, they describe the R package BrailleR and how it can be used to generate text from figures made in R. This post describes a few example uses of the functions found within the package."
  },
  {
    "objectID": "post/2023-06-30-making-r-graphs-more-accessible-using-brailler/index.html#the-brailler-package",
    "href": "post/2023-06-30-making-r-graphs-more-accessible-using-brailler/index.html#the-brailler-package",
    "title": "Making R graphs more accessible using BrailleR",
    "section": "",
    "text": "I’ve read online that R is a software program that is already friendly for the blind and visually impaired. The BrailleR package contains functions that do more to help blind and visually impaired people interpret graphs and figures produced in R.\nYou can install the R package and load it in your workspace:\n\n# install.packages(\"BrailleR\")\nlibrary(BrailleR)\n\nAs an example, I’ll show a series of graphs using the redpine data set available in the stats4nr package. Data contain information collected from 450 red pine trees in Minnesota, including their diameter at breast height (DBH) and total height (HT). Data were collected from various forest cover types (CoverType*) and forest inventory plots (PlotNum) across the forest:\n\n# install.packages(\"devtools\")\n\n# devtools::install_github(\"mbrussell/stats4nr\")\n\nlibrary(stats4nr)\n\ndata(redpine)\n\nredpine\n\n# A tibble: 450 × 5\n   PlotNum CoverType TreeNum   DBH    HT\n     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1       5 Red pine       46  13      51\n 2       5 Red pine       50   8.3    54\n 3       5 Red pine       54   8.2    48\n 4       5 Red pine       56  11.8    55\n 5       5 Red pine       63   9      54\n 6       5 Red pine       71  12.5    46\n 7      11 Cut            47  25.3    90\n 8      13 Red pine        1  16.2    85\n 9      13 Red pine        2  18.8    86\n10      13 Red pine        7  22.2    90\n# ℹ 440 more rows\n\n\nWe can make a plot of tree height and diameter using the following code from ggplot:\n\nlibrary(tidyverse)\n\np_HT &lt;- ggplot(redpine, aes(DBH, HT)) +\n  geom_point() +\n  labs(x = \"Diameter at breast height (inches)\",\n       y = \"Height (feet)\",\n       title = \"Red pine height-diameter observations\")\n\nWhen we call the p_HT object, the BrailleR package provides the graph in addition to four lines of text describing what’s contained in the plot:\n\np_HT\n\n\n\n\n\n\n\n\nThis chart has title 'Red pine height-diameter observations'.\nIt has x-axis 'Diameter at breast height (inches)' with labels 10, 20 and 30.\nIt has y-axis 'Height (feet)' with labels 25, 50, 75, 100 and 125.\nThe chart is a set of 450 big solid circle points of which about 91% can be seen.\n\n\nThe output reads us the title of the plot, the axis labels and axis titles, and the number of data points. This is all very helpful attributes for interpreting the plot and can be read on the screen.\nThe data are collected from different cover types, so we can add a different color for each tree using col = CoverType and produce the results:\n\np_HT2 &lt;- ggplot(redpine, aes(DBH, HT, col = CoverType)) +\n  geom_point() +\n  labs(x = \"Diameter at breast height (inches)\",\n       y = \"Height (feet)\",\n       title = \"Red pine height-diameter observations\")\n\np_HT2\n\n\n\n\n\n\n\n\nThis chart has title 'Red pine height-diameter observations'.\nIt has x-axis 'Diameter at breast height (inches)' with labels 10, 20 and 30.\nIt has y-axis 'Height (feet)' with labels 25, 50, 75, 100 and 125.\nThere is a legend indicating colour is used to show CoverType, with 13 levels:\nAspen shown as strong reddish orange colour, \nBalsam fir shown as strong orange colour, \nCut shown as strong yellow colour, \nGrass shown as vivid yellow green colour, \nJack pine shown as vivid yellowish green colour, \nPaper birch shown as vivid yellowish green colour, \nRed pine shown as brilliant bluish green colour, \nRight of way shown as vivid blue colour, \nScotch pine shown as brilliant blue colour, \nSwamp conifers shown as vivid violet colour, \nUpland mixed hardwoods shown as vivid violet colour, \nWhite pine shown as deep purplish pink colour and \nWhite spruce shown as vivid purplish red colour.\nThe chart is a set of 450 big solid circle points of which about 91% can be seen.\n\n\nNow, you can see the additional detail of the number of levels in each cover type and their colors.\nThe **BrailleR package also works for graphics developed in base R, which can be specified with the VI() function. Here’s a histogram of the height of red pines.\n\nVI(hist(redpine$HT))\n\n\n\n\n\n\n\n\nThis is a histogram, with the title: with the title: Histogram of redpine$HT\n\"redpine$HT\" is marked on the x-axis.\nTick marks for the x-axis are at: 20, 40, 60, 80, 100, and 120 \nThere are a total of 450 elements for this variable.\nTick marks for the y-axis are at: 0, 20, 40, 60, and 80 \nIt has 11 bins with equal widths, starting at 10 and ending at 120 .\nThe mids and counts for the bins are:\nmid = 15  count = 4 \nmid = 25  count = 7 \nmid = 35  count = 17 \nmid = 45  count = 47 \nmid = 55  count = 59 \nmid = 65  count = 61 \nmid = 75  count = 46 \nmid = 85  count = 77 \nmid = 95  count = 75 \nmid = 105  count = 50 \nmid = 115  count = 7\n\n\nThe functions available in BrailleR are a great start for generating alt text copy that can be used on a figure or image. But they won’t tell you the trends in the data or the “big message” that you want to leave your reader with. This is essential to have an effective graphic.\nFor this, Amy Cesal has any excellent post on writing alt-text images. Her template is to use:\n\n“CHART TYPE of TYPE OF DATA where REASON FOR INCUDING CHART.”\n\nFor example, for the first figure plotting tree diameter and height, one could provide the following for the alt text description:\n\n“Scatter plot of red pine tree diameters and heights where larger diameter trees are taller than shorter ones.”\n\nIn combination with the text provided by BrailleR, these steps can help improve the accessibility of your graphs and figures made in R. For more, a great resource I found for learning more about the BrailleR package is the book BrailleR In Action by A. Jonathan R. Godfrey.\n–\nBy Matt Russell. Email Matt with any questions or comments."
  },
  {
    "objectID": "post/2023-04-21-lorey-s-height-the-remote-sensing-way-to-estimate-tree-height/index.html#section",
    "href": "post/2023-04-21-lorey-s-height-the-remote-sensing-way-to-estimate-tree-height/index.html#section",
    "title": "Lorey’s height: the remote sensing way to estimate tree height",
    "section": "",
    "text": "One of the first things you learn in an forestry field course is how to measure the height of trees. Doing so is tricky, as it’s difficult to locate the tops of many trees in dense forests (especially deciduous ones during leaf-on periods). Many foresters will sub-sample tree heights, for example only measuring the heights for every fifth tree or the four closest trees to plot center. Accurate tree height information is critical yet time consuming and expensive to collect in the field.\nForesters require high-quality information on the heights of stands for estimating forest volume, growth, and other forest characteristics. With inventory data in hand, foresters have many ways to represent the average height of a stand. Because top height is widely used in determining forest site productivity, understanding how the definition of top height impacts site index is important, as work by Sharma and others have reported. A popular technique to represent average top height of a stand is Lorey’s height.\nLorey’s height is a measure of average stand height that is weighted by basal area. A benefit is that if you’re using horizontal point sampling to collect data, the average height of all “in” trees is Lorey’s mean height.\nLorey’s height is popular in the remote sensing community because the height of the largest diameter trees is more likely to be detected by active or passive sensors. Although technologies have come a long way, the ability to account for the heights of shorter trees underneath the canopy remains difficult. That may be okay–in a forestry context it’s often the larger trees that provide more economic value and store more carbon relative to shorter and smaller trees in the stand.\nMany remote sensing papers that use it seem to point to a paper written by Naesset in 1997 that used it to quantify tree canopy heights from lidar data collected in Norway. It’s a variable that is provided in many spatial databases, such as from lidar collects in the US and Canada.\nThis post describes the calculation of Lorey’s height using data collected from the US national forest inventory program in New York State. It then compares this definition of top height to other common ones used in forestry."
  },
  {
    "objectID": "post/2023-04-21-lorey-s-height-the-remote-sensing-way-to-estimate-tree-height/index.html#calculating-loreys-height",
    "href": "post/2023-04-21-lorey-s-height-the-remote-sensing-way-to-estimate-tree-height/index.html#calculating-loreys-height",
    "title": "Lorey’s height: the remote sensing way to estimate tree height",
    "section": "Calculating Lorey’s height",
    "text": "Calculating Lorey’s height\nLorey’s height (\\(h_L\\)) can be defined as:\n\\[h_L=\\frac{\\sum{n_ig_ih_i}}{\\sum{n_ig_i}}\\]\nwhere\n\n\\(n_i\\) is the number of trees in the ith diameter class,\n\\(g_i\\) is the average basal area of the ith diameter class, and\n\\(h_i\\) is the average height of trees in the ith diameter class.\n\nAs an example, we can put together a small data set named tree where we can calculate Lorey’s height. Assume these trees were sample from a single fixed-radius plot that is 1/5th-acre in size. The tribble() function from the tidyverse package can be used for this:\n\nlibrary(tidyverse)\n\n\ntree &lt;- tribble(\n  ~treeid, ~dbh, ~ht,\n  1, 4.5,   30.8,\n  2, 4.6,   32.7,\n  3, 11.7, 62.8,\n  4, 13.8, 66.9,\n  5, 4.7,   29.3,\n  6, 7.3,   51.3,\n  7, 14.5,  65.5,\n  8, 9.6, 58.9,\n  9, 12.1, 67.5,\n  10, 16.9, 75.8,\n  11, 11.9, 69,\n  12, 13.2, 65.1,\n  13, 12,   63.3,\n  14, 14.2, 68.2,\n  15, 6.3, 47.9,\n  16, 4.9, 32.4,\n  17, 8.1, 49.2,\n  18, 9.1, 48.1,\n  19, 11.2, 48,\n  20, 9.9, 49.5,\n  21, 5.7, 26.8,\n  22, 12.9, 58.7,\n  23, 5.8, 35.4,\n  24, 4.9, 35.4,\n  25, 24,   89.6,\n  26, 6.2, 47.4,\n  27, 5.9, 54.4,\n  28, 4.5, 40.3\n  )\n\nWe can define the expansion factor (expf = 5) and then determine Lorey’s mean height. The denominator of the calculation is the basal area in the stand:\n\nexpf &lt;- 5\n\ntree %&gt;% \n  mutate(ba_tree = (0.00545415*dbh^2)*expf,\n         lorey_num = (0.00545415*dbh^2)*ht*expf) %&gt;% \n  summarize(BA = sum(ba_tree),\n            LOREY_HT = sum(lorey_num) / BA)\n\n# A tibble: 1 × 2\n     BA LOREY_HT\n  &lt;dbl&gt;    &lt;dbl&gt;\n1  87.3     65.7\n\n\nAs an aside, the sitreeE package in R also has a function for determining Lorey’s height:\n\n# install.packages(\"sitreeE\")\n\nlibrary(sitreeE)\n\nBA2 &lt;- pi*(tree$dbh/2)^2\n\nLOREY_HT2 &lt;- lorey.height(BA2, tree$ht)\n\nprint(LOREY_HT2)\n\n[1] 65.69421\n\n\nSo, we can conclude that this stand has a mean Lorey’s height of 65.7 feet."
  },
  {
    "objectID": "post/2023-04-21-lorey-s-height-the-remote-sensing-way-to-estimate-tree-height/index.html#fia-data-from-new-york-state",
    "href": "post/2023-04-21-lorey-s-height-the-remote-sensing-way-to-estimate-tree-height/index.html#fia-data-from-new-york-state",
    "title": "Lorey’s height: the remote sensing way to estimate tree height",
    "section": "FIA data from New York State",
    "text": "FIA data from New York State\nFor an example data set, let’s query data from the US Forest Inventory and Analysis program collected in New York State. This is an excellent data set to use in this application because tree heights are measure on all sized trees in a network of permanent sample plots. There are approximately 3,200 forested plots in the New York FIA data.\nWe can read in the data using the rFIA package. Specifically, we want to acquire the TREE and PLOT tables (uncomment to run code_:\n\nlibrary(rFIA)\n\n\n# nyfia &lt;- getFIA(states = 'NY', \n#                tables = c(\"TREE\", \"PLOT\"),\n#                dir = 'your/path/goes/here')\n\nThe plotFIA() function maps the approximate locations of all plots:\n\nplotFIA(nyfia)"
  },
  {
    "objectID": "post/2023-04-21-lorey-s-height-the-remote-sensing-way-to-estimate-tree-height/index.html#section-1",
    "href": "post/2023-04-21-lorey-s-height-the-remote-sensing-way-to-estimate-tree-height/index.html#section-1",
    "title": "Lorey’s height: the remote sensing way to estimate tree height",
    "section": "",
    "text": "Next, we want to filter the data to only live trees (STATUSCD = 1) with a diameter at breast height (DIA) greater than 5.0 inches. I’ll also filter the data to obtain the most recent measurement of trees. In New York, this equates to all trees measured since about 2013:\n\nnytree &lt;- nyfia$TREE %&gt;% \n  filter(STATUSCD == 1 & DIA &gt;= 5.0 & INVYR &gt;= 2013)\n\nNext, we can calculate the number of trees, basal area, and Lorey’s height for all New York plots. The PLT_CN variable records each plot identifier:\n\nnyplot &lt;- nytree %&gt;% \n  mutate(ba_tree = (0.00545415*DIA^2)*TPA_UNADJ,\n         lorey_num = (0.00545415*DIA^2)*ACTUALHT*TPA_UNADJ) %&gt;% \n  group_by(PLT_CN) %&gt;% \n  summarize(num_trees = n(),\n            BA = sum(ba_tree),\n            LOREY_HT = sum(lorey_num) / sum(ba_tree))\nnyplot\n\n# A tibble: 3,224 × 4\n    PLT_CN num_trees     BA LOREY_HT\n     &lt;dbl&gt;     &lt;int&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 1.58e13        57 145.       59.7\n 2 1.58e13        29 121.       61.2\n 3 1.58e13        13  24.7      61.3\n 4 1.58e13        30 128.       66.6\n 5 1.58e13         6   9.69     43.2\n 6 1.58e13         6  24.6      65.2\n 7 1.58e13        26 147.       68.2\n 8 1.58e13        35  94.4      58.4\n 9 1.58e13        25  98.5      73.9\n10 1.58e13        35 166.       63.5\n# ℹ 3,214 more rows\n\n\nWe can visualize the distribution of Lorey’s height with a violin plot. We can see that the heights are approximately normal distributed, albeit slightly right-skewed:\n\nggplot(nyplot, aes(x = LOREY_HT, y = 1)) +\n  geom_violin(draw_quantiles = c(0.5)) +\n  labs(x = \"Lorey's height (feet)\") +\n  theme(axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())"
  },
  {
    "objectID": "post/2023-04-21-lorey-s-height-the-remote-sensing-way-to-estimate-tree-height/index.html#comparing-loreys-height-to-other-top-height-definitions",
    "href": "post/2023-04-21-lorey-s-height-the-remote-sensing-way-to-estimate-tree-height/index.html#comparing-loreys-height-to-other-top-height-definitions",
    "title": "Lorey’s height: the remote sensing way to estimate tree height",
    "section": "Comparing Lorey’s height to other top height definitions",
    "text": "Comparing Lorey’s height to other top height definitions\nIn their text, Kershaw et al. 2016 describe three other approaches to determine top height. In addition to Lorey’s height, these include:\n\nThe arithmetic mean height of all trees in a stand. This is a non-weighted measure of stand height and considers trees of all sizes in a stand (including ones in the lower canopy).\nThe average height of dominant and co-dominant trees, only. This is the height calculation I first learned as a forestry student because it’s typically used in site index equations in North America.\nThe average height of the largest diameter trees (or tallest trees). Some examples include selecting the largest 20% of trees based on diameter, or the tallest 50 trees per unit area.\n\nWe may be interested in comparing how these three different metric of top height compare to Lorey’s height. The arithmetic mean height simply averages all heights in a stand. The average height of dominants and co-dominants can be determined by querying FIA’s CCLCD variable, a designation of the crown class code of each tree. (We’ll filter all trees with a CCLCD of 2 and 3 which represent dominant and co-dominant trees, respectively.) For the average height of the largest diameter trees, let’s take the largest diameter trees found in the top 20th percentile (or 80th quantile):\n\nnyplot_q &lt;- nytree %&gt;% \n  group_by(PLT_CN) %&gt;% \n  summarize(pctl_80 = quantile(DIA, p = 0.8))\n\nnyplot &lt;- nytree %&gt;% \n  inner_join(nyplot_q) %&gt;% \n  mutate(ba_tree = (0.00545415*DIA^2)*TPA_UNADJ,\n         lorey_num = (0.00545415*DIA^2)*ACTUALHT*TPA_UNADJ,\n         ACTUALHT_dom_codom = ifelse(CCLCD %in% c(2,3), ACTUALHT, NA),\n         ACTUALHT_pctl_80 = ifelse(DIA &gt;= pctl_80, ACTUALHT, NA)) %&gt;% \n  group_by(PLT_CN) %&gt;% \n  summarize(num_trees = n(),\n            BA = sum(ba_tree),\n            LOREY_HT = sum(lorey_num) / sum(ba_tree),\n            ARITH_HT = mean(ACTUALHT),\n            DOM_CODOM_HT = mean(ACTUALHT_dom_codom, na.rm = T),\n            LARGE_HT = mean(ACTUALHT_pctl_80, na.rm = T))\n\nnyplot\n\n# A tibble: 3,224 × 7\n    PLT_CN num_trees     BA LOREY_HT ARITH_HT DOM_CODOM_HT LARGE_HT\n     &lt;dbl&gt;     &lt;int&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n 1 1.58e13        57 145.       59.7     56.1         62.0     65.9\n 2 1.58e13        29 121.       61.2     50           60.6     71.2\n 3 1.58e13        13  24.7      61.3     58.1         60.4     65.7\n 4 1.58e13        30 128.       66.6     59.3         61.2     71.8\n 5 1.58e13         6   9.69     43.2     44           44.2     43  \n 6 1.58e13         6  24.6      65.2     56.2         74       60  \n 7 1.58e13        26 147.       68.2     62.8         71.4     70.7\n 8 1.58e13        35  94.4      58.4     54.9         60.6     65.4\n 9 1.58e13        25  98.5      73.9     68.1         75.6     81.2\n10 1.58e13        35 166.       63.5     55.0         73.6     69.7\n# ℹ 3,214 more rows\n\n\nLet’s plot the distributions of the different top height definitions:\n\nnyplot_long &lt;-nyplot %&gt;% \n  pivot_longer(LOREY_HT:LARGE_HT, names_to = \"Top height definition\", \n               values_to = \"Height (feet)\")\n\nggplot(nyplot_long, aes(x = `Height (feet)`, \n                        y = `Top height definition`)) +\n  geom_violin(draw_quantiles = c(0.5)) +\n  labs(x = \"Top height (feet)\") +\n  scale_y_discrete(labels = c(\"Arithmetic mean\", \"Dominant/co-dominant\", \n                              \"Largest 20% diameter\", \"Lorey's\"))\n\n\n\n\n\n\n\n\nThe definition of average height of the largest diameter trees produces the greatest median height, followed by the average height of dominants and co-dominants, Lorey’s height, and then the arithmetic mean height.\nWe can visualize the correlations between these different top height definitions. The GGally package has a ggpairs() function that produces a sharp-looking correlation matrix with scatter plots, densities, and Pearson correlation coefficients:\n\nlibrary(GGally)\n\n\nny_plot_ht &lt;- nyplot[,4:7]\n\nggpairs(ny_plot_ht, \n        title = \"Correlation of top height definitions from New York FIA data\")\n\n\n\n\n\n\n\n\nAll of the top height definitions provide a correlation in excess of 0.9. The correlation is lowest between arithmetic mean height and the average height of the largest diameter trees (r = 0.907). The correlation is highest between Lorey’s height and the average height of the largest diameter trees (r = 0.976)."
  },
  {
    "objectID": "post/2023-04-21-lorey-s-height-the-remote-sensing-way-to-estimate-tree-height/index.html#conclusion",
    "href": "post/2023-04-21-lorey-s-height-the-remote-sensing-way-to-estimate-tree-height/index.html#conclusion",
    "title": "Lorey’s height: the remote sensing way to estimate tree height",
    "section": "Conclusion",
    "text": "Conclusion\nWith the increasing use of remote sensing in forestry, Lorey’s height will likely continue to be a popular choice of representing average stand height in the future. Lorey’s height is advantageous because it weights average height by tree size (basal area) and is equal to the mean height if horizontal point sampling is used. It’s useful to understand how different stand height definitions can result in different estimates of growth and productivity when used in forestry applications.\n–\nBy Matt Russell. Email Matt with any questions or comments."
  },
  {
    "objectID": "post/2023-03-21-how-much-of-maine-s-forests-are-enrolled-in-carbon-programs/index.html#section",
    "href": "post/2023-03-21-how-much-of-maine-s-forests-are-enrolled-in-carbon-programs/index.html#section",
    "title": "How much of Maine’s forests are enrolled in carbon programs?",
    "section": "",
    "text": "Carbon storage and sequestration is one of the most important attributes that forests provide. Carbon has quickly become a mechanism to address current and future climate challenges.\nForest carbon projects have traditionally relied on compliance markets such as California Air Resources Board’s Cap and Trade Regulation program and their associated Forest Offset Protocol. Increasingly, new carbon programs developed in the voluntary markets are offering the ability for smaller landowners to enroll their lands in the forest carbon marketplace.\nIn two different conversations in the last year, forestry professionals have asked me if there is a data layer that exists that contains all properties enrolled in a forest carbon project. While that data set would cost considerable time and effort to put together (notwithstanding the data privacy concerns with many properties enrolled in voluntary markets), it would provide an excellent overview of forest carbon program enrollment in a spatial context.\nForest industry professionals are especially interested in knowing where forest carbon projects are located. This would enable them to know where timber harvest restrictions associated with forest carbon program enrollment might be across a region where timber is supplied.\nI was curious to know how much of Maine’s forestland is enrolled in forest carbon projects. I searched the web and several climate registries to identify the owners, project developers, number of acres enrolled, and general location. I found most helpful this webinar hosted by Maine Woodland Owners to identify Maine forest properties that are currently enrolled in forest carbon programs.\nI’ve compiled the data set which you can find here and provided a snapshot of properties enrolled in carbon markets in Maine which you can find below. The size of each point relates to the acreage of each property:\n\n\n\n\n\n\n\n\n\nIn total, approximately 531,000 acres of Maine forestlands are enrolled in forest carbon programs. In a state with 17.5 million acres of forests, this means that approximately 3% of Maine forests are currently enrolled in a forest carbon program.\n–\nBy Matt Russell. NOTE: I’ve done my best to compile this list of properties in Maine that are enrolled in carbon programs, but it may be incomplete. Email me with any properties you’re aware of that can be added to the list."
  },
  {
    "objectID": "post/2023-02-09-calculating-carbon-in-standing-dead-trees/index.html#section",
    "href": "post/2023-02-09-calculating-carbon-in-standing-dead-trees/index.html#section",
    "title": "Calculating carbon in standing dead trees",
    "section": "",
    "text": "Quantifying the presence and abundance of standing dead trees (SDTs) is notoriously difficult. While live and healthy trees generally appear similar in form and appearance to other live and healthy trees, no two standing dead trees are alike.\nTrees that recently died within a few months can have most of their tops and limbs intact. Their foliage may even still be attached. Trees that have been standing dead for a decade or more can only have a portion of the bole remaining with the rest already decaying on the forest floor.\nThese attributes make quantifying the carbon found within SDTs incredibly complex. The popular Jenkins biomass equations were developed for live trees, and cannot be applied to SDTs directly. If you’re only using common forest inventory measurements like species and diameter, there’s really no way to quantify the difference in the amount of carbon in a tree that recently died to one that has been standing dead for decades.\nFortunately, most inventories that measure dead wood will collect data on the stage of decay of SDTs. These data are needed to refine estimates of SDT biomass and carbon by including structural loss and decay adjustments.\nIn this post I implement a method for quantifying the amount of carbon in standing dead trees using R, drawing especially from functions in the tidyverse package. The approach presented here follows the component ratio method approach for determining carbon developed by the USDA Forest Service. A few key resources that go through the methods of this approach:\n\nAccounting for density reduction and structural loss in standing dead trees: implications for forest biomass and carbon stock estimates in the United States by Domke et al. 2011. This paper describes the method and how it impacts the Forest Inventory and Analysis data.\nAdditional file 1 from the paper (PDF). The supplemental file presents equations the equations used estimate above and belowground SDT biomass and carbon and provides an example calculation."
  },
  {
    "objectID": "post/2023-02-09-calculating-carbon-in-standing-dead-trees/index.html#standing-dead-tree-data",
    "href": "post/2023-02-09-calculating-carbon-in-standing-dead-trees/index.html#standing-dead-tree-data",
    "title": "Calculating carbon in standing dead trees",
    "section": "Standing dead tree data",
    "text": "Standing dead tree data\nI compiled a few records of standing dead trees from four Forest Inventory and Analysis plots located in Maine. The four plots were measured in 2021 and the data set contains 10 SDTs. Here is the data that can be read into R with the tribble() function from the tidyverse package:\n\nlibrary(tidyverse)\n\n\ntree &lt;- tribble(\n  ~STATECD, ~COUNTYCD, ~PLOT, ~TREE, ~SPCD, ~DIA, ~ACTUALHT, ~DECAYCD,\n  23, 29, 130, 6, 95, 6.7, 27, 3,\n  23, 29, 130, 9, 95, 5.6, 9, 2,\n  23, 29, 688, 1, 12, 7.9, 35, 1,\n  23, 29, 688, 6, 12, 8.6, 31, 1,\n  23, 29, 701, 1, 371, 11, 17, 5,\n  23, 29, 701, 6, 531, 5.5, 49, 1,\n  23, 29, 846, 8, 241, 13.6, 26, 2,\n  23, 29, 846, 12, 12, 6.2, 10, 4,\n  23, 29, 846, 17, 12, 6.8, 5, 4,\n  23, 29, 846, 23, 12, 5.1, 29, 2,\n)\n\nThe variables are:\n\nSTATECD: Numeric state ID code (23 = Maine),\nCOUNTYCD: Numeric county ID code (29 = Washington County),\n\nPLOT: Numeric plot ID code,\n\nTREE: Numeric tree ID code,\n\nSPCD: Numeric FIA tree species code,\n\nDIA: Tree diameter (inches),\nACTUALHT: Height of the tree (feet), from ground level to the highest remaining portion of the tree,\nDECAYCD: Numeric code indicating the stage of decay in a standing dead tree; ranges from 1 (all limbs and branches are present) to 5 (no evidence of branches remains; top is broken)"
  },
  {
    "objectID": "post/2023-02-09-calculating-carbon-in-standing-dead-trees/index.html#calculating-volume-biomass-and-carbon-in-standing-dead-trees",
    "href": "post/2023-02-09-calculating-carbon-in-standing-dead-trees/index.html#calculating-volume-biomass-and-carbon-in-standing-dead-trees",
    "title": "Calculating carbon in standing dead trees",
    "section": "Calculating volume, biomass and carbon in standing dead trees",
    "text": "Calculating volume, biomass and carbon in standing dead trees\n\nTree volume\nThe first step to calculating the amount of carbon stored in SDTs is to determine their volume. In your region, there are likely regional volume equations that are used for the species you’re interested in. For this example, I’ll use equations developed by Honer in 1967 which are widely used for commercial species in the northeastern US and Canada.\nThe Honer equations predict volume in cubic feet and rely on species, diameter at breast height (DBH; measured in inches) and total tree height (HT; measured in feet):\n\\[ VOL = \\frac{DBH^2}{a = \\frac{b}{HT}}\\] where a and b are species-specific coefficients.\nHere is an R function to determine the volumes of the five species of SDTs in the data set:\n\nfx_HONER_VOL &lt;- function(SPCD, DBH, HT){\n       if(SPCD == 12){a = 2.139; b = 301.634}   # balsam fir\n  else if(SPCD == 95){a = 1.588; b = 333.364}   # black spruce\n  else if(SPCD == 241){a = 4.167; b = 244.906}  # northern white-cedar\n  else if(SPCD == 371){a = 1.449; b = 344.754}  # yellow birch\n  else if(SPCD == 531){a = 0.959; b = 334.829}  # American beech\n  VOL = DBH**2/(a+(b/HT))\n  return(VOL)\n  }\n\nNow, we can estimate the total volume of SDTs by applying the function to the tree data set. I use the mutate() function along with mapply() to create the new variable VOL:\n\ntree &lt;- tree %&gt;% \n  mutate(VOL = mapply(fx_HONER_VOL, SPCD = SPCD, DBH = DIA, HT = ACTUALHT))\n\nA graph of the tree diameter-volume trends highlights the data. Note that because many of the SDTs may have a broken top, trees can be a large diameter with relatively small volume, which adds some “noise” to the relationship:\n\nggplot(tree, aes(x = DIA, y = VOL, col = factor(SPCD))) +\n  geom_point() +\n  labs(x = \"Diameter (in.)\",\n       y = \"Volume (cu.ft.)\")\n\n\n\n\n\n\n\n\n\n\nSpecies data\nTo determine biomass and carbon attributes of dead trees, the species reference data set (REF_SPECIES.csv) provided by the FIA program is a must. This file contains species-specific values of wood density, decay ratios, and more. We’ll read the .csv file in from the USDA website, then select the core variables needed to determine biomass and carbon for SDTs:\n\nref_spp &lt;- read_csv(\"C://Users//matt//Documents//Arbor//Projects//FFCP//Central Apps v1//Data//REF_SPECIES.csv\") %&gt;% \n  select(SPCD, WOOD_SPGR_GREENVOL_DRYWT, BARK_SPGR_GREENVOL_DRYWT, BARK_VOL_PCT,\n         WOOD_SPGR_MC12VOL_DRYWT, SFTWD_HRDWD, JENKINS_TOTAL_B1,JENKINS_TOTAL_B2,\n         STANDING_DEAD_DECAY_RATIO1, STANDING_DEAD_DECAY_RATIO2, \n         STANDING_DEAD_DECAY_RATIO3,\n         STANDING_DEAD_DECAY_RATIO4, STANDING_DEAD_DECAY_RATIO5,\n         RAILE_STUMP_DOB_B1, RAILE_STUMP_DIB_B1, RAILE_STUMP_DIB_B2)\n\nNext, we’ll join the ref_spp data set to the tree data set:\n\ntree &lt;- inner_join(tree, ref_spp, by = \"SPCD\")\n\nNow, we’ll modify some of the species-specific factors from the reference species table. The STANDING_DEAD_DECAY_RATIO variable contains the ratio of decayed wood density to non-decayed wood density. It’s stored in five different variables for each of the five different decay classes, so the code below will turn it into a data set organized in a long format:\n\nref_spp_dead &lt;- ref_spp %&gt;% \n  select(SPCD, STANDING_DEAD_DECAY_RATIO1, STANDING_DEAD_DECAY_RATIO2, \n         STANDING_DEAD_DECAY_RATIO3, STANDING_DEAD_DECAY_RATIO4, \n         STANDING_DEAD_DECAY_RATIO5) %&gt;% \n  pivot_longer(STANDING_DEAD_DECAY_RATIO1:STANDING_DEAD_DECAY_RATIO5, \n               names_to = \"DECAY_NAME\", \n               values_to = \"STANDING_DEAD_DECAY_RATIO\") %&gt;% \n  mutate(DECAYCD = as.numeric(str_sub(DECAY_NAME, 26, 26))) %&gt;% \n  select(-DECAY_NAME) %&gt;% \n  relocate(SPCD, DECAYCD, STANDING_DEAD_DECAY_RATIO)\n\nFor example, here are the decay ratios for standing dead trees for the five decay classes for balsam fir:\n\nprint(ref_spp_dead[ref_spp_dead$SPCD == 12, ])\n\n# A tibble: 5 × 3\n   SPCD DECAYCD STANDING_DEAD_DECAY_RATIO\n  &lt;dbl&gt;   &lt;dbl&gt;                     &lt;dbl&gt;\n1    12       1                      1.04\n2    12       2                      1.15\n3    12       3                      1.10\n4    12       4                      0.71\n5    12       5                      0.71\n\n\n\n\nStructural loss adjustments\nWhile decay ratios account for decay and decomposition of wood, they don’t account for structural losses. Standing dead trees experience sloughing and breakage as a result of wind damage, animal activity, and many more issues. For this, structural loss adjustments are needed to accurately determine the biomass and carbon stored in SDTs.\nThe structural loss adjustments can be obtained in Table 2 of the Domke et al. paper. These decay-class-specific adjustments are for the bole, bark, tops and limbs, stumps, and roots. The table sla produces these values. We can then merge the sla table with the ref_spp_dead table, then merge that to the tree table:\n\nsla &lt;- tribble(\n  ~DECAYCD, ~BOLE_SLA, ~BARK_SLA, ~TOP_SLA, ~STUMP_SLA, ~ROOTS_SLA,\n  1, 1, 0.92, 1, 1, 1,\n  2, 1, 0.66, 0.5, 1, 0.95,\n  3, 1, 0.39, 0.20, 1, 0.80,\n  4, 1, 0.21, 0.10, 1, 0.65,\n  5, 1, 0, 0, 1, 0.50)\n\nref_spp_dead &lt;- left_join(ref_spp_dead, sla, by = \"DECAYCD\")\n\ntree &lt;- left_join(tree, ref_spp_dead, by = c(\"SPCD\", \"DECAYCD\")) \n\nPhew! That was a lot of work and we haven’t even begun to calculate biomass yet. No fear, now we have all of the variables required in the tree data set to begin the biomass and carbon calculations."
  },
  {
    "objectID": "post/2023-02-09-calculating-carbon-in-standing-dead-trees/index.html#determing-component-biomass-in-standing-dead-trees",
    "href": "post/2023-02-09-calculating-carbon-in-standing-dead-trees/index.html#determing-component-biomass-in-standing-dead-trees",
    "title": "Calculating carbon in standing dead trees",
    "section": "Determing component biomass in standing dead trees",
    "text": "Determing component biomass in standing dead trees\n\nStem and bark\nWe start by calculating the biomass in wood found the the stem of the tree (\\(B_{ODW}\\); kg). We will also need the specific gravity of wood for that species, i.e., the WOOD_SPGR_GREENVOL_DRYWT variable from the reference species file and W, the weight of one cubic meter of water (1,000 kg). The structural loss adjustment for the bole (BOLE_SLA) is multiplied by these values: \\[B_{ODW} = VOL\\times \\text{WOOD_SPGR_GREENVOL_DRYWT}\\times \\\\ W \\times \\text{STANDING_DEAD_DECAY_RATIO}\\times \\text{BOLE_SLA}\\]\nThe biomass found in the bark of the tree is the next component to predict(\\(B_{ODB}\\); kg). For this we’ll also need to know the species-specific values for the percent of wood volume that is bark (BARK_VOL_PCT), the specific gravity of bark (BARK_SPGR_GREENVOL_DRYWT), and the structural loss adjustment for the bark component (BARK_SLA):\n\\[B_{ODB} = VOL\\times \\text{BARK_VOL_PCT/100}\\times \\text{BARK_SPGR_GREENVOL_DRYWT} \\times \\\\ W \\times \\text{STANDING_DEAD_DECAY_RATIO}\\times \\text{BARK_SLA}\\] Total SDT bole and bark biomass (\\(B_{ODT}\\)) can be found by adding the two components:\n\\[B_{ODT} = B_{ODW} + B_{ODB}\\]\nBecause this approach uses the component ratio method (CRM) to determine biomass stored in SDTs, the next step is to calculate the biomass in each SDT and them compute the ratios. First we need to calculate the bole biomass (kg) from the Jenkins et al. equation (JENKINS_AG_TREE). These equations have different parameters for 10 species groups:\n\\[ \\text{JENKINS_AG_TREE} = exp(\\text{JENKINS_TOTAL_B1} + \\\\ \\text{JENKINS_TOTAL_B2}*log(DIA)) \\] The bole ratio from the Jenkins et al. 2003 paper (their Table 6) can be computed for softwood species as:\n\\[ \\text{BOLE_RATIO_SFWD} = exp(-0.3737 + (-1.8055/(DIA)) \\] and for hardwood species as:\n\\[ \\text{BOLE_RATIO_HDWD} = exp(-0.3065 + (-5.4240/(DIA)) \\]\nRatios of bark can be computed for softwood species as:\n\\[ \\text{BARK_RATIO_SFWD} = exp(-2.0980 + (-1.1432/(DIA)) \\]\nand for hardwood species as:\n\\[ \\text{BOLE_RATIO_HDWD} = exp(-2.0129 + (-5.4240/(DIA)) \\] Merchantable oven-dry biomass (MST) is then calculated as:\n\\[MST = (\\text{JENKINS_AG_TREE} * \\text{BOLE_RATIO}) + \\\\ (\\text{JENKINS_AG_TREE} * \\text{BARK_RATIO})\\]\nThen, the CRM adjustment factor (\\(CRM_{AdjFac}\\)) for each SDT can be calculated as:\n\\[ CRM_{AdjFac} = B_{ODT}/MST \\]\n\n\nStumps\nVolume of stumps is calculated next. These equations are adopted from Raile’s equations. Stump volume is calculated as:\n\\[ S_{VOL} = (\\frac{\\pi*DIA^2}{4*144})[((A-B)^2h+11B(A-B)ln(h+1)- \\\\ (\\frac{30.25}{h+1}B^2)]_a^b)\\times0.02832 \\]\nwhere:\n\nS_VOL is stump volume inside bark (\\(S_{VISB}\\)) or outside bark (\\(S_{VOSB}\\)),\nA and B are species-specific coefficients (RAILE_STUMP_DOB_B1, RAILE_STUMP_DIB_B1, and RAILE_STUMP_DIB_B2),\nh is the height above ground (ft),\na is the lower stump height (ft; typically set to 0), and\nb is the upper stump height (ft; typically set to one.\n\nThe biomass in the wood of stumps is calculated by multiplying the volume of stumps (inside bark) by the species specific gravity and the weight of water:\n\\[S_{ODSW} = S_{VISB}\\times \\text{WOOD_SPGR_GREENVOL_DRYWT} \\times W\\] A similar approach can be used for calculating the biomass in bark, this time subtracting the volume inside and outside the bark:\n\\[S_{ODSB} = (S_{VOSB}-S_{VISB})\\times \\text{BARK_SPGR_GREENVOL_DRYWT} \\times W\\] Total biomass in the stump (\\(S_{ODT}\\)) can be calculated by applying the \\(CRM_{AdjFac}\\) and the structural loss adjustment for the stump (\\(STUMP_{SLA}\\)):\n\\[S_{ODT} = (S_{ODSW} + S_{ODSB}) \\times CRM_{AdjFac} \\times STUMP_{SLA}\\]\n\n\nTops and limbs\nNext comes estimating the biomass in tops and limbs. Although foliage biomass is not calculated for SDTs, we calculate it to subtract it from total biomass. The foliage ratio is different for softwood and hardwood species and can be computed as:\n\\[ \\text{FOL_RATIO_SFWD} = exp(-2.9584 + (-4.4766/(DIA)) \\]\nfor softwood species and:\n\\[ \\text{FOL_RATIO_HDWD} = exp(-4.0813 + (-5.8816/(DIA)) \\]\nfor hardwood species. The top and branch biomass can be calculated from subtracting all other components and multiplying by the structural loss adjustment for tops and limbs:\n\\[T_{ODT} = (\\text{JENKINS_AG_TREE} - MST - (S_{ODSW} + S_{ODSB}) - \\\\ (\\text{JENKINS_AG_TREE} * \\text{FOL_RATIO})) * CRM_{AdjFac} * TOP_{SLA}\\]\n\n\nBelowground (roots)\nThe belowground (coarse roots) ratio from the Jenkins et al. 2003 paper (their Table 6) can be computed for softwood species as:\n\\[ \\text{ROOT_RATIO_SFWD} = exp(-1.5619 + (0.6614/(DIA)) \\] and for hardwood species as:\n\\[ \\text{ROOT_RATIO_HDWD} = exp(-1.6911 + (0.8160/(DIA)) \\]\nBelowground biomass (\\(BG_{ODT}\\)) can then be calculated as:\n\\[BG_{ODT} = (\\text{ROOT_RATIO} \\times \\text{JENKINS_AG_tree}) \\\\ \\times CRM_{AdjFac} \\times ROOTS_{SLA}\\]\n\n\nTotal standing dead tree biomass and carbon\nFinally, the total biomass of a standing dead tree, including aboveground and belowground components, can be calculated as:\n\\[\\text{BIOMASS_TOTAL_SDT} = B_{ODT} + S_{ODT} + T_{ODT} + BG_{ODT}\\]\nAnd we can multiply by 0.5, assuming half of biomass is carbon, to compute total carbon in a standing dead tree:\n\\[\\text{CARBON_TOTAL_SDT} = \\text{BIOMASS_TOTAL_SDT} \\times 0.5\\]"
  },
  {
    "objectID": "post/2023-02-09-calculating-carbon-in-standing-dead-trees/index.html#r-functions-to-determine-standing-dead-tree-biomass-and-carbon",
    "href": "post/2023-02-09-calculating-carbon-in-standing-dead-trees/index.html#r-functions-to-determine-standing-dead-tree-biomass-and-carbon",
    "title": "Calculating carbon in standing dead trees",
    "section": "R functions to determine standing dead tree biomass and carbon",
    "text": "R functions to determine standing dead tree biomass and carbon\nThe R functions that calculate these values are written below:\n\n# Stem wood biomass\nfx_B_ODW &lt;- function(VOL, WOOD_SPGR_GREENVOL_DRYWT, W, \n                    STANDING_DEAD_DECAY_RATIO, BOLE_SLA){\n  B_ODW = VOL*WOOD_SPGR_GREENVOL_DRYWT*W*STANDING_DEAD_DECAY_RATIO*BOLE_SLA\n  return(B_ODW)\n}\n\n# Bark biomass\nfx_B_ODB &lt;- function(VOL, BARK_VOL_PCT, W, BARK_SPGR_GREENVOL_DRYWT,\n                    STANDING_DEAD_DECAY_RATIO, BARK_SLA){\n  B_ODB = VOL*(BARK_VOL_PCT/100)*BARK_SPGR_GREENVOL_DRYWT*1000*\n    STANDING_DEAD_DECAY_RATIO*BARK_SLA\n  return(B_ODB)\n}\n\n# Total SDT biomass (stem wood and bark)\nfx_B_ODT &lt;- function(B_ODW, B_ODB){\n  B_ODT = B_ODW + B_ODB\n  return(B_ODT)\n}\n\n# Jenkins aboveground biomass\nfx_JENKINS_AG_TREE &lt;- function(JENKINS_TOTAL_B1, JENKINS_TOTAL_B2, DIA){\n  JENKINS_AG_TREE = exp(JENKINS_TOTAL_B1 + JENKINS_TOTAL_B2*log(DIA))\n  return(JENKINS_AG_TREE)\n}\n\n# Bole ratio\nfx_BOLE_RATIO &lt;- function(SFTWD_HRDWD, DIA){\n       if(SFTWD_HRDWD == \"S\"){a=-0.3737; b =-1.8055}\n  else if(SFTWD_HRDWD == \"H\"){a=-0.3065; b =-5.4240}\n  BOLE_RATIO = exp(a + (b/(DIA)))\n  return(BOLE_RATIO)\n}\n\n# Bark ratio\nfx_BARK_RATIO &lt;- function(SFTWD_HRDWD, DIA){\n       if(SFTWD_HRDWD == \"S\"){a=-2.0980; b =-1.1432}\n  else if(SFTWD_HRDWD == \"H\"){a=-2.0129; b =-1.6805}\n  BOLE_RATIO = exp(a + (b/(DIA)))\n  return(BOLE_RATIO)\n}\n\n# Merchantable oven-dry biomass\nfx_MST = function(JENKINS_AG_TREE, BOLE_RATIO, BARK_RATIO){\n  MST = (JENKINS_AG_TREE * BOLE_RATIO) + (JENKINS_AG_TREE * BARK_RATIO)\n  return(MST)\n}\n\n# Component ratio adjustment factor\nfx_CRM_AdjFac = function(B_ODT, MST){\n  CRM_AdjFac = B_ODT/MST\n  return(CRM_AdjFac)\n}\n\n# Stump volume outside bark,\n\nfx_STUMP_VOSB &lt;- function(DIA, RAILE_STUMP_DOB_B1){\n  STUMP_VOSB = (3.14159265*((DIA)**2))/(4*144)*(((1-RAILE_STUMP_DOB_B1)**2*1+11*\n                RAILE_STUMP_DOB_B1*(1-RAILE_STUMP_DOB_B1)*log(1+1)-30.25/(1+1)* \n                  RAILE_STUMP_DOB_B1**2)-((1-RAILE_STUMP_DOB_B1)**2*0+11*\n                                            RAILE_STUMP_DOB_B1*(1-RAILE_STUMP_DOB_B1)*\n                                            log(0+1)-30.25/(0+1)*RAILE_STUMP_DOB_B1**2))*0.0283168\n   return(STUMP_VOSB)\n} \n  \n# Stump volume inside bark\nfx_STUMP_VISB &lt;- function(DIA, RAILE_STUMP_DIB_B1, RAILE_STUMP_DIB_B2){\n  STUMP_VISB = (3.14159265*((DIA)**2))/(4*144)*(((RAILE_STUMP_DIB_B1-RAILE_STUMP_DIB_B2)**2*1+11*\n                 RAILE_STUMP_DIB_B2*(RAILE_STUMP_DIB_B1-RAILE_STUMP_DIB_B2)* log(1+1)-30.25/(1+1)*             \n                   RAILE_STUMP_DIB_B2**2)-((RAILE_STUMP_DIB_B1-RAILE_STUMP_DIB_B2)**2*0+11*                                                            RAILE_STUMP_DIB_B2*(RAILE_STUMP_DIB_B1-RAILE_STUMP_DIB_B2)*\n                                             log(0+1)-30.25/(0+1)*RAILE_STUMP_DIB_B2**2))*0.0283168\n   return(STUMP_VISB)\n} \n   \n# Stump wood biomass\nfx_S_ODSW &lt;- function(STUMP_VISB, WOOD_SPGR_GREENVOL_DRYWT, W){\n  S_ODSW = STUMP_VISB*WOOD_SPGR_GREENVOL_DRYWT*W\n  return(S_ODSW)\n}\n  \n# Stump bark biomass\nfx_S_ODSB &lt;- function(STUMP_VOSB, STUMP_VISB, BARK_SPGR_GREENVOL_DRYWT, W){\n  S_ODSB = (STUMP_VOSB - STUMP_VISB)*BARK_SPGR_GREENVOL_DRYWT*W\n  return(S_ODSB)\n}\n\n# Total stump biomass:\nfx_S_ODT &lt;- function(S_ODSW, S_ODSB, CRM_AdjFac, STUMP_SLA){\n  S_ODT = (S_ODSW + S_ODSB)*CRM_AdjFac*STUMP_SLA\n}\n\n# Foliage ratio\nfx_FOL_RATIO &lt;- function(SFTWD_HRDWD, DIA){\n       if(SFTWD_HRDWD == \"S\"){a=-2.9584; b =-4.4766}\n  else if(SFTWD_HRDWD == \"H\"){a=-4.0813; b =-5.8816}\n  FOL_RATIO = exp(a + (b/(DIA)))\n  return(FOL_RATIO)\n}\n\n# Top and branch biomass\nfx_T_ODT &lt;- function(JENKINS_AG_TREE, MST, S_ODSW, S_ODSB,\n                    FOL_RATIO, CRM_AdjFac, TOP_SLA){\n  T_ODT = (JENKINS_AG_TREE - MST - (S_ODSW + S_ODSB) - \n             (JENKINS_AG_TREE * FOL_RATIO)) * CRM_AdjFac * TOP_SLA\n  return(T_ODT)\n}\n\n# Coarse root ratio\nfx_ROOT_RATIO &lt;- function(SFTWD_HRDWD, DIA){\n       if(SFTWD_HRDWD == \"S\"){a=-1.5619; b =0.6614}\n  else if(SFTWD_HRDWD == \"H\"){a=-1.6911; b =0.8160}\n  ROOT_RATIO = exp(a + (b/(DIA)))\n  return(ROOT_RATIO)\n}\n      \n# Belowground biomass\nfx_ROOT_ODT &lt;- function(ROOT_RATIO, JENKINS_AG_TREE, CRM_AdjFac, ROOTS_SLA){\n  ROOT_ODT = (ROOT_RATIO*JENKINS_AG_TREE) * CRM_AdjFac * ROOTS_SLA\nreturn(ROOT_ODT)\n}\n\n# Total biomass\nfx_BIOMASS_TOTAL_SDT &lt;- function(B_ODT, S_ODT, T_ODT, ROOT_ODT){\n  BIOMASS_TOTAL_SDT = B_ODT + S_ODT + T_ODT + ROOT_ODT\nreturn(BIOMASS_TOTAL_SDT)\n}\n\n# Total carbon\nfx_CARBON_TOTAL_SDT &lt;- function(BIOMASS_TOTAL_SDT){\n  CARBON_TOTAL_SDT = BIOMASS_TOTAL_SDT * 0.5\nreturn(CARBON_TOTAL_SDT)\n}\n\nNow, we’ll apply the SDT functions to the tree data set. Note we’ll need to convert the volume from cubic feet to cubic meters by multiplying by 0.0283168, and change the diameter from inches/centimeters where needed:\n\ntree &lt;- tree %&gt;% \n  mutate(B_ODW = mapply(fx_B_ODW, \n                        VOL = VOL*0.0283168, \n                        WOOD_SPGR_GREENVOL_DRYWT = WOOD_SPGR_GREENVOL_DRYWT, \n                        W = 1000, \n                        STANDING_DEAD_DECAY_RATIO = STANDING_DEAD_DECAY_RATIO, \n                        BOLE_SLA = BOLE_SLA),\n         B_ODB = mapply(fx_B_ODB,\n                        VOL = (VOL*0.0283168),\n                        BARK_VOL_PCT = BARK_VOL_PCT,\n                        BARK_SPGR_GREENVOL_DRYWT = BARK_SPGR_GREENVOL_DRYWT,\n                        W = 1000,\n                        STANDING_DEAD_DECAY_RATIO = STANDING_DEAD_DECAY_RATIO,\n                        BARK_SLA = BARK_SLA),\n         B_ODT = mapply(fx_B_ODT, \n                        B_ODW = B_ODW,\n                        B_ODB = B_ODB),\n         JENKINS_AG_TREE = mapply(fx_JENKINS_AG_TREE,\n                                  JENKINS_TOTAL_B1 = JENKINS_TOTAL_B1, \n                                  JENKINS_TOTAL_B2 = JENKINS_TOTAL_B2, \n                                  DIA = DIA*2.54),\n          BOLE_RATIO = mapply(fx_BOLE_RATIO, \n                              SFTWD_HRDWD = SFTWD_HRDWD, \n                              DIA = DIA*2.54),\n         BARK_RATIO = mapply(fx_BARK_RATIO, \n                             SFTWD_HRDWD = SFTWD_HRDWD, \n                             DIA = DIA*2.54),\n         MST = mapply(fx_MST,\n                      JENKINS_AG_TREE = JENKINS_AG_TREE,\n                      BOLE_RATIO = BOLE_RATIO,\n                      BARK_RATIO = BARK_RATIO),\n         CRM_AdjFac = mapply(fx_CRM_AdjFac,\n                             B_ODT = B_ODT,\n                             MST = MST),\n         STUMP_VOSB = mapply(fx_STUMP_VOSB,\n                             DIA = DIA,\n                             RAILE_STUMP_DOB_B1 = RAILE_STUMP_DOB_B1),\n         STUMP_VISB = mapply(fx_STUMP_VISB,\n                             DIA = DIA,\n                             RAILE_STUMP_DIB_B1 = RAILE_STUMP_DIB_B1,\n                             RAILE_STUMP_DIB_B2 = RAILE_STUMP_DIB_B2),\n         S_ODSW = mapply(fx_S_ODSW,\n                          STUMP_VISB = STUMP_VISB,\n                          WOOD_SPGR_GREENVOL_DRYWT = WOOD_SPGR_GREENVOL_DRYWT,\n                          W = 1000),\n         S_ODSB = mapply(fx_S_ODSB,\n                         STUMP_VOSB = STUMP_VOSB,\n                         STUMP_VISB = STUMP_VISB,\n                         BARK_SPGR_GREENVOL_DRYWT = BARK_SPGR_GREENVOL_DRYWT,\n                         W = 1000),\n         S_ODT = mapply(fx_S_ODT,\n                        S_ODSW = S_ODSW,\n                        S_ODSB = S_ODSB,\n                        CRM_AdjFac = CRM_AdjFac,\n                        STUMP_SLA = STUMP_SLA),\n         FOL_RATIO = mapply(fx_FOL_RATIO,\n                            SFTWD_HRDWD = SFTWD_HRDWD, \n                            DIA = DIA*2.54),\n         T_ODT = mapply(fx_T_ODT,\n                        JENKINS_AG_TREE = JENKINS_AG_TREE,\n                        MST = MST,\n                        S_ODSW = S_ODSW,\n                        S_ODSB = S_ODSB,\n                        FOL_RATIO = FOL_RATIO,\n                        CRM_AdjFac = CRM_AdjFac,\n                        TOP_SLA = TOP_SLA),\n         ROOT_RATIO = mapply(fx_ROOT_RATIO,\n                            SFTWD_HRDWD = SFTWD_HRDWD, \n                            DIA = DIA*2.54),\n         ROOT_ODT = mapply(fx_ROOT_ODT,\n                           ROOT_RATIO = ROOT_RATIO, \n                           JENKINS_AG_TREE = JENKINS_AG_TREE, \n                           CRM_AdjFac = CRM_AdjFac, \n                           ROOTS_SLA = ROOTS_SLA),\n         BIOMASS_TOTAL_SDT = mapply(fx_BIOMASS_TOTAL_SDT,\n                                    B_ODT = B_ODT,\n                                    S_ODT = S_ODT,\n                                    T_ODT = T_ODT,\n                                    ROOT_ODT = ROOT_ODT),\n         CARBON_TOTAL_SDT = mapply(fx_CARBON_TOTAL_SDT,\n                                   BIOMASS_TOTAL_SDT = BIOMASS_TOTAL_SDT))\n\nNow we can plot the diameter-carbon trends, and we end up with a graph that looks similar to to the volume-diameter trends from before:\n\nggplot(tree, aes(x = DIA, y = CARBON_TOTAL_SDT*2.20462, col = factor(SPCD))) +\n  geom_point() +\n  labs(x = \"Diameter (in.)\",\n       y = \"Total carbon (pounds)\")\n\n\n\n\n\n\n\n\nWe may be interested in a plot-level summary of SDT carbon. The code below summarizes the per-acre values of SDT carbon, by converting to pounds and multiplying by the tree’s expansion factor (In the FIA data, one tree sampled represents approximately six trees sampled on a per acre basis):\n\ndead_sum &lt;- tree %&gt;% \n  mutate(CARBON_TOTAL_SDT_part = CARBON_TOTAL_SDT*2.20462*6.018046) %&gt;% \n  group_by(STATECD, COUNTYCD, PLOT) %&gt;% \n  summarize(CARBON_TOTAL_SDT_lbs_ac = sum(CARBON_TOTAL_SDT_part))\n\nAs we can see, we have anywhere between 0.2 and 0.7 tons per acre of standing dead tree carbon from these four inventory plots:\n\nggplot(dead_sum, aes(x = factor(PLOT), y = CARBON_TOTAL_SDT_lbs_ac/2000)) +\n  geom_bar(stat = \"identity\") +\n  labs(x = \"PLOT ID\",\n       y = \"Total carbon (tons/ac)\")"
  },
  {
    "objectID": "post/2023-02-09-calculating-carbon-in-standing-dead-trees/index.html#other-notes",
    "href": "post/2023-02-09-calculating-carbon-in-standing-dead-trees/index.html#other-notes",
    "title": "Calculating carbon in standing dead trees",
    "section": "Other notes",
    "text": "Other notes\nA few other notes about determining the biomass and carbon in standing dead trees:\n\nDepending on the data collected on the SDTs from your inventory, there may also be measurements such as whether or not the stem is cull or rotten. After all, these characteristics are likely if the tree is dead. Some deductions in volume can be taken at the start before determining biomass and carbon.\nThe selection of which volume equation to use has a large influence on the amount of carbon. If using FIA data, the VOLCFSND variable is the one that’s used in the component ratio method. These are regionally specific equations that use either height or site index. If taper equations are available for your area, consider using those given the variability in heights (e.g., broken limbs) on standing dead trees. That’s an approach I applied on some SDT data in Maine. (At least the peer reviewers didn’t complain.)\nSome inventories, like FIA’s, collect a height measurement that assumes the tree has it’s top and limbs intact (e.g., the HT variable in FIA) and a true height (e.g., the ACTUALHT variable in FIA). When computing volume, biomass, and carbon of SDTs a true actual height is a better representation because it would incorporate broken stems.\nThese data and equations use a five-class decay class system. This approach can be adapted by refining the decay ratios and structural loss adjustments for trees in the region you work in.\nThe approach outlined here is similar to estimating the biomass and carbon in live trees. It’s the regional volume equations that are more complex and vary across the US.\nThese approaches often calculate terms in mixed English and metric units. See the Forest Carbon Cheat Sheet for a handy list on common units and conversion factors.\nThe USDA Forest Service will soon release updated equations for determining volume, biomass, and carbon of trees. No doubt this will also influence the calculation of standing dead trees.\n\n–\nBy Matt Russell. Sign up for my monthly newsletter for in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "post/2022-12-23-visualizing-stumpage-prices-for-sawlogs-in-maine-2019-2020/index.html",
    "href": "post/2022-12-23-visualizing-stumpage-prices-for-sawlogs-in-maine-2019-2020/index.html",
    "title": "Stumpage prices for sawlogs in Maine, 2019-2020",
    "section": "",
    "text": "Many states provide information on stumpage prices for commonly harvested timber species. This information provides land managers and landowners a sense of what current prices are for different timber species, products, and regions throughout a state.\nThe Maine Forest Service (MFS) collects data on timber sales through a notification process when timber sales occur. The last report from MFS contains over 1,600 reports from landowners that conducted a timber sale in 2020.\nThe information is used by MFS and the state to accurately determine land valuations and to estimate the amount of area harvested in a given year. Although markets can fluctuate quickly, the data is also useful to landowners and land managers to better understand a fair price for timber across different regions and product classes.\nThe 2020 stumpage report by MFS was published in March 2022 and contains statewide and county-level data for product classes (e.g., sawlogs, pulpwood, and palletwood) and species (e.g., oak, pine, maple). The report also contains values from the previous year to compare with current prices:\n\n\n\n\n\nStatewide stumpage prices in Maine, 2019 and 2020.\n\n\n\n\nHere we can see that red oak, sugar maple, and ash have the greatest sawlog prices for all species across the state. You will also note generally lower prices for sawlogs in 2020 compared to 2019 levels.\nFor species with a substantial number of reported values, the minimum and maximum values of stumpage prices are also provided. Here are the stumpage prices for sawlogs in 2020 for all Maine counties:\n\n\n\n\n\nStumpage prices by Maine county, 2019 and 2020. NOTE: Capital area includes Kennebec, Knox, Lincoln, and Waldo counties; Casco Bay includes Androscoggin, Cumberland, Sagadohoc, and York counties.\n\n\n\n\nForest data analysts can use data like these to understand market conditions for different species. If you’re interested in seeing the data set that contains these prices, I’ve made a spreadsheet available on Github (titled me_stumpage.csv).\n–\nHow do you use data like stumpage prices in your organization? I’m always curious to know, so let me know via email.\nBy Matt Russell. Sign up for my monthly newsletter for in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "post/2022-12-14-doing-statistics-in-the-tidyverse-exploring-the-infer-package/index.html#section",
    "href": "post/2022-12-14-doing-statistics-in-the-tidyverse-exploring-the-infer-package/index.html#section",
    "title": "Doing statistics in the tidyverse: exploring the infer package",
    "section": "",
    "text": "If you use R software for your data analysis, you are likely familiar with the tidyverse. The tidyverse package is a “megapackage” in R that includes several packages that import, reshape, and visualize data in a consistent manner, among other tasks.\nLike many analysts, I use the tidyverse suite of functions often for importing, wrangling, and visualizing data. More and more I’m using the tidymodels package to apply statistical modeling techniques like regression and random forests on data I work with.\nI’ve often wondered how to perform basic statistical tasks, like t-tests and simple linear regression, using a “tidy” approach. In writing code, I always found if clunky to mix writing a series of pipes (%&gt;%), common syntax in the tidyverse, with dollar signs ($), common syntax in base R that specifies a variable associated with a data set. This syntax-switching was always difficult to convey to students learning statistics. While we use one set of syntax for wrangling and visualizing data, we use a different syntax in the same programming language for performing statistical operations. It’s like trying to learn Latin American Spanish and Castilian Spanish at the same time.\nI recently read the excellent article An educator’s perspective of the tidyverse by Mine Çetinkaya-Rundel and colleagues that describes the infer package in R. The infer package performs statistical inference using grammar and syntax that is similar to the tidyverse design framework.\nI wanted to learn more about the doing statistical tasks in the tidyverse, so in this post I compare how the code written with the infer package compares to those available in base R. I use data sets from my book Statistics in Natural Resources: Applications with R, available in the stats4nr package."
  },
  {
    "objectID": "post/2022-12-14-doing-statistics-in-the-tidyverse-exploring-the-infer-package/index.html#one-sample-t-test-base-r-and-infer",
    "href": "post/2022-12-14-doing-statistics-in-the-tidyverse-exploring-the-infer-package/index.html#one-sample-t-test-base-r-and-infer",
    "title": "Doing statistics in the tidyverse: exploring the infer package",
    "section": "One-sample t-test: base R and infer",
    "text": "One-sample t-test: base R and infer\nFirst, I’ll load the packages to use throughout this comparison:\n\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(stats4nr)\n\nThe first statistical task I’ll perform is a one-sample t-test for a mean. I’ll use the chirps data set from the stats4nr package to perform a two-sided one-sample t-test at a level of significance of \\(\\alpha = 0.05\\). The data contain the number of chirps that a striped ground cricket makes each second (cps) at various temperatures:\n\nggplot(chirps, aes(cps)) +\n  geom_boxplot() +\n  labs(x = \"Chirps per second\")\n\n\n\n\n\n\n\n\nFor our statistical test, assume an entomologist makes a claim that a cricket makes 18 chirps per second. Our hypotheses are then:\n\nThe null hypothesis is that the true mean of the number of chirps a cricket makes is equal to 18.\nThe alternative hypothesis is that the true mean of the number of chirps a cricket makes is not equal to 18.\n\nIn base R, the t.test() function performs a number of hypothesis tests related to the t-distribution. We can write the code as:\n\nt.test(chirps$cps, mu = 18)\n\n\n    One Sample t-test\n\ndata:  chirps$cps\nt = -3.0643, df = 14, p-value = 0.008407\nalternative hypothesis: true mean is not equal to 18\n95 percent confidence interval:\n 15.71077 17.59589\nsample estimates:\nmean of x \n 16.65333 \n\n\nThe output provides the t-statistic, degrees of freedom, p-value, mean number of chirps, and the 95% confidence interval. The p-value of 0.008407 is less than our level of significance of \\(\\alpha = 0.05\\), hence, we can conclude that we have evidence to reject the null hypothesis and conclude that the true mean of the number of chirps a striped ground cricket makes is not equal to 18.\nWith the chirps data, we can use the t_test() wrapper function available in the infer package to provide similar calculations:\n\nt_test(chirps, response = cps, mu = 18)\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1     -3.06    14 0.00841 two.sided       16.7     15.7     17.6\n\n\nThe output is stored in a small data set (a “tibble”) that can be used in subsequent analyses. With infer, more information can be obtained from the analysis by specifying four primary functions, each which are expressed as verbs:\n\nspecify() lists the variable you want to examine,\nhypothesize() specifies the null hypothesis,\ngenerate()) produces data based on the null hypothesis, and\ncalculate() calculates a distribution of statistics based on the null hypothesis.\n\nWe can perform the same hypothesis test, this time by generating the null distribution using a bootstrap approach with 1,000 samples:\n\nchirps_null_dist &lt;- chirps %&gt;%\n  specify(response = cps) %&gt;% \n  hypothesize(null = \"point\", mu = 18) %&gt;% \n  generate(reps = 1000, type = \"bootstrap\") %&gt;% \n  calculate(stat = \"mean\")\n\nThis code produces a data set with 1,000 replicates each with a calculated test statistic (the mean). We can visualize the null distribution and test statistic using the visualize() function:\n\nobserved_test_stat &lt;- chirps %&gt;%\n  specify(response = cps) %&gt;%\n  calculate(stat = \"mean\")\n\nchirps_null_dist %&gt;%\n  visualize() + \n  shade_p_value(observed_test_stat,\n                direction = \"two-sided\")\n\n\n\n\n\n\n\n\nFrom the visualization, we can see that our observed mean of 16.7 chirps per second would be unlikely if the true mean was 18 chirps per second."
  },
  {
    "objectID": "post/2022-12-14-doing-statistics-in-the-tidyverse-exploring-the-infer-package/index.html#analysis-of-variance-base-r-and-infer",
    "href": "post/2022-12-14-doing-statistics-in-the-tidyverse-exploring-the-infer-package/index.html#analysis-of-variance-base-r-and-infer",
    "title": "Doing statistics in the tidyverse: exploring the infer package",
    "section": "Analysis of variance: base R and infer",
    "text": "Analysis of variance: base R and infer\nThe second statistical task I’ll perform is an analysis of variance. I’ll use the iron data set from the stats4nr package which contains iron levels measured at several water depths in Chesapeake Bay. Experimenters took three measurements at six water depths: 0, 10, 30, 40, 50, and 100 feet. The response variable was iron content, measured in mg/L.\nHere is the distribution of iron contents at the different water depths:\n\nggplot(iron, aes(factor(depth), iron)) + \n  geom_boxplot()+\n  ylab(\"Iron content (mg/L)\") +\n  xlab(\"Water depth (feet)\")\n\n\n\n\n\n\n\n\nFirst, we’ll convert the water depth variable (currently stored as a number) to a factor variable. This is because the water depths are labeled as numbers, but they represent categorical variables in our treatment of them in the ANOVA:\n\niron &lt;- iron %&gt;% \n  mutate(depth.fact = as.factor(depth))\n\nIn this ANOVA, our hypotheses are:\n\nThe null hypothesis is that there are no differences among the mean iron contents collected across the six water depths.\nThe alternative hypothesis is that there is at least one mean value of iron content that differs from the rest.\n\nIn base R, we can perform a one-way ANOVA with the lm() function and view the ANOVA table with the anova() function:\n\niron.aov &lt;- lm(iron ~ depth.fact, data = iron)\n\nanova(iron.aov)\n\nAnalysis of Variance Table\n\nResponse: iron\n           Df   Sum Sq   Mean Sq F value    Pr(&gt;F)    \ndepth.fact  5 0.064802 0.0129605  35.107 9.248e-07 ***\nResiduals  12 0.004430 0.0003692                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe output provides the ANOVA table with degrees of freedom, sums of squares, and mean squares. The F-statistic and p-value provide the results of the ANOVA. Here, we see that with a small p-value of the ANOVA F-test (9.248e-07), we reject the null hypothesis that all iron contents are equal and conclude that at least one mean differs from the rest.\nUsing the infer package, We can perform the ANOVA by generating the null distribution. We can permute the variables in the iron data set to match each water depth with the iron content levels:\n\niron_null_dist &lt;- iron %&gt;%\n  specify(iron ~ depth.fact) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 1000, type = \"permute\") %&gt;%\n  calculate(stat = \"F\")\n\nThen, we can calculate the p-value from the observed statistic and null distribution using the get_p_value() function:\n\nobserved_f_stat &lt;- iron %&gt;%\n  specify(iron ~ depth.fact) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  calculate(stat = \"F\")\n\niron_null_dist %&gt;%\n  get_p_value(obs_stat = observed_f_stat,\n              direction = \"greater\")\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an approximation\nbased on the number of `reps` chosen in the `generate()` step.\nℹ See `get_p_value()` (`?infer::get_p_value()`) for more information.\n\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0\n\n\nIn this case, a p-value of 0 is reported, which should be understood after seeing the “Please be cautious…” warning. This result is likely because the observed F-statistic is quite large and unlikely given the null hypothesis:\n\niron %&gt;%\n  specify(iron ~ depth.fact) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  calculate(stat = \"F\")\n\nResponse: iron (numeric)\nExplanatory: depth.fact (factor)\nNull Hypothesis: independence\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  35.1\n\n\nWe can visualize the null distribution and test statistic using the visualize() function to see that the F-statistic is unlikely given null hypothesis:\n\niron_null_dist %&gt;%\n  visualize() + \n  shade_p_value(observed_f_stat,\n                direction = \"greater\")\n\n\n\n\n\n\n\n\nIndeed, observing an F-statistic of 35.1 would be quite unlikely given the ANOVA hypothesis."
  },
  {
    "objectID": "post/2022-12-14-doing-statistics-in-the-tidyverse-exploring-the-infer-package/index.html#takeaways-from-the-infer-package",
    "href": "post/2022-12-14-doing-statistics-in-the-tidyverse-exploring-the-infer-package/index.html#takeaways-from-the-infer-package",
    "title": "Doing statistics in the tidyverse: exploring the infer package",
    "section": "Takeaways from the infer package",
    "text": "Takeaways from the infer package\nThere are many advantages for using the infer package for completing elementary statistical tasks and teaching them to students. Here are a few:\n\nThe package uses verbs as a part of it’s grammar. The user writes the code to do this action, a concept that is consistent within the tidyverse ecosystem.\nVisualization of statistical output is easy. I’ve often found that when teaching hypothesis testing, I ask students to “do the math”, then draw the distribution (even a simple sketch will do) to see where the observed statistic falls. This is essentially what the visualize() function does, and the red line in the graphs above is a handy comparison to the null distribution.\nFor those functions with a wrapper function, infer outputs statistics into a table which facilitates using them in subsequent analyses, particularly when multiple tests are run in an analysis.\n\nThere are some drawbacks to using and teaching introductory statistics with the infer package:\n\nThere is a need to write more lines of code to obtain output. There seems to be more output that can be provided with infer functions compared to base R, but you’ll need to write the code out. In contrast, it seems that many statistical functions in base R provide an overload of output that learners will need to sift through.\n\nThe package does not have a wrapper functions for some common statistical tests like ANOVA. This could mean more code to produce similar results that base R functions provide.\n\nIn summary, I’m glad there is a package that provides a “tidy” approach to performing statistical tests. The syntax and grammar used in the infer package is compatible with how many students are learning R today by using the tidyverse suite of functions and packages.\n–\nBy Matt Russell. Sign up for my monthly newsletter for in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "post/2022-10-20-how-to-spot-poor-data-analyses/index.html#section",
    "href": "post/2022-10-20-how-to-spot-poor-data-analyses/index.html#section",
    "title": "How to spot poor data analyses",
    "section": "",
    "text": "Being an excellent data analyst requires you to understand the techniques that go into analyzing data and how to communicate them effectively. While having a foundation in statistics and the quantitative sciences is essential, the most effective data analysts communicate information that influences decision makers.\nUnfortunately, there are plenty of examples of sloppy data analyses that distort the reality of what data can tell us. As an example, genetics researchers have found that Microsoft Excel’s autocorrecting feature had led to errors in 30% of gene list observations over a six year span. Perusing Reddit’s Data Is Ugly page is a reminder of how poor data visualization can go very wrong.\nHere I share a few red flags that may help to identify data analyses of poor quality. These come mostly from my experience in consulting with forestry companies and organizations, mentoring graduate students, and serving as a journal reviewer.\n\nSample sizes aren’t shown.\nIf a polling company reported that 70% of voters support candidate A over candidate B, you might think they’re a lock to win. But if only 10 people were polled, you’ll have more uncertainty and suspicion about the usefulness of the poll. Not reporting the number of observations can often obscure one’s interpretation of statistical results.\nA good practice is to always show the number of samples taken when providing a summary of a data set. This can be as simple as adding a footnote or a column in a table that provides the reader more insights into the data being shown.\n\n\nMeasure of variability isn’t reported.\nNate Silver’s “The Signal and the Noise” popularized a lot of methods used in political polling and statistics. The signal is generally understood as the trend in which the data points to, while the noise encompasses all of the uncertainty and variability associated with the data. It is often the noise which is the most challenging (but oftentimes fun) to deal with.\nAlways present a measure of variation when summarizing a variable from a data set. When collecting data from a population, the statistical variance or standard deviation is a useful metric. After a sample is taken, the standard error is a useful value that can also be used to determine the appropriate confidence interval.\nUnderstand the typical amount of variation you should expect for measurements you work with frequently. For example, the coefficient of variation (CV) is a useful measure which standardizes variability as a reflection of the mean value. The CV of stand volume in mixed-species uneven-aged forests is typically around 100%, while the CV for single-species stands grown in a plantation is typically around 30%. Understanding typical values of variation can also help in determining the appropriate number of samples to collect in future projects, saving time and cost.\n\n\nDistribution of data is unknown.\nWhile the mean and variation of a variable is informative, understanding the complete distribution of the data is essential. Having the complete distribution of a variable is important to identify outlier data points and identify clustering, in addition to determining whether or not subsequent statistical procedures can be applied to the data.\nVisualizations help immensely to show distributions of data. Histograms and box plots have long been used to show distributions. A newer visualization type, termed raincloud plots, combines density plots with box plots to display a full depiction of the distribution of data.\n\n\nToo many results are displayed.\nBeware of the report you’re reading with 50 pages of analysis that don’t seem to answer the question that needs to be answered. Oftentimes, analysts will present too many results because they have lost sight or are unsure what the motivation for the work originally was.\nIn my experience, analysts will often show too much data because they’re not confident in the results that are produced. I’ve sat through too many presentations where the speaker has shown tables with a dozen rows and columns and only allowed the audience 15 seconds to interpret the results. Presenting too many results leaves the reader unsure and suspect of your analysis.\nThe Law of Parsimony, which states that the simplest explanation is most likely the correct one, is a good adage to follow here. Be parsimonious with what you share. Only show them what they need to see to make an informed decision. But, always have additional results close by and be willing to share them if you’re asked.\n\n\nOutput is copy/pasted from software.\nOne of the benefits of having great software for analyzing data is that we can too easily rely on the output it produces. For example, all too often, we only need a key number or two after running a statistical test, yet a dozen tables and figures can be provided by software which show information related to a test.\nSimply copying output from software and pasting it into a report is lazy and indicates you haven’t spent time thinking about how to communicate your data. To combat this, I will often look at other reports and papers that quantify the same kinds of data that I’m analyzing. In doing this, you can be inspired with ideas and unique ways to present information that will resonate with your audience.\n\n\nTables and figures are not used effectively.\nTables are often one of the best ways to present data in a compact form. But too many people present tables that are too large to comprehend. Figures are useful to convey trends and showcase differences from one variable to another.\nTables should be used (1) to convey precise numbers or text, (2) to compare individual values, and, (3) to present several variables each with different units of measure. Figures can be used to (1) showcase trends or relationships between different values and (2) display photographs, maps, or other illustrations.\n–\nBy Matt Russell. Which data analysis practices are “red flags” to you? Leave me a comment on the LinkedIn post for this article and I’d love to hear it."
  },
  {
    "objectID": "post/2022-08-15-comparing-logging-trucker-wages-to-other-trucking-industries/index.html",
    "href": "post/2022-08-15-comparing-logging-trucker-wages-to-other-trucking-industries/index.html",
    "title": "Comparing logging trucker wages to other industries",
    "section": "",
    "text": "Log truck (photo: Darin Oswald)"
  },
  {
    "objectID": "post/2022-08-15-comparing-logging-trucker-wages-to-other-trucking-industries/index.html#section",
    "href": "post/2022-08-15-comparing-logging-trucker-wages-to-other-trucking-industries/index.html#section",
    "title": "Comparing logging trucker wages to other industries",
    "section": "",
    "text": "“To solve any problem in forestry, first figure out where the roads will go.”\nThat’s a quote I’ve heard a retired university professor tell students a number of times. There is a lot of wisdom in it: many forest resources are located in remote areas. Hauling products from the woods to the mill can be challenging. The economics of utilizing wood becomes less appealing if forest products are located far away from the mill that processes it.\nThe COVID-19 pandemic led to greater challenges to the trucking industry, with shutdowns and slowdowns impacting supply chains across many industries. After all, working from home is not an option for truckers.\nThis makes truckers an essential part of the forest products industry. Not unlike the logging industry, the average age of truckers is increasing. Many loggers and truckers are at retirement age or will reach retirement age within the next 10 years. Recruiting a new cohort of younger workers to take their place is a challenge that many companies in the industry are facing.\nThe forest products industry, along with others, is facing a workforce shortage in truckers. This presents a tremendous concern that will impact the supply of wood to mills, the viability of the forest products industry, and ultimately the health of forests.\nTruckers that haul products from the woods to the mill might have better-paying options doing the same kind of work in other industries. For example, in Maine, a recent study observed that trucker wages in growing industries such as merchant wholesalers (e.g., big box stores) offer wages significantly higher than similar jobs in the forestry and logging sector.\nTruckers in the forest products sector also face challenges in their daily work that may not be found in other sectors. Logging truckers deal with significant wait times at mills, which may limit the number of trips a trucker can haul in a day. Logging truckers must also follow federal weight limits on roads, which often means avoiding interstates and trucking on state and local roads, adding additional time to trips. While many efficiencies have been made in logging equipment in the last several decades, fewer advances in technology and equipment have been made to log trucks and the trucking process."
  },
  {
    "objectID": "post/2022-08-15-comparing-logging-trucker-wages-to-other-trucking-industries/index.html#trucking-workforce-data",
    "href": "post/2022-08-15-comparing-logging-trucker-wages-to-other-trucking-industries/index.html#trucking-workforce-data",
    "title": "Comparing logging trucker wages to other industries",
    "section": "Trucking workforce data",
    "text": "Trucking workforce data\nData from the US Bureau of Labor Statistics quantifies wages in different industries using the North American Industry Classification System (NAICS). Specific occupations are listed within each NIACS classification.\nFor the data presented in this article, all Heavy and Tractor-Trailer Truck Drivers were obtained if total employment in an occupation was at least 4,750 workers.\nAcross all US states, data from May 2021 indicated 8,070 truckers employed in the Forestry and Logging classification. Truckers in this classification earned a median of $22.66 per hour. This represents a 16.6% increase since prior to the pandemic in May 2019.\nA few other statistics about truckers in the Forestry and Logging classification:\n\nEighty percent of logging truckers earned between $14.65 and $29.49 per hour.\nFifty percent of logging truckers earned between $18.02 and $23.73 per hour.\nThe annual median wage of logging truckers was $47,130 per year.\n\nThe following figure shows the 10th, 25th, 75th, and 90th percentiles along with the median for hourly trucking wages. Compared to the top 25 industries that employ Heavy and Tractor-Trailer Truck Drivers, truckers in the Forestry and Logging industry rank 12th median hourly wage:\n\n\n\n\n\n\n\n\n\nWhile logging truckers earn similar rates compared to other sectors, the distribution of their wages is much narrower. The 10th percentile of wages for logging truckers is the eighth lowest across all sectors. The 90th percentile of wages for logging truckers is the fourth lowest across all sectors.\nIn summary, truckers in the logging industry face a number of challenges to remain competitive with similarly-skilled truckers in other professions. Logging truckers also face barriers to productivity such as long wait times at mills, federal weight limits on roads, and lower wage potential compared to other industries.\n–\nBy Matt Russell. Sign up for my monthly newsletter for in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "post/2022-07-27-simple-volume-to-weight-conversion-for-us-tree-species/index.html#volume-to-weight-conversions-with-fia-data",
    "href": "post/2022-07-27-simple-volume-to-weight-conversion-for-us-tree-species/index.html#volume-to-weight-conversions-with-fia-data",
    "title": "Simple volume to weight conversion for US tree species",
    "section": "Volume-to-weight conversions with FIA data",
    "text": "Volume-to-weight conversions with FIA data\nTo develop a simple set of volume-to-weight conversions, we can turn to the USDA Forest Service’s Forest Inventory and Analysis data. Specifically, the reference species table available through the FIA DataMart contains data on over 2,600 tree and shrub species that occur across the US. Contained in it are a few key variables for help when converting between wood volume and weight:\n\nWOOD_SPGR_GREENVOL_DRYWT: The green specific gravity of wood (green volume and oven-dry weight),\nBARK_SPGR_GREENVOL_DRYWT: The green specific gravity of bark (green volume and oven-dry weight),\nBARK_VOL_PCT: Bark volume as a percent of wood volume,\nMC_PCT_GREEN_WOOD: The moisture content of green wood as a percent of oven-dry weight, and\nMC_PCT_GREEN_BARK: The moisture content of green bark as a percent of oven-dry weight.\n\nTop start, we can calculate the weight of one cubic meter of wood, in kilograms:\n\\[ \\mbox{KG_PER_CUMTR_WOOD} = (\\mbox{WOOD_SPGR_GREENVOL_DRYWT}*1000)*(1+(\\mbox{MC_PCT_GREEN_WOOD}/100))\\]\nThen, we can convert this value to pounds per cubic foot:\n\\[ \\mbox{LBS_PER_CUFT_WOOD} = (\\mbox{KG_PER_CUMTR_WOOD}*2.2046)/35.3145\\]\nCords are a typical volume measure used to describe the volume of wood across many regions. We then convert this value to the weight of pounds per cord, assuming 90 cubic feet of wood are found in one cord of wood:\n\\[ \\mbox{LBS_PER_CORD_WOOD} = \\mbox{LBS_PER_CUFT_WOOD} * 90\\]\nWe shouldn’t forget about the bark component of trees, which can make up a considerable portion of the total volume of a tree. We can follow the same approach to determine volume in bark by using the following formulas:\n\\[ \\mbox{KG_PER_CUMTR_BARK} = (\\mbox{BARK_SPGR_GREENVOL_DRYWT}*1000)*\\\\(1+(\\mbox{MC_PCT_GREEN_BARK}/100))*\\\\(\\mbox{BARK_VOL_PCT}/100)\\]\n\\[ \\mbox{LBS_PER_CUFT_BARK} = (\\mbox{KG_PER_CUMTR_BARK}*2.2046)/35.3145\\]\n\\[ \\mbox{LBS_PER_CORD_BARK} = \\mbox{LBS_PER_CUFT_BARK} * 90\\]\nFinally, we can compute the number of tons per cord by adding the wood and bark components and dividing by the number of pounds in a ton (2,000):\n\\[ \\mbox{GREEN_TONS_PER_CORD} = (\\mbox{LBS_PER_CUFT_WOOD} + \\mbox{LBS_PER_CUFT_BARK})/2000 \\]"
  },
  {
    "objectID": "post/2022-07-27-simple-volume-to-weight-conversion-for-us-tree-species/index.html#an-example-with-sugar-maple",
    "href": "post/2022-07-27-simple-volume-to-weight-conversion-for-us-tree-species/index.html#an-example-with-sugar-maple",
    "title": "Simple volume to weight conversion for US tree species",
    "section": "An example with sugar maple",
    "text": "An example with sugar maple\nAs an example, consider we wish to determine the number of green tons per cord in sugar maple (Acer saccharrum). We start by calculating the volume in the wood component:\n\\[ \\mbox{KG_PER_CUMTR_WOOD}_{SM} = (0.56*1000)*(1+(57.39286/100)) = 881.4\\]\n\\[ \\mbox{LBS_PER_CUFT_WOOD}_{SM} = (881.4*2.2046)/35.3145 = 55.0237\\]\n\\[ \\mbox{LBS_PER_CORD_WOOD}_{SM} = 55.0237* 90 = 4952.13\\]\nThen, we can determine the amount in the bark component:\n\\[ \\mbox{KG_PER_CUMTR_BARK}_{SM} = (0.56*1000)*\\\\(1+(89.92593/100))*\\\\(15.6/100) = 165.9\\]\n\\[ \\mbox{LBS_PER_CUFT_BARK}_{SM} = (165.9*2.2046)/35.3145 = 10.3579\\]\n\\[ \\mbox{LBS_PER_CORD_BARK}_{SM} = 10.3579 * 90 = 932.22\\]\nFinally, we can compute the GREEN_TONS_PER_CORD conversion factor for sugar maple:\n\\[ \\mbox{GREEN_TONS_PER_CORD}_{SM} = (4952.13 + 932.22)/2000 = 2.94\\]\nSo, each cord of sugar maple wood weighs approximately 2.94 tons.\nIf you commonly deal with measurements in thousand board feet (MBF), you can multiply that number by two to convert between MBF and green weight. This uses the common assumption that one cord is equal to 500 board feet. Using this approach, one MBF of sugar maple would weigh 5.88 tons."
  },
  {
    "objectID": "post/2022-07-27-simple-volume-to-weight-conversion-for-us-tree-species/index.html#volume-weight-data-set-for-us-species",
    "href": "post/2022-07-27-simple-volume-to-weight-conversion-for-us-tree-species/index.html#volume-weight-data-set-for-us-species",
    "title": "Simple volume to weight conversion for US tree species",
    "section": "Volume-weight data set for US species",
    "text": "Volume-weight data set for US species\nFrom the FIA species list, here’s the distribution of 119 conifers and 339 hardwood species, representing the primary ones across the US. Hardwoods have a greater ratio than conifers:\n\n\n\n\n\n\n\n\n\nUsing this approach, I’ve provided a data set based on FIA’s species reference table. It contains volume to weight conversions for over 2,500 species, which you can find here:\n\nVolume to mass conversion data set\n\nYou might be interested to compare these conversion factors with ones you may be familiar with. As mentioned previously, these simple conversion factors are not always easy to find, but here are a few sources I came across that might be of interest:\n\nThe USDA Forest Service’s Southern Research Station presents a number of conversion factors for US species that are separated by region and product class in Timber products monitoring: unit of measure conversion factors for roundwood receiving facilities.\nIn his Forest Research Notes Vol. 16 Num. 1, Jack Lutz presents volume to weight conversion factors for several species. (See table 1).\n\nCheck with your local state agency. As an example, the Maine Forest Service and New Hampshire Division of forests and Lands present conversion factors for the primary commercial species in their state.\n\nHow do the volume-weight conversions work for you? Drop me an email to let me know if you’ve found them useful in your own work.\n– Update August 22, 2022. Special thanks to Jon Lundsford for pointing out an error in an equation from a previous version of this article.\nBy Matt Russell. Sign up for my monthly newsletter for in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "post/2022-07-14-a-forest-carbon-data-dashboard-for-minnesota/index.html#section",
    "href": "post/2022-07-14-a-forest-carbon-data-dashboard-for-minnesota/index.html#section",
    "title": "A forest carbon data dashboard for Minnesota",
    "section": "",
    "text": "By total forest land area, Minnesota ranks 21st in the United States. At 17.7 million acres, the state has an incredible amount of carbon being stored and sequestered in its trees and forests.\nI was recently a part of a year-long project funded by the Minnesota Forest Resources Council to do a deep dive into the state’s forest carbon resource. This project involved two primary tasks: (1) to create a report summarizing the current status and opportunities with forest carbon in Minnesota and (2) develop a data dashboard to effectively visualize current and past forest carbon attributes across the state."
  },
  {
    "objectID": "post/2022-07-14-a-forest-carbon-data-dashboard-for-minnesota/index.html#report-carbon-in-minnesotas-forests",
    "href": "post/2022-07-14-a-forest-carbon-data-dashboard-for-minnesota/index.html#report-carbon-in-minnesotas-forests",
    "title": "A forest carbon data dashboard for Minnesota",
    "section": "Report: Carbon in Minnesota’s Forests",
    "text": "Report: Carbon in Minnesota’s Forests\nThe report Carbon in Minnesota’s Forests: Current Status and Future Opportunities is a scoping document that outlines the current status and future opportunities of forest carbon in the state. The project involved compiling relevant literature and data sources, delivering a series of outreach events, holding focus group sessions with key stakeholders, and performing data analysis and forest growth simulations.\nHere are a few highlights of the report:\n\nNine information needs and opportunities were identified. The project identified nine major thematic information needs and opportunities that warrant closer exploration and research related to forest carbon in Minnesota. These themes came about through a series of focus groups with stakeholders that helped identify them. The themes focus on three broad areas: carbon markets, forest management, and data and technology.\nForest Inventory and Analysis data provide detailed forest carbon estimates. Data from the USDA Forest Service’s Forest Inventory and Analysis program were used to quantify baseline information on forest carbon. In Minnesota, carbon attributes were summarized from over 6,300 forested plots, representing approximately one plot for every 2,791 acres. Forest carbon information was also obtained for the state using greenhouse gas emissions and removals data reported on a statewide basis.\nStates are tackling forest carbon differently. Every state is looking at forest carbon opportunities in different ways. In the report, Table 1 lists 15 different states with a number of current projects ongoing within their boundaries. The table serves as a handy resource to compare state-level investments in forest carbon programs and research opportunities.\nGrowth and yield simulations provide a glimpse into how management impacts forest carbon. To better understand carbon dynamics, the project simulated carbon stocks for several forest types and forest management treatments. Three silvicultural treatments were simulated for each forest type: a no management treatment, a “business as usual” treatment (i.e., a commonly-implemented forest management strategy used throughout Minnesota), and a climate-adapted treatment. The Forest Vegetation Simulator, an individual tree model that uses lists of trees to forecast forest growth through time, was used to perform the simulation.\nCarbon markets present opportunities for al landowners. The report also lists the current status of several carbon markets available to landowners. With an emphasis on voluntary markets and impacts to private woodland owners with small acreages, the report can serve as a useful roadmap to better understanding market-based approaches to forest carbon."
  },
  {
    "objectID": "post/2022-07-14-a-forest-carbon-data-dashboard-for-minnesota/index.html#forest-carbon-data-dashboard",
    "href": "post/2022-07-14-a-forest-carbon-data-dashboard-for-minnesota/index.html#forest-carbon-data-dashboard",
    "title": "A forest carbon data dashboard for Minnesota",
    "section": "Forest carbon data dashboard",
    "text": "Forest carbon data dashboard\nAnother component of the project was the development of the Minnesota Forest Carbon Dashboard. This data dashboard provides a summary of Minnesota’s forest carbon resource, with the ability to query information by forest type, ownership, regions, and stocking levels.\nThe dashboard was developed by ArcGIS Online. Data tables present carbon stocks and flux estimates dating back to 1990 with a number of units, including carbon and carbon dioxide equivalent.\nSpecial thanks to the Minnesota Forest Resources Council for their support with this project and especially to Chris Edgar and the University of Minnesota USpatial team for developing the data dashboard.\nWhat data on forest carbon would be most beneficial to you and your organization? Email me and I’d love to learn more.\n–\nBy Matt Russell. Sign up for my monthly newsletter for in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "post/2022-04-10-carbon-credits-quantity-versus-quality/index.html#section",
    "href": "post/2022-04-10-carbon-credits-quantity-versus-quality/index.html#section",
    "title": "Carbon credits: quantity versus quality",
    "section": "",
    "text": "The forestry community is focused on carbon, and the question of quantity versus quality of credits is increasingly being raised. Credits associated with forest carbon are bringing new opportunities to landowners and the investors that value timberlands.\nWith about 800 million acres of forests in the US alone, there is no lack of carbon for emerging markets to tap into. Some carbon programs are limited by not including public lands, but the volume of forests in other ownerships is large. Last week’s news of the state of Washington entering 10,000 acres of school trust lands into a carbon project is a reflection of more diverse land ownerships jumping into carbon markets.\nThe quality of carbon credits relates to how valuable the credit is relative to other factors such as costs of forest management and deferring timber harvest. Carbon credits that integrate the full value of the carbon in the trees and forests would be considered of high quality. Carbon programs which contain details on issues such as proving additionality and minimizing leakage can help show that their credits are of high quality.\nThe different aspects of quality and quantity of carbon credits have been at the forefront of many conversations in forestry in the last six months."
  },
  {
    "objectID": "post/2022-04-10-carbon-credits-quantity-versus-quality/index.html#a-timber-investors-view-on-carbon-credits",
    "href": "post/2022-04-10-carbon-credits-quantity-versus-quality/index.html#a-timber-investors-view-on-carbon-credits",
    "title": "Carbon credits: quantity versus quality",
    "section": "A timber investor’s view on carbon credits",
    "text": "A timber investor’s view on carbon credits\nJim Hourdequin, CEO of The Lyme Timber Company, has recently brought the issue of quality and quantity of carbon credits to many conversations. Last fall, he provided a presentation titled “You get what you pay for: a TIMO’s perspective on forest carbon offsets and evolving carbon markets”(36 min.). This presentation was delivered at the Who Will Own the Forest Conference sponsored by the World Forestry Center.\nLast month, Hourdequin was featured prominently in an article in Bloomberg titled “This timber company sold millions of dollars of useless carbon offsets”. Many of the issues pointed out in his presentation were also mentioned in the article.\nFor perspective, the Lyme Timber Company owns 1.6 million acres of forests in the US. The company has approximately 200,000 of those acres currently enrolled in carbon markets.\nIn the article and presentation, Hourdequin mentions many of the drawbacks to some carbon programs. For example, with a carbon program that operates on a harvest deferral model, there is speculation that payments are made to the landowner for actions they were not planning to do anyway. Hourdequin uses a different analogy, saying it’s like “paying to enter a beer hall, enjoying a fine meal, but finding out they ran out of beer.”\nHe goes on to mention that with a 100-year compliance framework, typical in the California Air Resources Board’s (CARB) Compliance Offset Program, the carbon sale proceeds do not fully cover the cost of the timberlands. According to CARB’s recent Auction Information and Results from late last year, the settlement price of one metric tonne of carbon was $28.26. On the voluntary markets, carbon prices are going from $6 to $13 dollars per metric tonne, a value that favors a larger quantity of credits.\nIn his presentation, Hourdequin presents three case studies using forests in his company’s portfolio. His major findings across these three case studies are that the price of carbon was considerably higher after taking into account harvest reductions and forest management obligations:\n\nIn Michigan, Lyme forests have had a long history of timber harvesting. In Lyme’s analysis, the true cost of carbon for them was $60/tonne.\nIn Florida, this Lyme forest is under conservation easement. True cost of carbon: $50/tonne.\nIn a West Virginia property, this forest is currently enrolled in a CARB project. True cost of carbon: $30/tonne.\n\nThe primary conclusions are that at low prices, the quantity for carbon credits will generally be preferred instead of the quality of credits. If buyers of carbon credits are willing to pay, there may be additional opportunities for focusing on credits that reflect the true value of carbon. For example, the three case studies from Lyme’s forests indicate higher rates payments that integrate the full suite of costs that are higher than current market rates.\nHourdequin’s perspectives are essential to listen to and understand. Although there are abundant carbon credits available today, with many carbon programs going after them, understanding the quality of those credits will be needed to better understand the role of forests in natural capital markets.\nPS - Understanding the values behind forest carbon is essential to understanding how markets work. My eBook Forest Carbon by the Numbers is available on Amazon for more perspective on forest carbon and the metrics behind them.\n–\nBy Matt Russell\nEmail Matt with any questions or comments. Sign up for The Landing for monthly in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "post/2022-02-19-forestry-is-a-stem-discipline/index.html#section-1",
    "href": "post/2022-02-19-forestry-is-a-stem-discipline/index.html#section-1",
    "title": "Forestry is a STEM discipline",
    "section": "",
    "text": "The Homeland Security Department recently recognized forestry as a science, technology, engineering, and mathematics (STEM) discipline. Homeland Security has added both the general forestry and forest resources production and management fields of study to the Optional Practical Training (OPT) program.\nThis is significant for the forestry profession because colleges and universities will have access to additional funding to support students. The announcement also makes it easier for non US citizens with advanced degrees (such as Masters and PhD graduates) that are in STEM disciplines to obtain lawful permanent resident status in the United States.\nUltimately, this announcement can help to spur growth and innovation in the US economy. In an industry like forestry with limited pools for professionals with advanced degrees, this announcement is welcomed.\nSpecial thanks to the Society of American Foresters for their work in getting forestry recognized as a STEM discipline.\n–\nBy Matt Russell. Sign up for my monthly newsletter for in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "post/2021-11-27-co2-offsets-and-trips-to-the-family-cabin/index.html#section-1",
    "href": "post/2021-11-27-co2-offsets-and-trips-to-the-family-cabin/index.html#section-1",
    "title": "CO2 offsets and trips to the family cabin",
    "section": "",
    "text": "Often I think that if the numbers behind forest carbon estimates were easier to calculate and interpret, we’d all have a better understanding of the carbon stored and sequestered in our forests. It starts simple when we say that “half of the biomass of a tree is carbon”. But beyond that, transitions between gases and solids and conversions between metric and English units quickly add complexity.\nThe true amount of forest carbon is only measured in detailed research studies, yet we discuss carbon stocks like they’re known values. A tremendous amount of research has gone into determining the amount of biomass and carbon using allometric equations, and more recently, remote sensing and other technologies.\nFor the past two years I’ve run the Tree and Woodland Carbon Capture Challenge, a friendly citizen science contest with the goal of finding the tree and woodland in Minnesota that sequesters the most carbon. Participants measure a tree or woodland at the beginning of the growing season in April and then remeasure that same tree or woodland in October. Behind the scenes, I calculate the biomass and carbon stored and sequestered in the trees and tabulate the results.\nOne participant measured a silver maple in his suburban yard in the Twin Cities. The tree was about 15 inches in diameter, gaining about 84 pounds in biomass through the growing season. So, it sequestered about 42 pounds of carbon.\nThe participant was interested to know how much CO2 his trees “offset” and how he can relate that to driving his car. Secretly, he told me, he wanted to justify the several road trips he made to his family’s cabin in northern Minnesota this summer.\nAs foresters we think of carbon in terms of pounds, but it is often useful to convert it to its CO2 equivalent. One unit of carbon is equal to 3.667 units of CO2 equivalent. For the 15-inch silver maple, this equated to removing 154 units of CO2 from the atmosphere this year.\nBurning a gallon of gas emits about 20 pounds of carbon dioxide. Hence, the silver maple removed the equivalent of 7.7 gallons of gas from the atmosphere in the 2021 growing season. My participant’s cabin is 220 miles away outside Ely, MN: if his vehicle gets 29 miles to the gallon, a single one-way trip would be offset by the silver maple in his yard back home.\nSuch calculations are important to interpret and convey the importance of trees in removing CO2 from the atmosphere. The Environmental Protection Agency maintains its Greenhouse Gas Equivalencies Calculator to help put carbon and CO2 values in context. I’ve maintained the Forest Carbon Cheat Sheet to help convert key values of forest carbon to different units to contextualize their metrics.\nForest resource professionals will need to be able to articulate the role of trees and forests in the carbon cycle. Being effective communicators on this topic can lend trust to forestry professionals and position the industry to be an essential player in addressing future climate challenges.\n–\nBy Matt Russell. Email Matt with any questions or comments.\nFor more on forest carbon, I’ve compiled the eBook Forest Carbon by the Numbers: Understanding the Hottest Forest Product of the 2020s, sharing insights and visualizations of the current status and trends in forest carbon and how it impacts the forest products industry."
  },
  {
    "objectID": "post/2021-09-26-random-forests-a-tutorial-with-forestry-data/index.html#the-parresol-tree-biomass-data",
    "href": "post/2021-09-26-random-forests-a-tutorial-with-forestry-data/index.html#the-parresol-tree-biomass-data",
    "title": "Random forests: a tutorial with forestry data",
    "section": "The Parresol tree biomass data",
    "text": "The Parresol tree biomass data\nAs an example, we’ll use a data set of 40 slash pine trees from Louisiana USA presented in Parresol’s 2001 paper Additivity of nonlinear biomass equations. The data are presented in Table 1 of the paper, which is replicated in this Google Sheet.\nWe’ll read in the data using the read_sheet() function from the googlesheets4 package. We will also load the tidyverse package to use some of its plotting features:\n\nlibrary(tidyverse)\nlibrary(googlesheets4)\n\ntree &lt;- read_sheet(\"https://docs.google.com/spreadsheets/d/1TPutUVyZLWr7XopKguT5Nvh9lo1EOG4wvOZ6_lD1F_M/edit?usp=sharing\")\n\nThe data contain the following variables:\n\nTreeID: Tree observation record,\nDBH: Tree diameter at breast height, cm,\nHT: Tree height, m,\nLCL: Tree live crown length, m,\nAge: Age of the tree, years,\nMass_wood: Green mass of the wood in the tree, kg,\nMass_bark: Green mass of the bark in the tree, kg,\nMass_crown: Green mass of the crown of the tree, kg, and\nMass_tree: Green mass of all tree components, kg.\n\nOur ultimate interest is in predicting the mass all tree components using common tree measurements such as tree diameter, height, live crown length, and age. Before we start modeling with the data, it is a good practice to first visualize the variables. The ggpairs() function from the GGally package is a useful tool that visualizes the distribution and correlation between variables:\n\nlibrary(GGally)\n\nggpairs(tree, columns = c(2:5, 9))\n\n\n\n\n\n\n\n\nYou can see a few variables have strong positive correlations with the mass of the tree (e.g., height and diameter) and some more moderate positive correlations (e.g., age)."
  },
  {
    "objectID": "post/2021-09-26-random-forests-a-tutorial-with-forestry-data/index.html#the-randomforest-r-package",
    "href": "post/2021-09-26-random-forests-a-tutorial-with-forestry-data/index.html#the-randomforest-r-package",
    "title": "Random forests: a tutorial with forestry data",
    "section": "The randomForest R package",
    "text": "The randomForest R package\nR and Python both have numerous packages that implement random forests. In R alone, there are nearly 400 packages with the word “tree” or “forest” in their name. (Sidebar: This is not ideal if you’re a forest analyst of biometrician because only 31 of them are actually about forestry.)\nBreiman wrote about random forests in 2001 and a year later Liaw and Wiener created an R package that implements the technique. To date, the randomForest R package remains one of the most popular ones in machine learning.\nWe can install and load the randomForest package:\n\n# install.packages(\"randomForest\")\nlibrary(randomForest)\n\nWe will use the randomForest() function to predict total tree mass using several variables in the tree data set. A few other key statements to use in the randomForest() function are:\n\nkeep.forest = T: This will save the random forest output, which will be helpful in summarizing the results.\nimportance = TRUE: This will assess the importance of each of the predictors, essential output in random forests!\nmtry = 1: This tells the function to randomly sample one variable at each split in the random forest. For applications in regression, the default value is the number of predictor variables divided by three (and rounded down). In the modeling, several small samples of the entire data set are taken. Any observations that are not taken are called “out-of-bag” samples.\nntree = 500. This tells the function to grow 500 trees. Generally, a larger number of trees will produce more stable estimates. However, increasing the number of trees needs to be done with consideration of time and memory issues when dealing with large data sets.\n\nOur response variable in the random forests model is Mass_tree and predictors are DBH, HT, LCL, and Age.\n\ntree.rf &lt;- randomForest(Mass_tree ~ DBH + HT + LCL + Age,\n                        data = tree,\n                        keep.forest = T,\n                        importance = TRUE, \n                        mtry = 1,\n                        ntree = 500)\ntree.rf\n\n\nCall:\n randomForest(formula = Mass_tree ~ DBH + HT + LCL + Age, data = tree,      keep.forest = T, importance = TRUE, mtry = 1, ntree = 500) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 9415.002\n                    % Var explained: 87.27\n\n\nNote the mean of squared residuals and the percent variation explained (analogous to R-squared) provided in the output. (We’ll revisit them later.)\nAnother way to visualize the out-of-bag error rates of the random forests models is to use the plot() function. In this application, although we specified 500 trees, the out-of-bag error generally stabilizes after 100 trees:\n\nplot(tree.rf)\n\n\n\n\n\n\n\n\nSome of the most helpful output in random forests is the importance of each of the predictor variables. The importance score is calculated by evaluating the regression tree with and without that variable. When evaluating the regression tree, the mean square error (MSE) will go up, down, or stay the same.\nIf the percent increase in MSE after removing the variable is large, it indicates an important variable. If the percent increase in MSE after removing the variable is small, it’s less important.\nThe importance() function prints the importance scores for each variable and the varImpPlot() function plots them:\n\nimportance(tree.rf)\n\n      %IncMSE IncNodePurity\nDBH 17.507804      880292.7\nHT  15.296300      807963.8\nLCL 13.388462      639453.3\nAge  8.131857      408963.7\n\nvarImpPlot(tree.rf,type=1)\n\n\n\n\n\n\n\n\nThe output indicates that DBH is the most important variable for predicting Mass_tree and age the least important."
  },
  {
    "objectID": "post/2021-09-26-random-forests-a-tutorial-with-forestry-data/index.html#comparing-random-forests-and-regression-models",
    "href": "post/2021-09-26-random-forests-a-tutorial-with-forestry-data/index.html#comparing-random-forests-and-regression-models",
    "title": "Random forests: a tutorial with forestry data",
    "section": "Comparing random forests and regression models",
    "text": "Comparing random forests and regression models\nForest analysts are often compare multiple models and determine which one has a better predictive ability. In this case, we can fit a multiple linear regression model to the data and compare to the random forests model.\nThe lm() function can be used to develop a parametric model for Mass_tree:\n\ntree.reg &lt;- lm(Mass_tree ~ DBH + HT + LCL + Age, data = tree)\nsummary(tree.reg)\n\n\nCall:\nlm(formula = Mass_tree ~ DBH + HT + LCL + Age, data = tree)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-136.285  -57.177   -9.399   43.822  189.758 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -545.374     67.916  -8.030 1.89e-09 ***\nDBH           40.523      5.778   7.013 3.68e-08 ***\nHT           -15.048      8.079  -1.862   0.0709 .  \nLCL            2.490     12.259   0.203   0.8402    \nAge           15.431      3.198   4.825 2.72e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 82.33 on 35 degrees of freedom\nMultiple R-squared:  0.9198,    Adjusted R-squared:  0.9106 \nF-statistic: 100.4 on 4 and 35 DF,  p-value: &lt; 2.2e-16\n\n\nNote the residual standard error of 82.33 kg and the adjusted R-squared of 0.91. The residual standard error is slightly lower and the R-squared value slightly higher for the multiple regression model compared to the random forest output. In addition, further work may be conducted on the multiple regression model by removing the non-significant variables and refitting the model.\nAnother aspect of model evaluation is comparing predictions. Although random forests models are often considered a “black box” method because their results are not easily interpreted, the predict() function provides predictions of total tree mass:\n\nMass_pred_rf &lt;- predict(tree.rf, tree, predict.all = F)\nMass_pred_reg &lt;- predict(tree.reg, tree, predict.all = F)\n\nIn an ideal setting we might test our model on an independent data set not used in model fitting. However, we can combine the predicted tree weights from both models to the tree data set:\n\ntree2 &lt;- as.data.frame(cbind(tree, Mass_pred_rf, Mass_pred_reg))\n\nNote that some predictions from the linear regression model on the 40 trees provide negative values for predicted total tree mass, an undesirable feature that may need to be addressed before implementing the model:\n\ntree2 %&gt;% \n  summarize(Mass_tree, Mass_pred_rf, Mass_pred_reg)\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n   Mass_tree Mass_pred_rf Mass_pred_reg\n1        9.8     25.21493   -108.051811\n2       12.1     25.05643    -86.903051\n3       24.4     40.88979    -62.814807\n4       27.0     34.52743    -42.513067\n5       33.6     43.41506     -7.391764\n6       43.5     45.98330     -8.814627\n7       46.0     99.98358    168.354603\n8       56.1     77.63545     -8.073626\n9       64.4     65.60557     47.563293\n10      70.8    108.85428     64.024945\n11      75.9    111.52683    189.278688\n12      88.7     84.93915    103.439719\n13      95.7    102.76580     18.126885\n14     102.4    143.41118    238.684823\n15     123.7    145.36054     90.880242\n16     147.6    176.06111    261.258307\n17     148.5    143.53247    174.276553\n18     174.8    170.33346    186.750002\n19     193.0    169.30845    199.939638\n20     211.7    207.50296    306.293014\n21     214.6    269.89235    186.964881\n22     225.3    245.55534    240.537957\n23     244.7    247.63206    277.932654\n24     258.2    289.27981    263.034375\n25     285.8    259.22561    363.444771\n26     297.6    285.91326    317.816460\n27     309.8    270.94465    366.051168\n28     316.2    342.13331    392.605018\n29     318.0    314.38717    283.934957\n30     401.1    424.69591    399.000959\n31     402.2    389.01151    463.875242\n32     411.9    401.71263    450.015697\n33     446.3    467.22418    458.158909\n34     490.3    419.49775    546.871939\n35     522.6    519.87912    583.516204\n36     522.7    472.62866    519.012527\n37     593.6    580.70856    652.592720\n38     900.3    788.07493    714.152257\n39    1034.9    892.33950    845.142484\n40    1198.5   1006.03689   1095.330861\n\n\nWe may also be interested in plotting residual values from both model types to compare their performance:\n\np.rf &lt;- ggplot(tree2, (aes(x = Mass_pred_rf, y = Mass_tree - Mass_pred_rf))) +\n  geom_point() + \n  scale_y_continuous(limits = c(-200, 200)) +\n  labs(x = \"Predicted tree mass (kg)\",\n       y = \"Residual (kg)\",\n       subtitle = \"Random forests model\") \n\np.reg &lt;- ggplot(tree2, (aes(x = Mass_pred_reg, y = Mass_tree - Mass_pred_reg))) +\n  geom_point() + \n  scale_y_continuous(limits = c(-200, 200)) +\n  labs(x = \"Predicted tree mass (kg)\",\n       y = \"Residual (kg)\",\n       subtitle = \"Regression model\") \n\nlibrary(patchwork)\n\np.rf + p.reg\n\n\n\n\n\n\n\n\nWith the heteroscedastic residuals in the models, we’d likely want to explore transforming the data prior to model fitting, or to explore other modeling techniques."
  },
  {
    "objectID": "post/2021-09-26-random-forests-a-tutorial-with-forestry-data/index.html#summary",
    "href": "post/2021-09-26-random-forests-a-tutorial-with-forestry-data/index.html#summary",
    "title": "Random forests: a tutorial with forestry data",
    "section": "Summary",
    "text": "Summary\nRandom forests techniques are flexible and can perform comparably with other regression or classification methods. Random forests can handle all types of data (e.g., categorical, continuous) and are advantageous because they work well with data sets containing a large number of predictor variables. The randomForest package has seen a lot of development and can be used to help solve modeling problems in your future forest analytics work.\n–\nBy Matt Russell. Email Matt with any questions or comments. Sign up for my monthly newsletter for in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "post/2021-08-22-if-50-of-a-tree-s-biomass-is-carbon-what-s-the-other-half/index.html",
    "href": "post/2021-08-22-if-50-of-a-tree-s-biomass-is-carbon-what-s-the-other-half/index.html",
    "title": "If 50% of a tree’s biomass is carbon, what’s the other half?",
    "section": "",
    "text": "“You wrote earlier that “since half of biomass is carbon, I multiply by 0.5.” What is the other half of biomass?”\n\nI was sent this question this question over email last week. As foresters, we’re often concerned with only carbon stored and sequestered in trees. But the are other elements that form the chemical composition of wood. While there are slight deviations to the “50% of biomass in wood is carbon” assumption, it is a statement that provides an approximation about the amount of carbon in wood.\nFlashbacks of your middle school biology classes might enter your mind as you remember equations for photosynthesis and plant growth. The other 50% of wood is mostly oxygen (~44%), hydrogen (~6%), nitrogen (~1%), and other trace metals. Other trace metals include calcium, potassium, and iron, among other elements. We can view the wood characteristics in a polar area diagram:\n\n\n\nAlthough the chemical compositions of wood vary by species and regions, these are approximations for “the other half”. The major components of wood - lignin, cellulose, and hemicellulose - each vary in terms of the proportion of carbon found in them. It’s a useful reminder of the unique chemical composition of wood.\n–\nBy Matt Russell. Email Matt with any questions or comments. Sign up for my monthly newsletter for in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "post/2021-08-08-forest-carbon-key-definitions-and-numbers-in-perspective/index.html",
    "href": "post/2021-08-08-forest-carbon-key-definitions-and-numbers-in-perspective/index.html",
    "title": "Forest carbon: key definitions and numbers in perspective",
    "section": "",
    "text": "Carbon storage in US forests. Source: USDA Forest Service."
  },
  {
    "objectID": "post/2021-08-08-forest-carbon-key-definitions-and-numbers-in-perspective/index.html#section-1",
    "href": "post/2021-08-08-forest-carbon-key-definitions-and-numbers-in-perspective/index.html#section-1",
    "title": "Forest carbon: key definitions and numbers in perspective",
    "section": "",
    "text": "Carbon is quickly becoming one of the hottest forest products of the 2020s. The term “carbon” is a broad one, and when reading an article or listening to a speaker, you may not know exactly which process or specific term is being referred to.\nHere are a few basic terms that will help guide your understanding:\nA carbon pool is a component of the forest that can gain or lose carbon over time. We often think of this as the trees, but trees are not the only component where carbon is stored. Forests store carbon in five different pools: (1) in live trees, aboveground, (2) in live trees, belowground, (3) in dead wood, including standing dead trees and downed dead wood, (4) in litter, including leaves and other small woody material, and (5) in soil, including mineral and organic soils. The majority of carbon in US forests in found in the mineral soil, followed by aboveground live trees.\nCarbon storage refers to the amount of carbon retained in a forest and/or carbon pool. For perspective, the average carbon density stored in aboveground trees across forests in the United States is 22.6 US tons per acre. Forests store the equivalent of 33 years of all carbon dioxide emissions produced across the US. Carbon storage across US forests has increased by 11% from 1990 to 2019.\nCarbon sequestration is the process by which trees and plants use carbon dioxide and photosynthesis to store carbon as biomass. Across the US, forests, urban trees, and harvested wood products remove 14% of all carbon dioxide emissions in a given year. Averaged across the entire US, a 50 year-old forest will sequester approximately 0.3 US tons of carbon per acre.\nIn the US, forest carbon is typically expressed in US tons per acre, where 1 US ton = 2,000 lbs. In many carbon markets in the US and internationally, carbon is discussed in metric tons (also spelled “tonnes”). To convert, 1 metric ton = 1.10 US tons.\nLast year I worked with the Forest Resources Association to develop Forest Carbon Fact Sheets for every US state. The fact sheets present the status and trends in forest carbon storage and sequestration using data from the USDA Forest Service Forest Inventory and Analysis program. The fact sheets are a useful tool to show the importance of managed forestlands to address climate change at the national, regional, and state levels. Find your state’s fact sheet.\n–\nBy Matt Russell. Email Matt with any questions or comments. Sign up for my monthly newsletter for in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "post/2021-06-12-forest-carbon-cheat-sheet-updated-for-2021/index.html",
    "href": "post/2021-06-12-forest-carbon-cheat-sheet-updated-for-2021/index.html",
    "title": "Forest carbon cheat sheet: updated for 2021",
    "section": "",
    "text": "The Forest Carbon Cheat Sheet is a quick reference guide for converting forest carbon measurements and understanding typical carbon values at the tree, stand, and landscape levels.\nThe Cheat Sheet has been updated with the most recent values published in the US Environmental Protection Agency Inventory of U.S. Greenhouse Gas Emissions and Sinks: 1990-2019.\nDOWNLOAD (PDF)\nDOWNLOAD (JPEG)\n\n\n\n\nThe Forest Carbon Cheat Sheet.\n\n\n\n–\nBy Matt Russell. Email Matt with any questions or comments. Sign up for my monthly newsletter for in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "post/2021-05-08-understanding-the-forest-products-industry-using-naics-codes/index.html",
    "href": "post/2021-05-08-understanding-the-forest-products-industry-using-naics-codes/index.html",
    "title": "Understanding the forest products industry using NAICS codes",
    "section": "",
    "text": "To understand any industry, an analyst should be well acquainted with the codes designating the North American Industry Classification Systems. These NAICS codes are often used along with economic data to analyze an industry or sector. When a new business is created, they self-select into one or more NAICS codes.\nIn forestry, there are several designations that represent different areas of the forest products industry. These range from wood products manufacturers, loggers, pulp mills, and forest nurseries. NAICS codes have been used extensively in several analyses of the forestry industry, including:\n\nUnderstanding how many jobs were lost in the forestry sector as a result of the Great Recession,\nUnderstanding foreign direct investments between the US and Canada within the forest products sector, and\nDetermining heat-related illnesses in the forestry sector in Washington.\n\nUsing data from the NAICS website, the following graph shows the number of existing marketable businesses according to their NAICS industry category:\n\n\n\n\n\n\n\n\n\nThe names of the specific NAICS codes appear below along with the number of businesses. Businesses conducting millwork, logging, and support activities for forestry are the most common:\n\n\n\n\nNAICS codes, names, and number of marketable US businesses in the forest products industry.\n\n\nNAICSCode\nNAICSName\nBusinesses\n\n\n\n\n321918\nOther Millwork (including Flooring)\n14687\n\n\n113310\nLogging\n8250\n\n\n115310\nSupport Activities for Forestry\n5277\n\n\n321999\nAll Other Miscellaneous Wood Product Manufacturing\n4507\n\n\n321920\nWood Container and Pallet Manufacturing\n3383\n\n\n321113\nSawmills\n2480\n\n\n113110\nTimber Tract Operations\n2067\n\n\n322121\nPaper (except Newsprint) Mills\n1921\n\n\n322211\nCorrugated and Solid Fiber Box Manufacturing\n1852\n\n\n322220\nPaper Bag and Coated and Treated Paper Manufacturing\n1471\n\n\n321911\nWood Window and Door Manufacturing\n1259\n\n\n321912\nCut Stock, Resawing Lumber, and Planing\n1225\n\n\n322299\nAll Other Converted Paper Product Manufacturing\n1140\n\n\n321992\nPrefabricated Wood Building Manufacturing\n1111\n\n\n113210\nForest Nurseries and Gathering of Forest Products\n962\n\n\n321214\nTruss Manufacturing\n869\n\n\n322219\nOther Paperboard Container Manufacturing\n663\n\n\n322130\nPaperboard Mills\n658\n\n\n321114\nWood Preservation\n640\n\n\n321991\nManufactured Home (Mobile Home) Manufacturing\n570\n\n\n322230\nStationery Product Manufacturing\n412\n\n\n322291\nSanitary Paper Product Manufacturing\n374\n\n\n322110\nPulp Mills\n343\n\n\n321219\nReconstituted Wood Product Manufacturing\n311\n\n\n321211\nHardwood Veneer and Plywood Manufacturing\n299\n\n\n322212\nFolding Paperboard Box Manufacturing\n147\n\n\n321212\nSoftwood Veneer and Plywood Manufacturing\n80\n\n\n321213\nEngineered Wood Member (except Truss) Manufacturing\n58\n\n\n322122\nNewsprint Mills\n34\n\n\n\n\n\n\n\n\nIf you work with forest industry and economic data, get acquainted with the NAICS codes and how they can be used in your analysis.\n–\nBy Matt Russell. Email Matt with any questions or comments. Sign up for my monthly newsletter for in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "post/2021-02-21-private-forest-landowner-demographics-compared-to-the-us-population/index.html",
    "href": "post/2021-02-21-private-forest-landowner-demographics-compared-to-the-us-population/index.html",
    "title": "Private forest landowner demographics compared to the US population",
    "section": "",
    "text": "Private individuals own a third of the nation’s forests. These private landowners value them primarily due to the beauty, natural values, and wildlife habitat they provide.\nForests owned by private non-industrial individuals are diverse: they vary in terms of species composition, structure, and management objectives. While privately-owned forests are diverse, the demographics of their owners are not.\nData from the National Woodland Owner Survey, a program administered by the USDA Forest Service, indicate that the primary decision makers on private forestlands are older, predominantly male, White, and non-Hispanic. These demographics do not align with the population demographics of the state or US as a whole.\n\n\n\n\n\n\n\n\n\nWhile 72% of the US population are White, 95% of private forests are owned by Whites. Men make up 49% of the US population, yet 79% of private forest are owned by men. People aged 65 years or over are 13% of the US population, but 43% of private land owners are of this age.\nPrivate forestland is often passed on to family members through legacy planning. Because of this, landowner demographics will not likely change unless there is increased interest in passing land to non-heirs, a tool that can diversify landowner demographics.\nLittle research exists about the demographics of private forestland ownership by non-traditional groups. Here are a few resources to learn more:\n\nWomen Owning Woodlands. Communities of women owning woodlands with a shared mindset of land stewardship have increased in recent years. The WOW website supports women in forest leadership, women who manage their own woodlands, and all who facilitate the stewardship of forests.\nSustainable Forestry and African American Land Retention Network (SFLR). The SFLR program improves forest management by connecting African American landowners to established networks of forestry professionals. The third episode of America’s Forest with Chuck Leavell highlights the SFLR program.\nNative American Land Ownership. Native American tribes own 2% of all US forestland, including in individual allotments or lands managed by tribal entities. The Story Map “Who owns America’s forest? has excellent data on ownership by Native Americans.\n\n–\nBy Matt Russell. Email Matt with any questions or comments. Sign up for my monthly newsletter for in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "post/2021-02-04-forest-carbon-a-reading-list-for-beginners/index.html",
    "href": "post/2021-02-04-forest-carbon-a-reading-list-for-beginners/index.html",
    "title": "Forest carbon: a reading list for beginners",
    "section": "",
    "text": "A black spruce forest in northern Minnesota"
  },
  {
    "objectID": "post/2021-02-04-forest-carbon-a-reading-list-for-beginners/index.html#carbon-facts-and-figures",
    "href": "post/2021-02-04-forest-carbon-a-reading-list-for-beginners/index.html#carbon-facts-and-figures",
    "title": "Forest carbon: a reading list for beginners",
    "section": "Carbon facts and figures",
    "text": "Carbon facts and figures\n\nGreenhouse gas emissions and removals from forest land, woodlands, and urban trees in the United States, 1990-2018 (USDA Forest Service). Every year the USDA Forest Service reports an economy-wide inventory of greenhouse gas (GHG) emissions and removals in the country. This most recent document provides an overview of the status and trends of GHG emissions and removals from forests, woodlands, harvested wood products, and urban trees in the United States from 1990 to 2018. It includes national and state-level reports.\nCarbon fact sheets (Forest Resources Association). I worked with FRA to develop fact sheets that show the role of forests in sequestering and storing carbon. The fact sheets present the status and trends in forest carbon storage and sequestration using data from the USDA Forest Service. The fact sheets have reports at the national, regional, and state levels."
  },
  {
    "objectID": "post/2021-02-04-forest-carbon-a-reading-list-for-beginners/index.html#carbon-and-forest-management",
    "href": "post/2021-02-04-forest-carbon-a-reading-list-for-beginners/index.html#carbon-and-forest-management",
    "title": "Forest carbon: a reading list for beginners",
    "section": "Carbon and forest management",
    "text": "Carbon and forest management\n\nForest carbon: an essential natural solution to climate change (UMass Extension). This is an excellent publication that lists definitions of forest carbon with great graphics and illustrations. It showcases the differences between managing forests for carbon sequestration, storage, or both. It is one of the best publications for landowners and decision makers to learn about the complexities of forest carbon.\nForest carbon management (USDA Forest Service, Northern Institute of Applied Climate Science). Managing forests for carbon and climate change introduces the topic of adaptation as a forest management tool. This website describes adaptation strategies and approaches for forest carbon management, including links to case studies showing real-world adaptation demonstration projects.\nCarbon benefits of wood-based products and energy (USDA Forest Service, Northern Institute of Applied Climate Science). Forest management produces diverse forest products that store carbon for decades or even centuries. This resource describes the role of forest management in providing carbon storage in harvested wood products and bioenergy."
  },
  {
    "objectID": "post/2021-02-04-forest-carbon-a-reading-list-for-beginners/index.html#carbon-markets",
    "href": "post/2021-02-04-forest-carbon-a-reading-list-for-beginners/index.html#carbon-markets",
    "title": "Forest carbon: a reading list for beginners",
    "section": "Carbon markets",
    "text": "Carbon markets\n\nAn introduction to forest carbon offset markets (North Carolina State Extension). A gentle introduction to forest carbon markets is provided in this resource. It includes a description of the different types of forest carbon offset projects, how voluntary and compliance markets differ, and how carbon credit registries work.\nFamily-owned forests: how to unlock the carbon potential in America’s backyard (American Forest Foundation). This white paper captures a lot of the energy behind carbon markets designed for small landowners, focusing on AFF’s program. Many of these voluntary markets have exploded in the last year, and this is a great reading that distills the language of carbon markets for a private landowner. You’ll need to sign up for their email list to download the white paper.\nState of the voluntary carbon markets (Ecosystem Marketplace). This document has been the best consistent resource that tracks the status of voluntary markets. Updated annually, this resource provides insights into voluntary carbon market dynamics, supply, and demand. You’ll need to sign up for their email list to download the document."
  },
  {
    "objectID": "post/2021-02-04-forest-carbon-a-reading-list-for-beginners/index.html#conclusion",
    "href": "post/2021-02-04-forest-carbon-a-reading-list-for-beginners/index.html#conclusion",
    "title": "Forest carbon: a reading list for beginners",
    "section": "Conclusion",
    "text": "Conclusion\nThere is an abundance of information on forest carbon on the web. To be able to discuss it the context of forest as natural climate solutions or in terms of the role forests play in carbon markets, these readings present the material in an approachable way.\nWhich documents did I miss? Send me an email with any suggestions.\n–\nBy Matt Russell. Sign up for my monthly newsletter for in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "events/2024-03-28-placing-harvested-wood-products-in-the-forest-carbon-discussion/index.html#section",
    "href": "events/2024-03-28-placing-harvested-wood-products-in-the-forest-carbon-discussion/index.html#section",
    "title": "Placing harvested wood products in the forest carbon discussion",
    "section": "",
    "text": "This presentation provides a background on harvested wood products from a carbon perspective, highlighting the role that wood durability and longevity plays in understanding how different wood product types contribute to carbon storage. It also describes a recent analysis that summarized annual harvest removals from Northeastern US states to estimate the longevity of harvested wood stored in products in use and tracked their decay to landfills and the atmosphere. A few tools are highlighted which show how foresters can determine wood product storage using forest inventory and timber sale data. These resources can be leveraged by foresters when engaged in discussions that focus on the role of forest products in forest carbon.\n\nVIEW SLIDES"
  },
  {
    "objectID": "events/2023-10-16-harvested-wood-products-and-forest-carbon/index.html#section",
    "href": "events/2023-10-16-harvested-wood-products-and-forest-carbon/index.html#section",
    "title": "Harvested wood products and forest carbon",
    "section": "",
    "text": "Harvested wood products store carbon from sustainably-managed forests. These slides were presented at the Northeast Silviculture Institute for Foresters in Gardner, MA on Oct. 11, 2023.\nREAD THE SLIDES."
  },
  {
    "objectID": "events/2023-03-19-quantifying-harvested-wood-products-and-forest-management-in-carbon-analyses/index.html#section",
    "href": "events/2023-03-19-quantifying-harvested-wood-products-and-forest-management-in-carbon-analyses/index.html#section",
    "title": "Quantifying harvested wood products and forest management in carbon analyses",
    "section": "",
    "text": "There remains great interest in understanding the role of forest management in storing and sequestering carbon. While researchers and land managers have a number of tools available to quantify the impacts of forest management on carbon stocks in growing forests, fewer tools are available that depict the carbon stored “off site” in the form of harvested wood products. These can include short-lived products (i.e., paper or pulp) or long-lived ones (i.e., utility poles). Not incorporating harvested wood product data in simulations of forest carbon can underestimate the total amount of carbon being stored, thus diminishing the benefits of active forest management. Fortunately tools like the Forest Vegetation Simulator (FVS), a growth and yield model widely used throughout New England, include output on harvested wood products. The harvested carbon report from FVS simulates carbon in harvested merchantable material and is proportioned into different pools, including products in use and in solid waste disposal sites. Carbon emissions output includes those from combustion with energy capture and from combustion or decay. The amount of carbon stored in a product pool within FVS changes over time as wood decays or transitions to another pool.\nIn these slides, an example case study using data from the long-term silvicultural experiment at the Penobscot Experimental Forest in central Maine will be shown. Results show that tools like FVS can be leveraged to better understand the contributions of harvested wood products in understanding forest management impacts and the longevity of forest carbon.\nREAD THE SLIDES."
  },
  {
    "objectID": "events/2022-09-27-pandemic-assistance-to-the-forest-industry-a-summary/index.html#section",
    "href": "events/2022-09-27-pandemic-assistance-to-the-forest-industry-a-summary/index.html#section",
    "title": "Pandemic assistance to the forest industry: a summary",
    "section": "",
    "text": "Forest products and timber harvesting and hauling businesses secured nearly $5 billion of federal funds through the Paycheck Protection Program and Pandemic Assistance to Timber Harvesters and Haulers programs to combat economic challenges presented by the COVID-19 pandemic. These slides were presented at the 2022 Society of American Foresters Convention in Baltimore, MD.\nREAD THE SLIDES."
  },
  {
    "objectID": "events/2022-09-27-a-review-of-the-log-trucking-workforce-and-comparisons-to-other-industries/index.html#section",
    "href": "events/2022-09-27-a-review-of-the-log-trucking-workforce-and-comparisons-to-other-industries/index.html#section",
    "title": "A review of the log trucking workforce and comparisons to other industries",
    "section": "",
    "text": "The trucking industry is faced with a number of challenges, including recruiting workers, pending retirements, and an increase in demand due to e-commerce sales. These slides provide an overview of the log trucking workforce and were presented at the Forest Resources Association’s Fall Board Meeting in Portland, ME.\nREAD THE SLIDES."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Forest analytics consulting\nArbor Custom Analytics LLC offers data and analytics consulting services to forestry companies and organizations. Browse our company slidedeck for more info.\n\nSpecializations include:\n\nForest analytics\nForest carbon assessment\nForest growth and yield modeling\nForest Vegetation Simulator modeling and scenario analysis\nStatistical modeling\nShort courses and workshops for forest and natural resource professionals\nIndependent research\n\n\n\n\n\n \n\nMatt Russell\nPresident and CEO\nWebpage | LinkedIn | Google Scholar | GitHub\n\nBackground\n\nExperience in forestry and analytics research, teaching, and outreach\nAuthor/co-author of over 80 peer-reviewed articles focusing on applied forestry research\nLeadership in Society of American Foresters: New England SAF (Newsletter Editor; 2023-current); Forest Science and Technology Board (Chair, 2021-2022); A1-Inventory and Biometrics Working Group (Officer, 2014-2017)\nMember of Society of American Foresters and Forest Products Society\n\n\n\nEducation\n\nUniversity of Maine, PhD in Forest Resources\nVirginia Tech, MS in Forestry\nPaul Smith’s College, BS in Forestry\n\n\n\n\nWant to get started on a project?\nSchedule a 30-minute call with Matt so we can learn more."
  },
  {
    "objectID": "events/2022-04-23-potential-improvements-to-support-forest-carbon-management-and-accounting/index.html#section",
    "href": "events/2022-04-23-potential-improvements-to-support-forest-carbon-management-and-accounting/index.html#section",
    "title": "Potential improvements to support forest carbon management and accounting",
    "section": "",
    "text": "Improvements are needed in the current forest inventory and monitoring programs to support forest carbon management and accounting. This presentation was a part of a Special Focus session of the Forest Inventory and Analysis National User Group.\nREAD THE SLIDES."
  },
  {
    "objectID": "events/2022-09-27-data-science-for-forestry-applications/index.html#section",
    "href": "events/2022-09-27-data-science-for-forestry-applications/index.html#section",
    "title": "Data Science for Forestry Applications",
    "section": "",
    "text": "This hands-on workshop showcased how data science can be used in forestry applications. Participants learned how to use open-source software (specifically R) to work quickly and efficiently with forestry data, with a focus on using large datasets such as Forest Inventory and Analysis data. This workshop was perfect for students and professionals who want to learn more about how to use R to work with the data they have.\n\nVIEW SLIDES"
  },
  {
    "objectID": "events/2022-12-01-recent-research-insights-in-forest-biomass-and-carbon-estimation/index.html#section",
    "href": "events/2022-12-01-recent-research-insights-in-forest-biomass-and-carbon-estimation/index.html#section",
    "title": "Recent research insights in forest biomass and carbon estimation",
    "section": "",
    "text": "Recent research within the forest biometrics community is helping to generate more precise estimates of forest carbon using new methods and technologies. This presentation was delivered to the California Air Resources Board Public Workshop on US Forest Projects Compliance Offset Protocol on November 30, 2022.\nREAD THE SLIDES."
  },
  {
    "objectID": "events/2023-05-22-sampling-design-and-statistical-approaches-in-forestry/index.html#section",
    "href": "events/2023-05-22-sampling-design-and-statistical-approaches-in-forestry/index.html#section",
    "title": "Sampling design and statistical approaches in forestry",
    "section": "",
    "text": "This workshop for Tag Ng showcased how sampling and statistics can be used in forestry applications. The two-hour virtual workshop discussed common sampling design used in forestry and statistical approaches with forest inventory data. Example coding sessions used the forestsamplr and tidymodels packages in R.\n\nVIEW SLIDES\nVIEW R CODE AND DATA"
  },
  {
    "objectID": "events/2023-12-08-unlocking-value-harvested-wood-products-in-forest-carbon-programs/index.html#section",
    "href": "events/2023-12-08-unlocking-value-harvested-wood-products-in-forest-carbon-programs/index.html#section",
    "title": "Unlocking value: harvested wood products in forest carbon programs",
    "section": "",
    "text": "Harvested wood products store carbon in many different forms. Increasingly, forest carbon programs are available to diverse forest landowners, and harvested wood products that result from enrolled properties need to be counted. These slides were presented at the Linkiwood 2023 Wood Tech Leadership Summit on Nov. 30, 2023.\nREAD THE SLIDES HERE.\nWATCH THE RECORDING (18:09) on YouTube."
  },
  {
    "objectID": "events.html",
    "href": "events.html",
    "title": "Events",
    "section": "",
    "text": "Placing harvested wood products in the forest carbon discussion\n\n\n\nForest carbon\n\n\nPresentation\n\n\n\nNew England Society of American Foresters Annual Winter Meeting.\n\n\n\n\n\n\nMar 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnlocking value: harvested wood products in forest carbon programs\n\n\n\nForest products\n\n\nPresentation\n\n\n\nPresentation at the Linkiwood 2023 Wood Tech Leadership Summit.\n\n\n\n\n\n\nNov 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHarvested wood products and forest carbon\n\n\n\nForest carbon\n\n\nPresentation\n\n\n\nPresentation at the Northeast Silviculture Institute for Foresters.\n\n\n\n\n\n\nOct 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling design and statistical approaches in forestry\n\n\n\nStatistics\n\n\nWorkshop\n\n\n\nOnline workshop for Tag Ng.\n\n\n\nMatt Russell\n\n\nMay 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantifying harvested wood products and forest management in carbon analyses\n\n\n\nForest carbon\n\n\nPresentation\n\n\n\nPresentation at the New England Society of American Foresters meeting.\n\n\n\n\n\n\nMar 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecent research insights in forest biomass and carbon estimation\n\n\n\nForest carbon\n\n\nPresentation\n\n\n\nPresentation for the California Air Resources Board.\n\n\n\n\n\n\nNov 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA review of the log trucking workforce and comparisons to other industries\n\n\n\nForest industry\n\n\nPresentation\n\n\n\nPresentation at the Forest Resources Association Fall Meeting.\n\n\n\n\n\n\nSep 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPandemic assistance to the forest industry: a summary\n\n\n\nForest industry\n\n\nPresentation\n\n\n\nPresentation at the SAF National Convention.\n\n\n\n\n\n\nSep 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Science for Forestry Applications\n\n\n\nData science\n\n\nWorkshop\n\n\n\nWorkshop at SAF National Convention.\n\n\n\n\n\n\nSep 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPotential improvements to support forest carbon management and accounting\n\n\n\nForest carbon\n\n\nPresentation\n\n\n\nPresentation for the FIA National User Group-Special Focus session.\n\n\n\n\n\n\nMar 21, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "newsletter.html",
    "href": "newsletter.html",
    "title": "Newsletter",
    "section": "",
    "text": "Here are two newsletter options to follow our work:\n\n\n\n\nThis biweekly(-ish) newsletter delivered through LinkedIn provides insights for professionals that work with forests and data. Anyone can read on LinkedIn, and you can also get an email when a new article publishes.\nREAD AND SUBSCRIBE HERE\n\n\n\n\n\n\nThis email newsletter provides in-depth analysis on data and analytics in the forest products industry. A rundown of what’s been on top of mind on over the last month. One email delivered on the last Thursday of every month.\nSUBSCRIBE HERE\nREAD PAST ISSUES"
  },
  {
    "objectID": "newsletter.html#linkedin-newsletter",
    "href": "newsletter.html#linkedin-newsletter",
    "title": "Newsletter",
    "section": "",
    "text": "This biweekly(-ish) newsletter delivered through LinkedIn provides insights for professionals that work with forests and data. Anyone can read on LinkedIn, and you can also get an email when a new article publishes.\nREAD AND SUBSCRIBE HERE"
  },
  {
    "objectID": "newsletter.html#email-newsletter",
    "href": "newsletter.html#email-newsletter",
    "title": "Newsletter",
    "section": "",
    "text": "This email newsletter provides in-depth analysis on data and analytics in the forest products industry. A rundown of what’s been on top of mind on over the last month. One email delivered on the last Thursday of every month.\nSUBSCRIBE HERE\nREAD PAST ISSUES"
  },
  {
    "objectID": "post/2021-02-12-a-forestproud-moment-after-listening-to-the-meateater-podcast/index.html",
    "href": "post/2021-02-12-a-forestproud-moment-after-listening-to-the-meateater-podcast/index.html",
    "title": "A #ForestProud moment after listening to the MeatEater podcast",
    "section": "",
    "text": "The MeatEater podcast.\n\n\n\n\nThe MeatEater podcast.\n\n\n\nThe MeatEater Podcast is one of the most widely listened to podcasts in the outdoor genre. It has over 33,000 ratings on Apple Podcasts and consistently ranks as one of the most downloaded podcasts in the outdoors and wilderness categories.\nI’m a semi-regular listener and enjoy the questions from listeners, hunting stories, and discussions on conservation issues. For a podcast that talks about the outdoors so much, I’ve always found it lacks a discussion on forests, one of the ecosystems that the MeatEater crew spends so much time in.\nConversations on trees, forests, and forest management have typically been few and far between on the podcast. This is somewhat surprising even as podcast host and famed author Steven Rinella often tells stories of his past career as a tree climber and arborist.\nForestry recently came up on an episode of the podcast. It was a great conversation that placed our profession in good light. On Episode 258 (“The Chit and the Poof”), guests Rick Hutton and Seth Morris described their time as forestry students in the forest technology program at Penn State Mont Alto.\nBoth mentioned their desires for wanting to enroll in a forestry program, for reasons that are familiar to many of us. Studying forestry would mean more time in the woods. It would make you a better outdoorsman and naturalist. The logic goes, a degree in forestry might be easy because the people that sign up for them are already familiar with the outdoors.\nOn the podcast, the guests reflected on being wrong about the “forestry is easy” idea. They mentioned enrolling in challenging classes in math and statistics as a part of their forest technician program. They reminisce on their dendrology class and describe the pain and headaches in learning how to identify 120 different tree species. You have to memorize Latin names for the family, genus, AND species!\nThe guests also mentioned the attrition of forestry students in their program and how they “dropped like flies” by the end of the program. They talk about concepts of tree growth and the definition of stands.\nThe lack of a basic understanding of trees and forests by some of the most famous outdoorsmen also comes up. This is summarized well when Rinella describes how some of the most seasoned and well-rounded deer hunters he knows often lack basic tree identification skills: “They can’t tell you what kind of tree their treestand is standing in.”\nWith an audience of hundreds of thousands of listeners, the MeatEater conversation sent a good message about our profession: a forestry education is challenging, but learning about trees can take your understanding of the natural world to the next level. After listening, the podcast made me feel proud of the message it sent to younger people thinking about becoming foresters in the future.\nListen to the podcast here. The conversation on forestry begins at minute 24:00 in the podcast and lasts for about 10 minutes.\n–\nBy Matt Russell. Email Matt with any questions or comments. Sign up for my monthly newsletter for in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "post/2021-03-20-an-overview-of-forest-carbon-markets-for-small-landowners/index.html#section-1",
    "href": "post/2021-03-20-an-overview-of-forest-carbon-markets-for-small-landowners/index.html#section-1",
    "title": "Five forest carbon markets for small landowners",
    "section": "",
    "text": "Voluntary forest carbon markets have expanded considerably over the last year. While compliance markets such as the California Air Resource Board’s Cap-and-Trade Regulation and their associated Forest Offset Protocol have existed for several years, it has primarily targeted large landowners with thousands of acres.\nOne of the benefits of newer voluntary forest carbon programs is that they typically have small minimum acreage requirements and shorter contract lengths. These markets may be the greatest opportunity for private forestland owners with small properties (e.g., less than 100 acres).\nWith the expansion of many voluntary forest carbon market programs, it’s difficult to keep track of them. Each program has their own minimum acreage. Contract lengths vary from one program to the next. Most allow timber harvesting, but many have restrictions about the volume of harvest allowed or the time period when harvesting can occur.\nHere is an overview of five voluntary forest carbon market programs that may be appealing to small landowners. Note that program enrollments continue to evolve and this is not meant to be a complete list of all available programs. This post only represents information obtained as of 12 March 2021."
  },
  {
    "objectID": "post/2021-03-20-an-overview-of-forest-carbon-markets-for-small-landowners/index.html#family-forest-carbon-program",
    "href": "post/2021-03-20-an-overview-of-forest-carbon-markets-for-small-landowners/index.html#family-forest-carbon-program",
    "title": "Five forest carbon markets for small landowners",
    "section": "Family Forest Carbon Program",
    "text": "Family Forest Carbon Program\nThe Family Forest Carbon Program by the American Forestry Foundation and The Nature Conservancy was created to bring together forest landowners and companies to address climate change. A pilot program has been completed in Pennsylvania and it will soon be coming to landowners in the Upper Great Lakes.\n\nMinimum acreage: 30 acres\nContract length: 20 years\nIs harvesting allowed?: In the Growing Mature Forests program, harvesting is allowed with some restrictions. In their Enhancing the Future Forest program, any recent harvest or upcoming harvest is allowed."
  },
  {
    "objectID": "post/2021-03-20-an-overview-of-forest-carbon-markets-for-small-landowners/index.html#core-carbon",
    "href": "post/2021-03-20-an-overview-of-forest-carbon-markets-for-small-landowners/index.html#core-carbon",
    "title": "Five forest carbon markets for small landowners",
    "section": "CORE Carbon",
    "text": "CORE Carbon\nThe CORE Carbon program by Finite Carbon uses a digital platform and connects landowners with sustainable management and carbon sequestration. Their website indicates enrollment in the program will begin in early 2021.\n\nMinimum acreage: 40 acres\nContract length: Unknown\nIs harvesting allowed?: Unknown"
  },
  {
    "objectID": "post/2021-03-20-an-overview-of-forest-carbon-markets-for-small-landowners/index.html#ncapx",
    "href": "post/2021-03-20-an-overview-of-forest-carbon-markets-for-small-landowners/index.html#ncapx",
    "title": "Five forest carbon markets for small landowners",
    "section": "NCAPX",
    "text": "NCAPX\nThe NCAPX program by SilviaTerra is a “data-driven forest carbon marketplace”. A pilot program has been completed in Pennsylvania. It will expand to 11 states in the US South in March 2021, followed by the rest of the US later in 2021.\n\nMinimum acreage: None\nContract length: One year\nIs harvesting allowed?: Focuses on deferring harvest for over the annual contract year."
  },
  {
    "objectID": "post/2021-03-20-an-overview-of-forest-carbon-markets-for-small-landowners/index.html#forest-carbon-works",
    "href": "post/2021-03-20-an-overview-of-forest-carbon-markets-for-small-landowners/index.html#forest-carbon-works",
    "title": "Five forest carbon markets for small landowners",
    "section": "Forest Carbon Works",
    "text": "Forest Carbon Works\nThe Forest Carbon Works program makes forest carbon projects accessible for small landowners. The program employs technology that uses a smartphone app to reduce the costs of inventorying forest carbon.\n\nMinimum acreage: 40 acres\nContract length: 125 years, in renewable six year contract periods\nIs harvesting allowed?: Harvesting shall not have occurred in the ten years prior to enrolling; must forgo harvesting in the first six years after enrolling."
  },
  {
    "objectID": "post/2021-03-20-an-overview-of-forest-carbon-markets-for-small-landowners/index.html#bluesource",
    "href": "post/2021-03-20-an-overview-of-forest-carbon-markets-for-small-landowners/index.html#bluesource",
    "title": "Five forest carbon markets for small landowners",
    "section": "Bluesource",
    "text": "Bluesource\nThe Bluesource program manages North America’s largest carbon offset portfolio at over 2.3 million acres. Bluesource is also involved in other environmental services like biogas and methane reduction. It has several projects ongoing in northern US states.\n\nMinimum acreage: 3,000 acres\nContract length: 40-100 years (depending on voluntary or compliance market)\nIs harvesting allowed?: Harvesting is permitted, so long as it does not exceed growth.\n\nI encourage you to visit the websites mentioned to learn more about the requirements of each program. Many have email lists that keep you updated on the status and enrollment in the program.\n–\nBy Matt Russell. Email Matt with any questions or comments. Sign up for my monthly newsletter for in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "post/2021-05-15-harvested-wood-products-an-increasing-contributor-to-us-carbon-storage/index.html#section",
    "href": "post/2021-05-15-harvested-wood-products-an-increasing-contributor-to-us-carbon-storage/index.html#section",
    "title": "Harvested wood products: a growing contributor to US carbon storage",
    "section": "",
    "text": "There has been a lot of talk recently about the importance of carbon in forests. Carbon dioxide is the leading source of greenhouse gas emissions, resulting from burning fossil fuels for transportation and energy and producing electricity, among other sources. Trees sequester carbon dioxide from the air and turn it into carbon stored in wood. As a result, healthy trees and forests increase carbon storage and avoid greenhouse gas emissions. Many entrepreneurs and policymakers have sought to capitalize on these “natural climate solutions” that trees provide by offering payments to landowners for the carbon benefits their trees provide or by offering incentives to implement “climate-smart” forest management practices.\nForests are a net sink of carbon in the United States, meaning they absorb more carbon dioxide from the atmosphere than they emit. Recent analysis from the USDA Forest Service estimate that in 2019, forests and harvested wood products offset more than 11 percent of all of the greenhouse gas emissions.\nEven with all of the major forest disturbances that have recently occurred, such as wildfires, blowdowns, and insect and disease outbreaks, forests in the United States continue to store more carbon. The total amount of carbon stored in US forests has ranged from 50,913 million metric tons in 1990 to 55,993 million metric tons in 2019. Foresters and loggers may often deal with carbon in the live trees, but trees are not the only component where carbon is stored in forests. In addition to live trees (including the stem, branches, and roots), carbon is also found in pools such as the deadwood (standing snags and coarse woody debris), litter, and soil.\nThe carbon stored in harvested wood products (HWPs) is also included in estimates of forest carbon. This is important because it counts the carbon benefits of wood that the forest products industry provides. For example, a mass timber building designed and built with wood will “lock up” the carbon in those trees, storing the carbon for decades or even centuries later. This has tremendous benefits when compared to building with other more greenhouse gas-intensive materials such as concrete or steel.\nCarbon stored in HWPs includes estimates from two primary sectors: products in use and solid waste disposal sites (SWDS). Harvested wood products include all wood-derived products such as furniture, plywood, paper and energy uses. Wood that is harvested remains in use in products for different lengths of time: think a piece of paper with a short life cycle compared to a utility pole that can serve its use for up to a century. In a complete life cycle of carbon accounting, different wood products vary in how fast or slow they decay, related to how much potential they will ultimately emit back to the atmosphere.\nSolid waste disposal sites also store a tremendous amount of wood, most notably when wood products reach the end of their life cycle. The sites include landfills and dumps where wood is slowing decaying for decades. For example, a common metric is to assume that the half-life for wood in a landfill is 29 years. That is, if would take 29 years for half of the biomass of the wood to decay.\nEstimates for the carbon stored in HWPs in use and SWDS have increased steadily since 1990. In the USDA Forest Service’s recent estimates, these pools stored 1,532 and 1,167 million metric tons of carbon in 2019, a 23% and 81% increase since 1990:\n\n\n\n\n\n\n\n\n\nThe carbon stored in HWPs has outpaced growth increases in the forests themselves. The amount of carbon stored in forests is much greater than that stored in HWPs, yet growth increases in forests have increased by 10% since 1990. Because of this, carbon stored in HWPs has contributed increasingly more to the total carbon storage across the US in recent years when measured on a relative scale.\nIn 1990, HWPs represented 3.7% of all forest carbon stored in the US. In 2019, HWPs represented nearly 5% of all carbon stocks:\n\n\n\n\n\n\n\n\n\nIn the US, HWPs have long been a net sink for carbon. Recent increases in the contribution of HWPs are the result of a productive economy over the last several decades. While economic downturns such as the Great Recession turned HWP pools in the US briefly into a source rather than a sink, production and use of HWPs quickly rebounded. As consumers continue to use products derived from wood, the carbon benefits of those products will continue to be valued into the future.\nDespite many recent challenges to the forest products industry including mill closures and downsizes related to the COVID-19 pandemic, the increasing role of HWPs in the US’ “carbon portfolio” is a bright side that should make its workers proud. The increased use of HWPs represents a trend in the US: building and using forest products provides a number of environmental benefits. In particular as wood is being used in manufacturing to replace more greenhouse gas intensive materials like concrete and steel, the forest products industry can point to the carbon and climate benefits of building with wood.\n–\nBy Matt Russell. Email Matt with any questions or comments. Sign up for my monthly newsletter for in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "post/2021-07-19-peer-review-lessons-learned-from-reviewing-100-forest-science-manuscripts/index.html",
    "href": "post/2021-07-19-peer-review-lessons-learned-from-reviewing-100-forest-science-manuscripts/index.html",
    "title": "Peer review: Lessons learned from reviewing 100 forest science manuscripts",
    "section": "",
    "text": "Photo by Annie Spratt on Unsplash.\nThis summer I completed my 100th peer review of a manuscript in forest science. Peer review is mostly a non-compensated service to the profession, but the process probably works as best as any other system for evaluating the merit and impact of scientific studies. The individuals best qualified to evaluate scientific work are scientists working on similar research problems.\nFor the process, typically a journal will reach out to a reviewer over email with an invitation. The reviewer has between two and six weeks to read the manuscript, write a thoughtful review, and provide comments back to the journal through their online editorial management website. The process generally works fine, provided the associate editor of a journal can (1) find enough reviewers to evaluate the manuscript and (2) receive comments from reviewers in a timely manner.\nAs a graduate student, the peer review process was a black box to me. I remember attending a national forestry conference and seeing on the agenda a meeting scheduled for one of forestry’s most well-known journals. Next to the event listing on the agenda was listed in big italics: For Associate Editors only. What were they talking about in there? How to take over the world? Even after completing a healthy share of manuscript reviews, some aspects of peer review are still curious to me.\nIn the effort to share more about experiences in peer review, below I share some takeaways as I reflect on completing my 100th peer review. I separate them into strategies and tactics:"
  },
  {
    "objectID": "post/2021-07-19-peer-review-lessons-learned-from-reviewing-100-forest-science-manuscripts/index.html#put-yourself-in-a-growth-mindset-as-you-review",
    "href": "post/2021-07-19-peer-review-lessons-learned-from-reviewing-100-forest-science-manuscripts/index.html#put-yourself-in-a-growth-mindset-as-you-review",
    "title": "Peer review: Lessons learned from reviewing 100 forest science manuscripts",
    "section": "Put yourself in a growth mindset as you review",
    "text": "Put yourself in a growth mindset as you review\nAfter being asked to evaluate someone else’s work, it’s easy to put on your “critiquing glasses” and immediately begin finding flaws in a study. While pointing out flaws is certainly one of the responsibilities of reviewers, instead, consider your time with the review as a learning experience. This will allow you to have a favorable view on the peer review experience.\nFor example, I’ve never set foot in a forest in eastern Africa, but I’ve reviewed manuscripts from the region. Having the opportunity to review work from Ghana allowed me to learn more about the deforestation of hardwood forests and the contribution of the forest products industry in the country.\nGood reviewers will get asked to provide reviews more than bad reviewers. This is fortunate for the journal, but unfortunate for you. As you publish more articles and simultaneously conduct more reviews, the number of review requests sent your way will increase. Do all that you can to use your peer reviews as a learning experience instead of just a task that needs to be checked off your list."
  },
  {
    "objectID": "post/2021-07-19-peer-review-lessons-learned-from-reviewing-100-forest-science-manuscripts/index.html#good-science-is-easy-to-review-poor-science-takes-time",
    "href": "post/2021-07-19-peer-review-lessons-learned-from-reviewing-100-forest-science-manuscripts/index.html#good-science-is-easy-to-review-poor-science-takes-time",
    "title": "Peer review: Lessons learned from reviewing 100 forest science manuscripts",
    "section": "Good science is easy to review, poor science takes time",
    "text": "Good science is easy to review, poor science takes time\nIt is difficult to judge how long a quality review will take. Certainly an 80-page manuscript will take more time than a 20-page manuscript, but I’ve learned to block out more time than I think is needed for any review.\nGreat manuscripts take a shorter time to review than poorly-written manuscripts. All manuscripts have at least some aspects of them that can be improved. The educator in me always wants to provide useful feedback even if it’s poor work. I once heard someone say that with anything you write, you should always imagine your grandmother is going to read it. Be critical and provide feedback on the work, but don’t do it in a way that is condescending, rude, or my favorite new term, “mansplains”."
  },
  {
    "objectID": "post/2021-07-19-peer-review-lessons-learned-from-reviewing-100-forest-science-manuscripts/index.html#keep-track-of-your-reviews",
    "href": "post/2021-07-19-peer-review-lessons-learned-from-reviewing-100-forest-science-manuscripts/index.html#keep-track-of-your-reviews",
    "title": "Peer review: Lessons learned from reviewing 100 forest science manuscripts",
    "section": "Keep track of your reviews",
    "text": "Keep track of your reviews\nFor most academics with research appointments, peer review is a service activity that is encouraged by university department heads and administrators. Documenting your peer review activities will benefit you as you gather materials for your tenure and promotion. How quickly you can forget if you reviewed that paper for this journal or that one, even a month after you complete the review.\nFor graduate students and other early career researchers, having experience with conducting a few peer reviews and putting them on your CV will set you apart from others at a similar career stage.\nHistorically, how many peer reviews a researcher conducts has been largely known only to themselves. Fortunately, websites like Publons allow you to digitally track your peer review contributions and see how many reviews your colleagues have done. I’ve been using Publons since 2017 and have found it to be a useful tool that peeks “behind the curtain” of the peer review process.\nIn addition to tools like Publons, I suggest keeping your own spreadsheet with information such as manuscript title, journal the work was submitted to, country/region of study, your decision on the manuscript (e.g., reject or resubmit with revisions), and the journal’s ultimate decision about the manuscript. Most journals in the forest sciences will correspond with reviewers via email on the final decision of the manuscript. In keeping my own data of peer review contributions up-to-date, I’m able to share graphs like the following with my department head:"
  },
  {
    "objectID": "post/2021-07-19-peer-review-lessons-learned-from-reviewing-100-forest-science-manuscripts/index.html#read-reviews-written-by-others",
    "href": "post/2021-07-19-peer-review-lessons-learned-from-reviewing-100-forest-science-manuscripts/index.html#read-reviews-written-by-others",
    "title": "Peer review: Lessons learned from reviewing 100 forest science manuscripts",
    "section": "Read reviews written by others",
    "text": "Read reviews written by others\nI am easily persuaded by reading what other experts say. Another benefit of the review process is that most journals in the forest sciences will forward all review comments to both the authors and all peer reviewers. This gives you the benefit of understanding how other researchers interpreted the study.\nReading reviews written by others can confirm your criticisms (e.g., “I’m glad the other reviewers found the error in how they ran the model…”). Reading other reviews can also open your mind to aspects of the study you missed (e.g., “Why didn’t I spot that they didn’t explain that graph?…”) or provide you a different perspective on an issue or interpretation of a finding (e.g., “I didn’t see that part of the analysis as a major part of the work, but the reviewer raises a good point…”).\nTake time to read the review comments that others provided to work that you reviewed, and you will learn much more about the work."
  },
  {
    "objectID": "post/2021-07-19-peer-review-lessons-learned-from-reviewing-100-forest-science-manuscripts/index.html#journal-editors-celebrate-peer-reviewers",
    "href": "post/2021-07-19-peer-review-lessons-learned-from-reviewing-100-forest-science-manuscripts/index.html#journal-editors-celebrate-peer-reviewers",
    "title": "Peer review: Lessons learned from reviewing 100 forest science manuscripts",
    "section": "Journal editors: celebrate peer reviewers",
    "text": "Journal editors: celebrate peer reviewers\nWith the rise of digital publishing and databases such as Web of Science and Google Scholar, finding metrics on the relative impact of journals is relatively straightforward. Google lists forestry’s top journals and additional information such as impact factors is often reported on journal websites.\nMuch of the success in journals is due to the peer reviewers that provide volunteer service. Little information about peer reviewers is found on journal websites aside from an annual appreciation list published by the journal. Some journals like the Canadian Journal of Forest Research dole out Outstanding Reviewer awards, but this recognizes a small portion of the total reviewer pool. More actions that recognize the service of peer reviewers should be initiated by forestry journals, in particular for reviewers early in their career stage and from diverse backgrounds."
  },
  {
    "objectID": "post/2021-07-19-peer-review-lessons-learned-from-reviewing-100-forest-science-manuscripts/index.html#ask-about-the-peer-review-process.",
    "href": "post/2021-07-19-peer-review-lessons-learned-from-reviewing-100-forest-science-manuscripts/index.html#ask-about-the-peer-review-process.",
    "title": "Peer review: Lessons learned from reviewing 100 forest science manuscripts",
    "section": "Ask about the peer review process.",
    "text": "Ask about the peer review process.\nPeer review and publishing in journals are not exactly transparent processes. Journal websites will only provide so much information. Don’t hesitate to ask your senior colleagues about their experiences with peer review or reach out to Associate Editors listed on journal websites. This can include their experiences with the peer review process, differences in journals they selected, and recommendations for outlets for your work.\nIn 2015, Aaron Weiskittel published a great paper on peer review in the forest sciences. It is well worth reading for its perspectives on roles and responsibilities in peer review. It also describes several alternatives to the process of traditional peer review."
  },
  {
    "objectID": "post/2021-07-19-peer-review-lessons-learned-from-reviewing-100-forest-science-manuscripts/index.html#review-two-papers-for-every-paper-you-publish",
    "href": "post/2021-07-19-peer-review-lessons-learned-from-reviewing-100-forest-science-manuscripts/index.html#review-two-papers-for-every-paper-you-publish",
    "title": "Peer review: Lessons learned from reviewing 100 forest science manuscripts",
    "section": "Review two papers for every paper you publish",
    "text": "Review two papers for every paper you publish\nThis was a recommendation by Weiskittel, and I have tried to keep to this ratio. If you can imagine that two or three reviewers provide comments on every manuscript, this ratio makes sense. For me, this means I’m reviewing a paper about once a month, so I generally always have a manuscript on my desk to review.\nFrom the journal’s perspective, it is often difficult to find enough reviewers to agree to complete a review, much less provide a quality review in return. This is magnified by the fact that the number of forestry researchers has declined by 12% since 2002, yet the number of journals and articles in forestry has increased. Don’t review too much or too little, but find the right number of manuscripts that you should be reviewing on an annual basis."
  },
  {
    "objectID": "post/2021-07-19-peer-review-lessons-learned-from-reviewing-100-forest-science-manuscripts/index.html#dont-review-too-much-for-a-single-journal",
    "href": "post/2021-07-19-peer-review-lessons-learned-from-reviewing-100-forest-science-manuscripts/index.html#dont-review-too-much-for-a-single-journal",
    "title": "Peer review: Lessons learned from reviewing 100 forest science manuscripts",
    "section": "Don’t review too much for a single journal",
    "text": "Don’t review too much for a single journal\nJust like you don’t want to write for only one journal, don’t review for the same journal over and over. For the journal editor, it certainly makes their lives easier if you continue to accept review requests and provide quality feedback. I once learned that a colleague was able to have the article processing charge (APC) for a manuscript he submitted waived because he provided seven reviews for a journal within the last 12 months. Every article review was rewarded with a few hundred dollars off the APC. While a nice gesture of appreciation by the journal, I think the journal received more value from his numerous review comments than they did for a single APC from an author.\nA good journal should value feedback from a diversity of reviewers. Journal editors should also recognize that you’ve been contributing a lot of reviews by keeping track of your efforts in their online system.\nI once replied back to a journal asking to be removed from their reviewer list because they asked me to review a new paper while I was already reviewing another one for their journal. While this might have been an honest mistake, everyone’s time is valuable and editors should recognize that.\nDon’t hesitate to decline reviews or ask not to be sent reviews by a specific journal. To diversify the journals you review for, sign up to be a reviewer on journal websites or reach out to senior colleagues inquiring if you can help them with a review for a new journal."
  },
  {
    "objectID": "post/2021-07-19-peer-review-lessons-learned-from-reviewing-100-forest-science-manuscripts/index.html#separate-between-general-comments-and-minor-comments.",
    "href": "post/2021-07-19-peer-review-lessons-learned-from-reviewing-100-forest-science-manuscripts/index.html#separate-between-general-comments-and-minor-comments.",
    "title": "Peer review: Lessons learned from reviewing 100 forest science manuscripts",
    "section": "Separate between general comments and minor comments.",
    "text": "Separate between general comments and minor comments.\nYour ability to make your review comments easy to digest and comprehend will benefit you, the authors, and the Associate Editor. Editors first like to see that the reviewer has understood and interpreted the work presented in the manuscript. Then, by starting your review with a narrative of your general comments on the work, you will show the editor that you’ve reflected on the entire work and synthesized it in a comprehensive way.\nMinor comments can be written in a bulleted list, but if your comments are more than two sentences on a topic, it might deserve to be treated as a general comment. As an AE I loathe when reviewers attach a Word document with Track Changes and edits as a part of their review. Your job as a peer reviewer is to provide feedback to the authors to improve the quality of the work and to judge the scientific merit of a study. Reviewers review, authors edit."
  },
  {
    "objectID": "post/2021-07-19-peer-review-lessons-learned-from-reviewing-100-forest-science-manuscripts/index.html#print-the-manuscript.",
    "href": "post/2021-07-19-peer-review-lessons-learned-from-reviewing-100-forest-science-manuscripts/index.html#print-the-manuscript.",
    "title": "Peer review: Lessons learned from reviewing 100 forest science manuscripts",
    "section": "Print the manuscript.",
    "text": "Print the manuscript.\nThere is value in reading things on paper, and I like to do this with manuscripts. Reviewing a paper on a screen is problematic for me as I often switch over to my email or other task which distracts me. Printing on paper also allows me to take the paper with me anywhere, whereas relying on a screen means I can only review the paper at my desk or where I can take my laptop.\nI like to read the manuscript first to understand what the authors set out to accomplish and their results. Then, during a second read through I’ll make notes and annotations in the margins. After that I’ll type my responses as I thumb through the manuscript and my annotations."
  },
  {
    "objectID": "post/2021-08-12-new-ebook-forest-carbon-by-the-numbers/index.html#section-2",
    "href": "post/2021-08-12-new-ebook-forest-carbon-by-the-numbers/index.html#section-2",
    "title": "New eBook: Forest Carbon by the Numbers",
    "section": "",
    "text": "Now available on Amazon is Forest Carbon by the Numbers: Understanding the Hottest Forest Product of the 2020s.\nCarbon is becoming a highly valued commodity of forests. Trees sequester carbon dioxide and turn it into wood, storing carbon in forests and long-lived forest products for long time periods. Despite the increased recognition of carbon as a natural climate solution and several emerging markets that monetize it, few professionals in forestry and natural resources have a grasp on the amount and extent of forest carbon in the ecosystems they work in.\nIn Forest Carbon by the Numbers, I share insights and visualizations of the current status and trends in forest carbon and how it impacts the forest products industry.\nIn Forest Carbon by the Numbers, you will understand:\n\nTerminology associated with forest carbon measurements and markets,\nHow forest carbon is quantified (from individual tree to landscape scales),\nHow to leverage forest inventory data to understand forest biomass and carbon, and\nTrends and future opportunities in forest carbon.\n\nGET THE EBOOK to learn more about forest carbon and how it can be better understood in your own organization.\n–\nBy Matt Russell. Email Matt with any questions or comments. Sign up for my monthly newsletter for in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "post/2021-09-07-the-importance-of-citizen-data-scientists-in-forest-analytics/index.html#citizen-data-scientists-in-forest-analytics",
    "href": "post/2021-09-07-the-importance-of-citizen-data-scientists-in-forest-analytics/index.html#citizen-data-scientists-in-forest-analytics",
    "title": "The importance of citizen data scientists in forest analytics",
    "section": "Citizen data scientists in forest analytics",
    "text": "Citizen data scientists in forest analytics\nThere are likely citizen data scientists working with you that can provide value to your next analytics project. Citizen data scientists are practitioners that are involved in data science and analytics work, but their primary role is outside of the data analysis group within an organization.\nCitizen data scientists often have the following traits within an organization:\n\nThey understand the structure and mission of the organization very well.\nThey have previous experience in applying analytics to solve problems in the organization.\nThey have expertise in a domain of interest (e.g., forestry or natural resources).\nThey offer a unique perspective.\n\nCitizen data scientists are analytically-minded individuals that can be used to fill the gap of an organization’s analytics needs. As many are aware, there is a shortage of highly-trained biometricians and data scientists within the forestry and natural resources profession. Add on top of that the reduction of research in forestry and the increasing reliance on data and analytics to inform business decisions, you will recognize the value of citizen data scientists in your organization.\nIn forestry organizations, the job titles of citizen data scientists are likely diverse. At the foundation, foresters and field crew technicians provide timely updates and feedback on what is working and not working when collecting data. While most forest analytics happens in front of a computer screen, there is no replacing the insights and perspectives that boots on the ground can provide when interpreting the data that foresters collect.\nCitizen data scientists are also likely professionals in your organization that have vast experiences in analyzing forestry data. They might be well versed in software like Excel and the Esri suite of products. While they may not be acquainted with recently developed analytics tools like machine learning and cloud computing, they can continue to provide value to your organization.\nThe best way to empower citizen data scientists in your organization is to democratize your data and analytics processes. Instead of permitting only your “numbers team” to do your analytics work, allow all practitioners the opportunity to weigh in, provide feedback, and analyze datasets when needed. In short, listen to all of the practitioners that have a stake in the collection and analysis of your data, and you will add value to your team and ultimately the work that results from it."
  },
  {
    "objectID": "post/2021-09-07-the-importance-of-citizen-data-scientists-in-forest-analytics/index.html#summary",
    "href": "post/2021-09-07-the-importance-of-citizen-data-scientists-in-forest-analytics/index.html#summary",
    "title": "The importance of citizen data scientists in forest analytics",
    "section": "Summary",
    "text": "Summary\nCitizen data scientists are practitioners that do not have formal data science and analytics roles, but provide value to analytics teams within organizations. They often have previous experience in analytics applications or play an important role in the collection of data. By making your analytics processes more transparent and available, you can empower citizen data scientists in your organization to bring it more value."
  },
  {
    "objectID": "post/2021-11-13-h-2b-workers-essential-to-the-forest-products-industry/index.html#section-1",
    "href": "post/2021-11-13-h-2b-workers-essential-to-the-forest-products-industry/index.html#section-1",
    "title": "H-2B workers: essential to the forest products industry",
    "section": "",
    "text": "In Cabinet Battle #1, one of the most popular songs from the musical Hamilton, Alexander Hamilton and Thomas Jefferson discuss the role of the federal government in assuming the financial debt of states. It’s the late 1780s, and Hamilton has ideas for a national bank that Jefferson and many Southerners aren’t fans of.\nNear the end of Hamilton’s verse, he states that it’s odd to take “civics lessons from a slaver”. He concludes by commenting on the South’s agricultural prosperity after saying “we know who’s really doing the planting.”\nThe scene is a useful reminder of the labor issues that play a large role in sectors such as agriculture. In particular, amid an ongoing supply chain crisis and the Great Resignation, labor issues continue to plague many organizations going into 2022.\nIn the forest products industry, H-2B guest workers help to provide it with essential labor costs that otherwise cannot be filled. By no means am I equating issues relating to guest workers in forestry to slavery in the 18th century, but it brings a reminder of how many forestry practices are implemented across US forests.\n\nH-2B workers are essential to forestry\nGuest workers under the H-2B program are essential for many organizations in the forest products industry. Guest workers help in efforts to plant trees, conduct site preparation and intermediate stand treatments, and work in nurseries and greenhouses to ensure tree seedlings are grown and planted with success.\nI wrote last year that around 15,000 certified guest workers are employed in forestry-related occupations. While the total number of H-2B guest workers in forestry-related occupations has increased by 77% since 2011, challenges still remain to ensure forestry operations can continue to hire H-2B workers.\n\n\nThe need for strong H-2B guest worker programs\nOur recent analysis in the Journal of Forestry surveyed 70 forestry businesses to inquire about their reliance on guest workers in the H-2B program. A few of of primary findings were:\n\nForestry businesses reported only 2% of advertised forestry jobs were filled by American workers, with the remaining 98% by H-2B visa guest workers. This reflects the strong need for an H-2B program that is supported.\nForestry businesses noted that it is becoming increasingly difficult to hire enough guest workers to complete forestry-related work, reflecting the limited supply of visas.\nApproximately 97% of forestry companies used H-2B guest workers for tree planting. Other common forestry practices done by guest workers included thinning (64% of companies), herbicide and vegetation control (61%), and clearing brush (57%).\n\nForestry companies have cited the need for a strong H-2B program to address current and future challenges relating to tree planting and maintaining forestry operations. As one respondent to our survey stated: “These workers are crucial to the reforestation on our forestland. Without them I would be forced to go out of business.”\n–\nBy Matt Russell. Email Matt with any questions or comments. Sign up for my monthly newsletter for in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "post/2021-12-17-most-read-articles-in-2021/index.html",
    "href": "post/2021-12-17-most-read-articles-in-2021/index.html",
    "title": "Most read articles in 2021",
    "section": "",
    "text": "Andrew Neel/Unsplash"
  },
  {
    "objectID": "post/2021-12-17-most-read-articles-in-2021/index.html#section",
    "href": "post/2021-12-17-most-read-articles-in-2021/index.html#section",
    "title": "Most read articles in 2021",
    "section": "",
    "text": "In 2021, 26 new posts were published to this blog on topics relating to data and forest products. Have an idea for a post you’d like to see? Email Matt with any suggestions.\nHere are the most read posts from the Arbor Custom Analytics blog in 2021:\n\nGetting your data into R from Google Sheets\nCreating animated bar charts to visualize sawtimber prices\nMixed models in R: a primer\nHow to created paged HTML documents with pagedown\nPresenting regression results better with forest plots\n\n–\nBy Matt Russell. Sign up for my monthly newsletter for in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "post/2022-02-22-what-does-the-price-of-carbon-have-to-be-for-landowners-to-enroll-in-carbon-markets/index.html#introduction",
    "href": "post/2022-02-22-what-does-the-price-of-carbon-have-to-be-for-landowners-to-enroll-in-carbon-markets/index.html#introduction",
    "title": "What does the price of carbon have to be for landowners to enroll in carbon markets?",
    "section": "Introduction",
    "text": "Introduction\nCarbon has quickly become one of the most talked about commodities in our forests today. A variety of new and emerging carbon market programs are paying landowners for the carbon in their forests Carbon is the currency of interest when discussing trees and forests as natural climate solutions.\nWhile compliance-driven markets such as the California Air Resource Board’s (CARB) Cap-and-Trade Regulation and their associated Forest Offset Protocol have existed for several years, it has primarily targeted large landowners with thousands of acres. Forest inventory costs to enroll property in the CARB program are high and 100-year contract lengths are signed. Most recently, the offering price of one metric ton of carbon dioxide was around $17.00 in the CARB program, with auction prices reaching $28.00 in November 2021.\nA number of voluntary forest carbon markets have expanded considerably over the last several years. One of the novelties of these voluntary forest carbon programs is that they typically have small minimum acreage requirements and shorter contract lengths. Voluntary markets may be the greatest opportunity for private forestland owners with small properties (e.g., less than 100 acres). These markets also provide unique ways to reward landowners with carbon payments.\nFor example, the NCX program is a “data-driven forest carbon marketplace” that operates on a harvest deferral model for one year. It is currently enrolling landowners across the continuous US. In contrast, the Family Forest Carbon Program by the American Forestry Foundation and The Nature Conservancy is an incentive-based program that recently completed a pilot in Pennsylvania, with plans to enroll landowners in the US Lake States in 2022. Contract lengths in this program are for 20 years and landowners can choose to enroll in their Growing Mature Forests or Enhancing the Future Forest programs.\nGiven the increased attention in providing payments to landowners for carbon, at what price does carbon need to be to encourage landowners to enroll in carbon markets? Fortunately, several studies have been conducted in the last decade that quantify landowner willingness to participate in carbon markets and the prices they are willing to accept."
  },
  {
    "objectID": "post/2022-02-22-what-does-the-price-of-carbon-have-to-be-for-landowners-to-enroll-in-carbon-markets/index.html#a-review-of-carbon-price-studies",
    "href": "post/2022-02-22-what-does-the-price-of-carbon-have-to-be-for-landowners-to-enroll-in-carbon-markets/index.html#a-review-of-carbon-price-studies",
    "title": "What does the price of carbon have to be for landowners to enroll in carbon markets?",
    "section": "A review of carbon price studies",
    "text": "A review of carbon price studies\nSome new voluntary carbon programs operate on a harvest deferral model. For example, the NCX program operates an annual harvest deferral. In this model, landowners will chose to forgo timber harvesting in favor of allowing their forest to store and sequester carbon and receive a carbon payment. It is fitting to summarize what the annual price of carbon would be need to be on a per acre basis if landowners seek to enroll.\nTable 1 summarizes five different studies across different regions in the US that analyzed this. Most studies analyzed private forest landowners through a mix of surveys or simulation analyses to quantify the price that landowners are willing to accept for carbon.\n\n\n\n\nPrices that forest landowners are willing to accept on a per acre basis for storing carbon annually.\n\n\nState/Region\nPrice ($/acre)\nStudy\nNotes\n\n\n\n\nVermont\n$5 to $15\nWhite et al. 2018\nResponses from a mail survey to landowners in the state’s Current Use Program.\n\n\nTexas\n$20 to $27\nSimpson and Li 2010\nValues range from being willing to accept a five-year contract versus a conservation easement.\n\n\nSoutheastern US\n$4 to $49\nTanger et al. 2021\nPayment rates vary depending on four treatments including different ages at thinning and final harvest.\n\n\nFlorida\n$20 to $30\nSoto et al. 2016\nConclusion that amount would have significantly stronger impacts on enrollment than $5 or $10.\n\n\nLake States\n$18 to $28\nMiller et al. 2012\n$18 payment required to generate 50% participation rate. $28 payment for survey respondents that expressed a high certainty in their response to a valuation question.\n\n\n\n\n\n\n\n\nAveraged across these studies, forest landowners are willing to accept $21.60 per acre for storing carbon annually. Although Table 1 is not a complete list of studies and prices are not all in 2021 dollars, it should be noted that this price might be competitive with stumpage prices for lower quality wood across most US regions.\nIn addition to comparing between stumpage rates and product classes that a landowner’s timber might provide, another key factor is incorporating the management costs incurred by a landowner. In particular for regions in the US with relatively short rotation ages (e.g., loblolly pine planted in the US Southeast), carbon payments that landowners would be willing to accept would likely be impacted by past and future silvicultural treatments, including ages of thinning operations and selection of the year of final harvest. Tree growth rates will vary across regions and species, which subsequently need to be taken into account when quantifying the potential for a forest to store and sequester carbon."
  },
  {
    "objectID": "post/2022-02-22-what-does-the-price-of-carbon-have-to-be-for-landowners-to-enroll-in-carbon-markets/index.html#conclusion",
    "href": "post/2022-02-22-what-does-the-price-of-carbon-have-to-be-for-landowners-to-enroll-in-carbon-markets/index.html#conclusion",
    "title": "What does the price of carbon have to be for landowners to enroll in carbon markets?",
    "section": "Conclusion",
    "text": "Conclusion\nMany carbon markets now appeal to a variety of landowners, including small private landowners. The current price of carbon on an annual basis will be a large determinant of a landowner’s willingness to enroll in a carbon market. A review of several research-based studies indicates an average landowner willingness to accept $21.60 per acre for storing carbon annually. Forest resources professionals should engage with landowners to help them determine if and how managing for carbon can be incorporated into their management plans."
  },
  {
    "objectID": "post/2022-02-22-what-does-the-price-of-carbon-have-to-be-for-landowners-to-enroll-in-carbon-markets/index.html#references",
    "href": "post/2022-02-22-what-does-the-price-of-carbon-have-to-be-for-landowners-to-enroll-in-carbon-markets/index.html#references",
    "title": "What does the price of carbon have to be for landowners to enroll in carbon markets?",
    "section": "References",
    "text": "References\nMiller, K.A., S.A. Snyder, and M.A. Kilgore. 2012. An assessment of forest landowner interest in selling forest carbon credits in the Lake States, USA. Forest Policy and Economics 25:113- 122.\nSimpson, H., and Y. Li. 2010. Environmental credit marketing survey report (PDF).\nSoto, J.R., D.C. Adams, and F.J. Escobedo. 2016. Landowner attitudes and willingness to accept compensation from forest carbon offsets: Application of best–worst choice modeling in Florida USA. Forest Policy and Economics 63:35-42.\nTanger, S.M., B. da Silva, and M.E. McDill. 2021. Cut or wait decision-making for landowners: determining payment amounts necessary for postponing harvest for a year.. Mississippi State University Extension Pub. 3593.\nWhite, A.E., D.A. Lutz, R.B. Howarth, and J.R. Soto. 2018. Small-scale forestry and carbon offset markets: An empirical study of Vermont Current Use forest landowner willingness to accept carbon credit programs. PLOS ONE 13(8):e0201967.\n–\nBy Matt Russell\nNOTE: This post was previously published by the Forest Resources Association as Technical Release 21-R-29. Special thanks to Tim O’Hara for his review of this post.\nEmail Matt with any questions or comments. Sign up for The Landing for monthly in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "post/2022-04-20-known-unknowns-unknown-unknowns/index.html#section",
    "href": "post/2022-04-20-known-unknowns-unknown-unknowns/index.html#section",
    "title": "Known unknowns, unknown unknowns",
    "section": "",
    "text": "Tell me if this has ever happened to you.\nYou’re taking a walk or you’re on a run in the dark, and you have a headlamp on. It’s raining or snowing - not too hard, but enough to make your time outside a little uncomfortable.\nWhen the light is shining on your headlamp, you can see every raindrop or snowflake in front of you. You feel every one of them as they hit your face.\nBut the interesting thing is if you turn your headlamp off, you barely seem to notice the raindrops or snowflakes hitting your face.\nWhat is revealing about this is that the conditions are the same, but you seem to suffer more when you have a bright headlight and can see the discomfort in front of you. But switch the headlamp off, and you can bound through the conditions as if nothing is in your way.\nThis revelation might relate to our sense of risk. If danger is ahead and we can see it, we’re likely to be more cautious and conservative in our decision making. If we do not sense danger ahead, we’re likely to go about our business and accept more risk.\nDonald Rumsfeld, former US Secretary of Defense, popularized the famous quote that there are “known unknowns” and “unknown unknowns”. Known unknowns are the everyday risks that you’re aware of, e.g., the risk of a car accident when driving through snow. Unknown unknowns are unexpected risks you might encounter that are nearly impossible to plan ahead for, e.g., a global pandemic.\nRun with a headlamp on in the rain and you’re faced with a known unknown. The rain can make conditions slippery, so you better have a great pair of running shoes with good tread. Turn the headlamp off and you might not know what you’re up against with the rain.\nIn the forestry community, we’re faced with several uncertainties ahead of us. Forests that have traditionally been managed for timber, recreation, or wildlife habitat are now being called on to store and sequester carbon. Forest analysts are being asked to forecast what these forests will look like in 50 or 100 years, based on little information about what environmental conditions will be in the future.\nWhile we know that forests have challenges ahead that we can plan for, we should also be cognizant that there are “unknowns unknowns” that we will face.\nI doubt in 2019 many forestry organizations had in their business plans a protocol for working through a global health pandemic. Economists could not predict the tremendous rise in lumber prices that we saw in 2020. The recognition of trees and forests as natural climate solutions has been building, but no one could have forecasted the tremendous rise in voluntary forest carbon markets in the last three years.\nForesters have always been challenged by unknown conditions ahead of them. Engaging in project management activities can help identify these uncertainties and plan for them. I encourage you to consider what known knowns and unknown unknowns are facing you and your organization.\n–\nBy Matt Russell\nEmail Matt with any questions or comments. Sign up for The Landing for monthly in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "post/2022-07-20-new-book-statistics-in-natural-resources-applications-with-r/index.html#section",
    "href": "post/2022-07-20-new-book-statistics-in-natural-resources-applications-with-r/index.html#section",
    "title": "New book: Statistics in Natural Resources: Applications with R",
    "section": "",
    "text": "At the start of the pandemic in 2020, I was worried. Not just for the serious global health impacts, but also for the workload I would soon be faced with.\nI taught an introductory statistics course for graduate students in the natural resources and agricultural disciplines, and was scrambling to find out how I would migrate my course material to an online, asynchronous format.\nAt the same time, I was searching for a different textbook to use for the course. I was using Introductory Statistics with R by Dalgaard, which I liked for its focus on the applied aspects of statistics. But the book wasn’t a great fit for graduate students and “use a different textbook” was a comment that showed up repeatedly on student evaluations of the course.\nSo, in an effort to tackle both of these issues, I decided it was worth my time and the community’s to invest in developing a new textbook that focuses on applied statistics with a natural resources focus. I’m happy to report that the Statistics in Natural Resources: Applications with R is now in production and available for pre-order. You can order from anywhere you buy books, including from the publisher or Amazon. It is currently in production and will ship August 19, 2022 in both hardcover and eBook formats.\nIf you order before Sept 30, 2022, I’m also happy to share a 20% discount code that you can use if you order through Routledge’s website:"
  },
  {
    "objectID": "post/2022-07-20-new-book-statistics-in-natural-resources-applications-with-r/index.html#section-1",
    "href": "post/2022-07-20-new-book-statistics-in-natural-resources-applications-with-r/index.html#section-1",
    "title": "New book: Statistics in Natural Resources: Applications with R",
    "section": "",
    "text": "The book is also available in its entirety online at stats4nr.com. It will always be free and available at that website. One of the primary reasons I chose to publish with Chapman and Hall/CRC Press was their commitment to make their textbooks accessible to a wider number of individuals.\nThis book focuses on the application of statistical analyses in the environmental, agricultural, and natural resources disciplines. There are thousands of textbooks on statistics, so I thought it be worth highlighting a few examples of what sets this book apart from others:\n\nFocuses on natural resources. There are many great statistics textbooks, but few that use examples solely from the natural resources disciplines. It’s been my experience that student comprehension is improved if they work with data they’re familiar with throughout their learning process.\n\nUses the tidyverse. The tidyverse package is a “megapackage” in R that includes several packages that import, reshape, and visualize data in a consistent manner, among other tasks. The tidyverse suite of R packages is continually updated and improved upon and provides a more seamless integration of learning how to code and perform data analysis.\nHas 130 exercises with solutions. Learning happens best when you use concepts and apply them to everyday problems. The book’s website contains solutions to over 130 exercises (PDF) in R that build on the concepts in the book.\n\nHas an accompanying R package. Loading data into R is often half the battle for people learning new statistical software. All of the data sets used in the book can be accessed by installing the stats4nr R package. This avoids the need to download external files when working through the book.\n\nThe first ten chapters form the basis of most semester-long introductory statistics classes at the undergraduate level. Chapters 11 through 15, which include topics such as mixed models, logistic regression, and count regression, could also be included in a statistics class that moves at an accelerated pace (e.g., for graduate students).\nFor some “behind the curtain” information, the book was written entirely in R using the bookdown package. I loved this approach because updating the online version of the book as it was being developed was a breeze. As I completed each chapter or significant sections, I uploaded new versions to the book’s website. I also gained feedback from students that used the book, in addition to LinkedIn followers, as this post from August 2021 shows:"
  },
  {
    "objectID": "post/2022-07-20-new-book-statistics-in-natural-resources-applications-with-r/index.html#section-2",
    "href": "post/2022-07-20-new-book-statistics-in-natural-resources-applications-with-r/index.html#section-2",
    "title": "New book: Statistics in Natural Resources: Applications with R",
    "section": "",
    "text": "I thank everyone who provided feedback on the book throughout the last year. I also have many people to thank for getting the book to press, namely the excellent folks at Chapman and Hall/CRC Press. Lara Spieker guided me through the process, Shashi Kumar helped to remedy several LaTEX issues, Michele Dimont provided a thorough copyedit, and technical reviewers improved the quality and scope of the book.\nIf you find the book valuable, I’d be interested to know. Don’t hesitate to send me an email with how you’re using it, or send any corrections or topics you’d like to see in the future. Enjoy!"
  },
  {
    "objectID": "post/2022-08-04-fvs-and-cbm-cfs3-comparing-two-forest-carbon-accounting-tools/index.html#the-fvs-model",
    "href": "post/2022-08-04-fvs-and-cbm-cfs3-comparing-two-forest-carbon-accounting-tools/index.html#the-fvs-model",
    "title": "FVS and CBM-CFS3: Comparing two forest simulation models",
    "section": "The FVS model",
    "text": "The FVS model\nThe Forest Vegetation Simulator (FVS) is a model developed by the USDA Forest Service in 1973. The FVS model is an individual tree model that uses lists of trees (e.g., species and tree diameter) to forecast forest growth through time. The model can simulate forest management activities, disturbances, and provide forest carbon and harvested wood products reports. A series of 22 different model variants provide regional differences in forest growth patterns.\nThere has been a tremendous amount of research and development on the FVS model through time. Their website contains excellent supporting information, video tutorials, and documentation. Although the model was originally developed in the US, here are a few applications of it in Canada:\n\nAn Acadian variant of FVS is available for Maine and the Canadian Maritime provinces, and\nThe FVS model has been calibrated for Ontario’s forests."
  },
  {
    "objectID": "post/2022-08-04-fvs-and-cbm-cfs3-comparing-two-forest-carbon-accounting-tools/index.html#the-cbm-cfs3-model",
    "href": "post/2022-08-04-fvs-and-cbm-cfs3-comparing-two-forest-carbon-accounting-tools/index.html#the-cbm-cfs3-model",
    "title": "FVS and CBM-CFS3: Comparing two forest simulation models",
    "section": "The CBM-CFS3 model",
    "text": "The CBM-CFS3 model\nThe Carbon Budget Model of the Canadian Forest Service 3 (CBM-CFS 3) is a model developed by the Canadian Forest Service in 2002. The model simulates carbon dynamics following the Good Practice Guidance established by the Intergovernmental Panel on Climate Change (IPCC) for reporting on carbon stocks and stock changes. The model works on an operational scale and is well suited for landscape-level predictions, but it can also operate at the stand level. Although the model was originally developed by the Canadian Forest Service, it has been applied in the US as well:\n\nAn analysis in seven US states used Forest Inventory and Analysis data in CBM-CFS3 to understand carbon impacts resulting from forest management.\nA recent analysis integrated CBM-CFS3 with a land use and land cover change model to understand spatially explicit estimates of carbon stocks and fluxes.\n\nAs for me, I’ve been a long time FVS user and am beginning to hear more about applications of CBM-CFS3 as a carbon accounting tool. The following table, a “cheat sheet” of sorts, compares the design, input, and outputs of both models:\n\n\n\n\n\n\nAttribute\nFVS\nCBM.CFS3\n\n\n\n\nDeveloper/maintainer\nUSDA Forest Service\nCanadian Forest Service\n\n\nYear developed\n1973\nOriginal model: 1989; CBM-CFS3 model: 2002\n\n\nModel type\nIndividual tree model; semi-distance independent\nStand and landscape-level model; distance independent\n\n\nHow are forested regions specified?\nIncludes 22 different model variants depending on region.\nDefault ecological parameters are provided, but can be modified by the user.\n\n\nTime step\nDefault cycle length is 10 years for most variants.\nAnnual\n\n\nCan forest management and disturbance be analyzed?\nYes\nYes\n\n\nIncludes climate change?\nYes, but only for Western US with Climate-FVS\nNo. But user can modify the default climate data (which only impacts decay), and use zero carbon impact disturbance events paired with transition rules to alter stand growth in unison with changes in climate.\n\n\nIncorporates uneven-aged stands?\nYes\nNo. But user can modify yield curves.\n\n\nHow is regeneration handled?\nA “full” regeneration establishment model is available for some variants in the western US. A “partial” establishment model is available for all other variants and simulates stump sprouting. User can specify information on planting and natural regeneration.\nFollowing a stand-replacing disturbance, regeneration will occur automatically, or can be delayed or accelerated using transition rules and/or switching of growth curves. By default, there is no regeneration assumed following non-stand-replacing disturbances. However the user can implement a transition rule to switch an impacted stand to a new growth curve(s) to account for multiple growth components (although the stand can only be represented by a single age or age class).\n\n\nIncludes harvested wood products report?\nYes\nNo. But annual carcon stocks harvested and transferred to a forest products pool are tracked, and can be viewed and exported for use in HWP carbon models.\n\n\nHow does it incorporate carbon?\nAccounts for carbon stocks and stock changes with the Fire and Fuels Extension.\nAccounts for carbon stocks and stock changes in tree biomass and dead organic matter pools.\n\n\nCarbon pools tracked\nAboveground biomass, belowground biomass (live and dead coarse roots), dead wood (standing and downed), forest floor (litter and duff), herbs and shrubs.\nAboveground biomass, belowground biomass (live coarse and live fine roots), dead wood (standing and downed {on ground and in mineral soil}), litter (includes fine woody debris), soil organic matter.\n\n\n\n\n\n\n\n\nHave you tried using both models, and if so, what are your thoughts on their differences? Drop me an email and I’d love to learn more.\nUpdated 6 Sept 2022. Special thanks to Stephen Kull with the Canadian Forest Service for highlighting some additional aspects of the CBM-CFS3 model and to Greg Paradis for pointing out differences between the two models.\n–\nBy Matt Russell. Sign up for my monthly newsletter for in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "post/2022-10-10-p-ing-in-the-woods-p-values-in-forest-science/index.html#section",
    "href": "post/2022-10-10-p-ing-in-the-woods-p-values-in-forest-science/index.html#section",
    "title": "P-ing in the woods: p-values in forest science",
    "section": "",
    "text": "The statistics community has had a lot of discussion in recent years about the role of p-values and statistical significance. This has led some to question if we’re relying too much on them as a way of interpreting results from a statistical analysis.\nThe “p” in p-value stands for probability, or the probability under a statistical model that a summary of the data would be equal to or more extreme than its observed value. There are few caveats with using p-values and interpreting them.\nFirst, assessing statistical significance is commonly done using the p-value as a threshold. As the American Statistical Association highlighted in 2016 “statistical significance is not equivalent to scientific, human, or economic significance”.\nSecond, a p-value can be small if the number of samples or measurement precision is high enough. Similarly, a p-value can be large if the sample size is small or measurements are imprecise.\nLastly, a p-value doesn’t measure the size of an effect or the importance of a result. So a p-value of 0.0001 does not indicate a more important result than a p-value of 0.045. This is where I’ve heard many a graduate student say “My p-value is so small, my results are VERY significant!”\nMuch of the data science community is moving away from using p-values. The scrutiny over the interpretation and use of p-values has also reached academic publishing. Some journals have outlawed the use of p-values in their journals. Some editors are providing specific instructions for authors on how to report (and not report) p-values in their journals."
  },
  {
    "objectID": "post/2022-10-10-p-ing-in-the-woods-p-values-in-forest-science/index.html#p-values-in-the-journal-of-forestry",
    "href": "post/2022-10-10-p-ing-in-the-woods-p-values-in-forest-science/index.html#p-values-in-the-journal-of-forestry",
    "title": "P-ing in the woods: p-values in forest science",
    "section": "P-values in the Journal of Forestry",
    "text": "P-values in the Journal of Forestry\nSo should foresters stop relying on using p-values? Probably not. (After all, I don’t want to be labeled a hypocrite because I use them a lot.) Out of curiosity, I was interested to see how often p-values are used in forestry research.\nI set up a small experiment: to look at how many p-values were reported in the Journal of Forestry in 2021 (find the data here).\nWhat counted as a p-value was more complicated than I first thought. Oftentimes authors will mention the true p-value from a statistical test either in the text or most commonly, in a table of results. However, other times, no doubt a result of the software being used, authors will report the asterisk or double asterisk that shows the significance at a specific level. For example, * and ** were written to denote significance at the 0.10 and 0.05 levels, respectively. Other times p-values will be embedded within figures, which makes finding them with a text string search problematic.\nDespite the challenges, out of the 36 research and review articles published in the Journal of Forestry in 2021, 61% did not present any p-values. Here is the distribution of p-values from the articles:\n\n\n\n\n\n\n\n\n\nOf those that did present p-values, the median number of p-values per article was 15. There was one paper that presented 191 different p-values (which made excellent use of the asterisk approach).\nI have no idea if the use of p-values in forestry has increased or decreased since they began to be used widely in the mid 1900’s. It strikes me that as we continue to teach hypothesis testing to students, p-values will continue to be used in the future.\nWhile there are many caveats, I really like the ASA’s shortcut to “remember ATOM” when using p-values. When thinking about them, remember to:\n\nAccept uncertainty. Data are noisy in the real world. By accepting the uncertainty inherent in the data, we move away from categorizing variables and attributes as either “significant” or “not significant”.\nBe thoughtful. Practicing thoughtful data analysis allows analysts to assess their workflow and applications of their work. Are you engaging in exploratory data analysis on a project? Or are your results going to inform key decisions and a thorough statistical analysis is needed?\nBe open. Being open about your analysis stresses the importance of recognizing that subjectivity is present in any statistical analysis. This is also true when considering the role of expert judgment in the data analysis project and the interpretation of a project’s findings. When communicating p-values, a practical tip is to present them as continuous values rather than dichotomized ones (e.g., report p = 0.04234 not p &lt; 0.05). The “asterisks” that are provided by statistical software don’t help with this one.\nBe modest. Being modest encourages you to think about how others might be able to reproduce your work. More broadly, being modest promotes the limitations of statistical knowledge in the larger discipline of scientific understanding.\n\nIt’s important to reflect on the role of hypothesis tests and p-values are and what they aren’t. Forest biometricians and analysts should have these in their toolbox, but also be aware of their proper use.\n–\nBy Matt Russell. Sign up for my monthly newsletter for in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "post/2022-11-23-new-updates-for-tools-that-query-forest-inventory-and-analysis-data/index.html#section",
    "href": "post/2022-11-23-new-updates-for-tools-that-query-forest-inventory-and-analysis-data/index.html#section",
    "title": "New updates for tools that query Forest Inventory and Analysis data",
    "section": "",
    "text": "The US national forest inventory has information contained from over 130,000 forest inventory plots across the country. Data collected and maintained by the Forest Inventory and Analysis (FIA) program are needed for answering core forestry questions, and having the right tools to help extract specific data that fits the analysis is essential.\nAfter attending the FIA Science Stakeholder Meeting last week, I was happy to learn about some important updates to two tools that are widely used by forest analysts: the FIA EVALIDator program and the FIA DataMart. This post describes new features that are now available in these applications."
  },
  {
    "objectID": "post/2022-11-23-new-updates-for-tools-that-query-forest-inventory-and-analysis-data/index.html#evalidator-version-2.0.3",
    "href": "post/2022-11-23-new-updates-for-tools-that-query-forest-inventory-and-analysis-data/index.html#evalidator-version-2.0.3",
    "title": "New updates for tools that query Forest Inventory and Analysis data",
    "section": "EVALIDator version 2.0.3",
    "text": "EVALIDator version 2.0.3\nEVALIDator is a web-based Application Programming Interface that generates population estimates for core forestry metrics. It provides the ability to query FIA information at the state level (e.g., total aboveground biomass in Maine) or a circular retrieval of information around a fixed geographic point (e.g., total aboveground biomass within a 50 mile radius of a proposed mill in Maine).\n\n\n\n\nHomepage for EVALIDator API."
  },
  {
    "objectID": "post/2022-11-23-new-updates-for-tools-that-query-forest-inventory-and-analysis-data/index.html#section-1",
    "href": "post/2022-11-23-new-updates-for-tools-that-query-forest-inventory-and-analysis-data/index.html#section-1",
    "title": "New updates for tools that query Forest Inventory and Analysis data",
    "section": "",
    "text": "The previous version of EVALIDator was built in Java, but it now runs in Python. It uses pandas in the data frame style to provide output for the variables that you specify.\nOne new feature that I especially like is that you can copy/paste the URL for an EVALIDator query you run. Then you can share the link in a document or share with a colleague. For example, see this EVALIDator output for a summary of aboveground carbon by forest type group and site productivity class in Maine. This is especially helpful for analysts that work in teams and want to share output with others that may not have the expertise or time available to navigate the EVAIDator API.\nAnother feature I really love is the addition of ‘normalized’ estimates as output in EVALIDator. This is akin to viewing the data in its “long format”. For those of us that work with tidy data (where each variable has its own column, each observation has its own row, and each value has its own cell), downloading the data in this format can save time down the line. Users can save output in a variety of formats, including CSV, JSON, or XML."
  },
  {
    "objectID": "post/2022-11-23-new-updates-for-tools-that-query-forest-inventory-and-analysis-data/index.html#fia-datamart-version-2.0.1",
    "href": "post/2022-11-23-new-updates-for-tools-that-query-forest-inventory-and-analysis-data/index.html#fia-datamart-version-2.0.1",
    "title": "New updates for tools that query Forest Inventory and Analysis data",
    "section": "FIA DataMart version 2.0.1",
    "text": "FIA DataMart version 2.0.1\nThe FIA DataMart is a tool that allows for the retrieval of state-level FIA data. This new version resides on an Esri platform and contains user-friendly ways to acquire data. An updated map on its landing page displays the most recent year that data are available for each state, a handy visualization especially if an analyst is interested in making comparisons across states.\n\n\n\n\nHomepage for FIA DataMart application."
  },
  {
    "objectID": "post/2022-11-23-new-updates-for-tools-that-query-forest-inventory-and-analysis-data/index.html#section-2",
    "href": "post/2022-11-23-new-updates-for-tools-that-query-forest-inventory-and-analysis-data/index.html#section-2",
    "title": "New updates for tools that query Forest Inventory and Analysis data",
    "section": "",
    "text": "Using the DataMart, users can select a state and navigate to a specific data table of interest. Data are available for download as comma-delimited files, SQLite3 State databases, or Microsoft® Excel workbooks.\nA new feature I like is the data update history, a short description indicating the dates when new data were added or modifications were made to existing data. Having this feature can help to sort out issues one may encounter if downloading the data at different times.\n–\nWhat are your thoughts on these changes to EVALIDator and DataMart? I’m always curious to know, so let me know via email.\nBy Matt Russell. Sign up for my monthly newsletter for in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "post/2022-12-21-forest-carbon-cheat-sheet-updated-for-2022/index.html",
    "href": "post/2022-12-21-forest-carbon-cheat-sheet-updated-for-2022/index.html",
    "title": "Forest carbon cheat sheet: updated for 2022",
    "section": "",
    "text": "The Forest Carbon Cheat Sheet is a quick reference guide for converting forest carbon measurements and understanding typical carbon values at the tree, stand, and landscape levels.\nThe Cheat Sheet has been updated with the most recent values published in the US Environmental Protection Agency Inventory of U.S. Greenhouse Gas Emissions and Sinks: 1990-2020.\nDOWNLOAD (PDF)\nDOWNLOAD (PNG)\n\n\n\n\nThe Forest Carbon Cheat Sheet.\n\n\n\n–\nBy Matt Russell. Email Matt with any questions or comments. Sign up for my monthly newsletter for in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "post/2023-01-05-forestland-on-the-blockchain-a-new-prospect-for-private-forest-owners/index.html#section",
    "href": "post/2023-01-05-forestland-on-the-blockchain-a-new-prospect-for-private-forest-owners/index.html#section",
    "title": "Forestland on the blockchain: a new prospect for private forest owners",
    "section": "",
    "text": "In 2021 a 40-acre tract of land in Wyoming was sold and purchased. A land sale like this will not typically make news headlines. But what turned heads about this sale was that the land was purchased by 5,000 different individuals from several countries.\nIt was possible because of the state of Wyoming passing Bill 38 in 2021, legislation that provided a structure for decentralized autonomous organizations, or DAOs, to have legal rights in the state. DAOs are member-owned communities that operate with decentralized leadership and employ blockchain technology to record financial transactions.\nWyoming has been a state that has led in innovation in business. In 1977 it was the first state to authorize and recognize limited liability companies (LLCs) operating in the state. Many private forest landowners operate their properties under LLCs because its structure facilitates one or many members and the transfer of ownership between generations is seamless.\nWyoming considers the business entity type of DAOs to be similar to LLCs. From there, the similarities between LLCs and DAOs ends. Whereas LLCs are organized by people termed members or managers, DAOs can be managed by a combination of people and computer algorithms.\nThe 40-acre tract that sits an hour north of Cody, WY is not much of anything. It’s largely barren and devoid of trees, but members of CityDAO, the online community that purchased the land, are seeking to build a “decentralized city of the future.”\nCityDAO raised $6 million for the project in 2021 through the sale of 10,000 non-fungible tokens, or NFTs. It currently has more than 5,200 citizens from over 100 locations that engage in discussions and govern CityDAO through their voting rights. Buying a piece of land in CityDAO is done in a transparent manner through Ethereum, blockchain software that operates with cryptocurrency. Discussions among its citizens happen through Discord, an online social messaging platform. Scott Fitsimones, the creator of CityDAO, refers to it as “group chat with a bank account.”\nThe decentralization of units and organizations has many benefits. Blockchain technologies like those operated by CityDAO employ smart contracts that can automate the execution of agreements. This can speed up decision-making and reduce bureaucracy, including eliminating long permitting times. Voting can take place among members to support or reject projects that are brought up in the community. In the broadest sense, DAOs are collectively owned and operated and work toward a shared mission.\nDAOs are still very early in their development and are not without their criticism. The use of cryptocurrency in DAOs brings negative stereotypes, as evident in what was seen in late 2022 with the fall of FTX. DAOs have also been subject to hacking, providing more uneasiness from early adopters of the technology.\nJust as members of DAOs operate toward a shared mission, so do private family forest owners. Private family forest owners manage nearly 300 million acres of forest in the US. They own them for many reasons, many of which cite “to enjoy beauty or scenery” or “to protect or improve wildlife habitat” as their primary motivations. They own 35% of all forestland in the US but provide up to 60% of all harvested timber in the country, providing an essential resource to the forest products industry.\nLandowners that collectively own a tract of land with others may have conflicts or ongoing issues with other individuals that have a legal claim to the property. Should we harvest timber this year or not? Should we enroll our property in a forest carbon program to become a part of the climate solution, even if it places barriers on our decisions like delaying a timber harvest?\nThe data point to the rising number of forest landowners that collectively own land. According to data from the National Woodland Owner Survey, 72% of family forest landowner properties have multiple owners. This can lead to a large number of acres in the US with potential differences in opinion about how the land should be managed.\nThe concept of shared ownership also has applications in entities such as land trusts and other conservation organizations that own and manage forestland. In the interest of shared stewardship, structures like DAOs can democratize stewardship decisions for tracts of land that have conservation or climate adaptation objectives.\nForests grow slowly. Trees take decades to mature. And land management decisions such as the decision to harvest should be made in careful consultation with professional foresters. Landowners will want to know the full environmental impacts of a decision to do something on their land. So it’s not likely that landowners will benefit from some of the offerings of DAOs like smart contracts.\nStill, owners of forest land should be aware of the benefits of collective decision-making and governance of DAOs. The democratization of DAOs can promote a shared stewardship ethic that many forest landowners have for the natural world, particularly in the face of understanding forests as natural climate solutions.\n–\nBy Matt Russell. Sign up for my monthly newsletter for in-depth analysis on data and analytics in the forest products industry."
  },
  {
    "objectID": "post/2023-02-15-imputing-missing-tree-heights-from-a-forest-inventory/index.html#section",
    "href": "post/2023-02-15-imputing-missing-tree-heights-from-a-forest-inventory/index.html#section",
    "title": "Imputing missing tree heights from a forest inventory",
    "section": "",
    "text": "Total height is one of the most useful measurements of a tree, but it’s often not measured. This is because measuring heights takes considerable time and effort. While it may be easy for a forester to measure the diameter of a tree, to obtain a proper measurement of a tree you generally need to walk away from it. This can take a lot of time if there are dozens of trees on a forest inventory plot.\nTo expedite the time spent in the field, most foresters will subsample tree heights. For example, foresters can measure the height of every fourth tree they encounter, or measure heights on all trees in every fourth inventory plot.\nForest inventory data are often used in a variety of applications after the data are collected. For example, data are used to determine stand volume, biomass, and carbon. However, most of the equations that determine those attributes require a value for the tree’s height. So what do you do when you didn’t collect heights on all trees?\nThis post will discuss the idea of imputation, or how to make the best prediction by using all data available. An example is shown in R using a few different approaches are used to impute tree heights based on traditional forest inventory measurements."
  },
  {
    "objectID": "post/2023-02-15-imputing-missing-tree-heights-from-a-forest-inventory/index.html#penobscot-experimental-forest-data",
    "href": "post/2023-02-15-imputing-missing-tree-heights-from-a-forest-inventory/index.html#penobscot-experimental-forest-data",
    "title": "Imputing missing tree heights from a forest inventory",
    "section": "Penobscot Experimental Forest data",
    "text": "Penobscot Experimental Forest data\nThe data used here is from the Penobscot Experimental Forest (PEF), a long-term silvicultural experiment based in central Maine. The species are typical of the mixed-species Acadian Forest, an ecotone between the boreal forests to the north and the broadleaf forests to the south. Permanent sample plots are located in several management units (MU) across the forest.\nThere are several remeasurements of permanent sample plots on trees in the PEF database. I use only the most recent measurement from each plot for this analysis. On the PEF, permanent sample plots consist of nested 1/5-, 1/20-, and 1/50-acre circular plots for measuring trees &gt;= 4.5 inches, 2.5 to 4.4 inches, and 0.5 to 2.4 inches in diameter at breast height, respectively. Tree heights are measured on one-third of the plots. (NOTE: I skip some of the data cleaning on this data set in this post, but you can find the raw data and the full code set on GitHub.)\nIn total there are 14,489 total trees in the database, but only 3,382 have height measurements. Here are 10 random rows of data, and note the missing heights for many of them:\n\nlibrary(tidyverse)\n\n\n\n\n\n\n\nMU\nPlot\nYear\nTreeNum\nFVSspID\ndbh\nHT\n\n\n\n\n23A\n84\n2001\n12100\nPB\n4.9\nNA\n\n\n16\n55\n2012\n40700\nEH\n16.1\nNA\n\n\n23A\n75\n2001\n42000\nWS\n5.2\nNA\n\n\n29B\n24\n2011\n4000\nRM\n4.8\nNA\n\n\n29A\n41\n2009\n54200\nRS\n6.6\nNA\n\n\n20\n31\n2008\n85100\nWC\n13.1\n51.2\n\n\n6\n13\n2010\n10700\nEH\n5.7\nNA\n\n\n22\n12\n2014\n1500\nBF\n4.6\nNA\n\n\n16\n44\n2012\n12800\nEH\n10.6\nNA\n\n\n23A\n62\n2001\n72900\nRM\n6.9\nNA\n\n\n\n\n\n\n\n\nWe can plot the tree diameter-height relationship, and note the many species are found in the database. Data are stored in the tree data set, and the dbh and HT variables are of interest. Note the warning message that R provides–this is a result of the 11,000+ trees where height was not measured:\n\nlibrary(tidyverse)\n\n\nggplot(tree, aes(x = dbh, y = HT, col = FVSspID)) +\n  geom_point() +\n  labs (x = \"Tree diameter (inches)\",\n        y = \"Tree height (feet)\")\n\nWarning: Removed 11108 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nOne way to determine the heights of missing trees is to apply a regional equation that predicts total height. In the northeast region of the United States, the Forest Vegetation Simulator-Northeast variant is a popular model for determining forest growth and yield. The FVS variant uses two model forms to determine height, depending on species. The first is the Curtis-Arney model form:\n\\[HT=4.5+a_0\\text{ exp}(a_1 \\times DBH^{a_2})\\] and the second is the Wykoff model form:\n\\[HT=4.5+\\text{ exp}(b_1 + b_2 / DBH+1)\\] To apply these equations to the tree data set, the FVSNE.SPP() function specifies which species uses which height equation by specifying the HTDBH_EQ variable:\n\nFVSNE.SPP &lt;- function(SPP){\n     if(SPP == 'BF'){HTDBH_EQ = 'C'; Dbw = 0.1}\nelse if(SPP == 'TA'){HTDBH_EQ = 'C'; Dbw = 0.1}\nelse if(SPP == 'WS'){HTDBH_EQ = 'C'; Dbw = 0.2}\nelse if(SPP == 'RS'){HTDBH_EQ = 'C'; Dbw = 0.2}\nelse if(SPP == 'RN'){HTDBH_EQ = 'C'; Dbw = 0.1}\nelse if(SPP == 'WP'){HTDBH_EQ = 'C'; Dbw = 0.4}\nelse if(SPP == 'WC'){HTDBH_EQ = 'W'; Dbw = 0.1}\nelse if(SPP == 'EH'){HTDBH_EQ = 'C'; Dbw = 0.1}\nelse if(SPP == 'RM'){HTDBH_EQ = 'W'; Dbw = 0.2}\nelse if(SPP == 'SM'){HTDBH_EQ = 'W'; Dbw = 0.2}\nelse if(SPP == 'YB'){HTDBH_EQ = 'W'; Dbw = 0.1}\nelse if(SPP == 'PB'){HTDBH_EQ = 'W'; Dbw = 0.1}\nelse if(SPP == 'GB'){HTDBH_EQ = 'W'; Dbw = 0.1}\nelse if(SPP == 'AB'){HTDBH_EQ = 'W'; Dbw = 0.1}\nelse if(SPP == 'WA'){HTDBH_EQ = 'W'; Dbw = 0.2}\nelse if(SPP == 'BA'){HTDBH_EQ = 'W'; Dbw = 0.2}\nelse if(SPP == 'QA'){HTDBH_EQ = 'W'; Dbw = 0.3}\nelse if(SPP == 'BP'){HTDBH_EQ = 'W'; Dbw = 0.2}\nelse if(SPP == 'BT'){HTDBH_EQ = 'W'; Dbw = 0.2}\nelse if(SPP == 'OK'){HTDBH_EQ = 'W'; Dbw = 0.2}\nelse if(SPP == 'EL'){HTDBH_EQ = 'W'; Dbw = 0.1}\nelse if(SPP == 'HH'){HTDBH_EQ = 'W'; Dbw = 0.2}\nelse{HTDBH_EQ = 'W'; Dbw = 0.1}\n  return(c(HTDBH_EQ = HTDBH_EQ, Dbw = Dbw))}\n\nThe FVSNE.HTDBH() function adds the species-specific coefficients for each species:\n\n#FVS HT-DBH (Table 4.1.1 in FVS-NE guide)\nFVSNE.HTDBH &lt;- function(SPP,HTDBH_EQ,DBH,Dbw){\n     if(SPP=='BF'){ca_a1=2163.9468; ca_a2=6.2688; ca_a3=-0.2161; w_a1=4.5084; w_a2=-6.0116}     \nelse if(SPP=='TA'){ca_a1=2163.9468; ca_a2=6.2688; ca_a3=-0.2161; w_a1=4.5084; w_a2=-6.0116}     \nelse if(SPP=='WS'){ca_a1=2163.9468; ca_a2=6.2688; ca_a3=-0.2161; w_a1=4.5084; w_a2=-6.0116}     \nelse if(SPP=='RS'){ca_a1=2163.9468; ca_a2=6.2688; ca_a3=-0.2161; w_a1=4.5084; w_a2=-6.0116}     \nelse if(SPP=='RN'){ca_a1=266.4562; ca_a2=3.9931; ca_a3=-0.386; w_a1=4.5084; w_a2=-6.0116}     \nelse if(SPP=='WP'){ca_a1=2108.8442; ca_a2=5.6595; ca_a3=-0.1856; w_a1=4.609; w_a2=-6.1896}     \nelse if(SPP=='WC'){ca_a1=2163.9468; ca_a2=6.2688; ca_a3=-0.2161; w_a1=4.5084; w_a2=-6.0116}     \nelse if(SPP=='EH'){ca_a1=266.4562; ca_a2=3.9931; ca_a3=-0.386; w_a1=4.5084; w_a2=-6.0116}     \nelse if(SPP=='RM'){ca_a1=268.5564; ca_a2=3.1143; ca_a3=-0.2941; w_a1=4.3379; w_a2=-3.8214}     \nelse if(SPP=='SM'){ca_a1=209.8555; ca_a2=2.9528; ca_a3=-0.3679; w_a1=4.4834; w_a2=-4.5431}     \nelse if(SPP=='YB'){ca_a1=170.5253; ca_a2=2.6883; ca_a3=-0.4008; w_a1=4.4388; w_a2=-4.0872}    \nelse if(SPP=='PB'){ca_a1=170.5253; ca_a2=2.6883; ca_a3=-0.4008; w_a1=4.4388; w_a2=-4.0872}     \nelse if(SPP=='GB'){ca_a1=170.5253; ca_a2=2.6883; ca_a3=-0.4008; w_a1=4.4388; w_a2=-4.0872}     \nelse if(SPP=='AB'){ca_a1=526.1393; ca_a2=3.8923; ca_a3=-0.2259; w_a1=4.4772; w_a2=-4.7206}     \nelse if(SPP=='WA'){ca_a1=91.3528; ca_a2=6.9961; ca_a3=-1.2294; w_a1=4.5959; w_a2=-6.4497}     \nelse if(SPP=='BA'){ca_a1=178.9308; ca_a2=4.9286; ca_a3=-0.6378; w_a1=4.6155; w_a2=-6.2945}     \nelse if(SPP=='QA'){ca_a1=337.6685; ca_a2=3.6273; ca_a3=-0.3208; w_a1=4.5128; w_a2= -4.9918}     \nelse if(SPP=='BP'){ca_a1=91.3528; ca_a2=6.9961; ca_a3=-1.2294; w_a1=4.5959; w_a2= -6.4497}     \nelse if(SPP=='BT'){ca_a1=91.3528; ca_a2=6.9961; ca_a3=-1.2294; w_a1=4.5959; w_a2=-6.4497}     \nelse if(SPP=='OK'){ca_a1=196.0565; ca_a2=3.0067; ca_a3=-0.385; w_a1=4.5225; w_a2=-4.9401}     \nelse if(SPP=='EL'){ca_a1=1005.8067; ca_a2=4.6474; ca_a3=-0.2034; w_a1=4.3744; w_a2=-4.5257}     \nelse if(SPP=='HH'){ca_a1=109.7324; ca_a2=2.2503; ca_a3=-0.413; w_a1=4.0322; w_a2=-3.0833}     \nelse{ca_a1=68.5564; ca_a2=3.1143; ca_a3=-0.2941; w_a1=4.3379; w_a2=-3.8214}\n    HT = ifelse(HTDBH_EQ ==\"C\" & DBH &lt; 3.0,(4.5+ca_a1*exp(-1*ca_a2*3**ca_a3)-4.51*(DBH-Dbw)/(3-Dbw))+4.51,\n          ifelse(HTDBH_EQ ==\"C\" & DBH &gt;= 3.0,4.5+ca_a1*exp(-1*ca_a2*DBH**ca_a3),\n            4.5+exp(w_a1+w_a2/(DBH+1))))\n    return(HT=HT)}\n\nNow we predict the height of each tree in the PEF data using the FVS-NE equation:\n\naa &lt;- sapply(tree$FVSspID, FVSNE.SPP)\n\ntree &lt;- tree %&gt;% \n  mutate(HTDBH_EQ = t(aa)[,1],\n         Dbw = as.numeric(t(aa)[,2]),\n         HT_PRED_FVS = mapply(FVSNE.HTDBH, SPP = FVSspID, \n                     HTDBH_EQ = HTDBH_EQ, \n                     DBH = dbh, \n                     Dbw = Dbw))\n\nWe can see the for the heights measured, the FVS-predicted heights show a bimodal distribution. This is likely a result of the two different equations used:\n\nggplot(tree_ht, aes(x = height_feet, fill = Height_model)) +\n  geom_density(alpha = 0.1) +\n  labs (x = \"Height (feet)\") +\n  theme(panel.background = element_rect(fill = \"NA\"),\n        axis.line = element_line(color=\"black\"),\n        legend.title = element_blank(),\n        legend.position = c(0.8, 0.8))\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\nWarning: Removed 11108 rows containing non-finite outside the scale range\n(`stat_density()`)."
  },
  {
    "objectID": "post/2023-02-15-imputing-missing-tree-heights-from-a-forest-inventory/index.html#modeling-tree-height-with-a-mixed-model",
    "href": "post/2023-02-15-imputing-missing-tree-heights-from-a-forest-inventory/index.html#modeling-tree-height-with-a-mixed-model",
    "title": "Imputing missing tree heights from a forest inventory",
    "section": "Modeling tree height with a mixed model",
    "text": "Modeling tree height with a mixed model\nWe could use the FVS equations as predictions of tree height, but the drawback is that those equations are not localized to the PEF. The benefit is we have numerous predictions of tree height on the same forest that we want to predict on. And we should take advantage of that. Hence, we can use mixed models to make localized predictions.\nFor most models of tree height, nonlinear models work well. However, there is a general linear trend between height and diameter, so for the purpose of this tutorial, we’ll choose a linear model:\n\np.ht &lt;- ggplot(tree, aes(x = dbh, y = HT)) +\n  geom_point() +\n  stat_smooth() +\n  labs (x = \"Tree diameter (inches)\",\n        y = \"Tree height (feet)\")\np.ht\n\n\n\n\n\n\n\n\nA popular R package for performing linear mixed models is lme4. We will use it to fit a linear mixed model that specifies fixed and random effects:\n\nlibrary(lme4)\n\nThe lmer() function is the mixed model equivalent of lm() and parameter estimates are fit using maximum likelihood. We know that there are several plots nested within several management units from multiple species. So, we can specify these as random effects on the intercept, writing + (1 | FVSspID/MU/Plot) after specifying the independent variable DBH:\n\nht.lme &lt;- lmer(HT ~ dbh + (1 | FVSspID/MU/Plot),\n             data = tree)\nsummary(ht.lme)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: HT ~ dbh + (1 | FVSspID/MU/Plot)\n   Data: tree\n\nREML criterion at convergence: 22280.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.3634 -0.5141  0.0374  0.5428  4.5896 \n\nRandom effects:\n Groups            Name        Variance Std.Dev.\n Plot:(MU:FVSspID) (Intercept) 10.94    3.308   \n MU:FVSspID        (Intercept) 32.66    5.715   \n FVSspID           (Intercept) 52.84    7.269   \n Residual                      34.65    5.886   \nNumber of obs: 3381, groups:  \nPlot:(MU:FVSspID), 437; MU:FVSspID, 148; FVSspID, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 23.94183    2.00369   11.95\ndbh          2.99752    0.04432   67.64\n\nCorrelation of Fixed Effects:\n    (Intr)\ndbh -0.171\n\n\nWe can see from the lmer() output that the values for the fixed effects are 23.94183 and 2.99752, respectively. The Random effects section contains details on the variance of the random effects and their residuals. We could use the ranef() function to extract the values of the random effect terms. (But note there will be many values because there are many Species-MU-Plot combinations.)"
  },
  {
    "objectID": "post/2023-02-15-imputing-missing-tree-heights-from-a-forest-inventory/index.html#imputing-tree-height",
    "href": "post/2023-02-15-imputing-missing-tree-heights-from-a-forest-inventory/index.html#imputing-tree-height",
    "title": "Imputing missing tree heights from a forest inventory",
    "section": "Imputing tree height",
    "text": "Imputing tree height\nOnce we have a model, we can use R’s predict() function to apply it to our tree data set. We use the re.form statement to specify that we want to make predictions using fixed effects only, or predictions using subject-specific random effects (e.g., the best linear unbiased predictions).\nIf re.form = NULL, predictions are made including all random effects. This would be fitting if you subsampled tree heights measured on an individual plot. Adding the allow.new.levels = TRUE statement will make predictions using fixed effects only. This is ideal for the PEF data because there are many plots where no trees were measured for height:\n\ntree &lt;- tree %&gt;% \n   mutate(HT_PRED_MIXED = predict(ht.lme, tree, re.form = NULL, allow.new.levels = TRUE))\n\nWhen compared to the FVS model, we can see that the mixed model shows smaller residuals and less bias across the range in heights:\n\np.ht.fvs &lt;- ggplot(tree, aes(x = HT_PRED_FVS, y = (HT-HT_PRED_FVS))) +\n  geom_point() +\n  stat_smooth(se = F) +\n  scale_y_continuous(limits = c(-50, 30)) +\n  labs(x = 'Predicted FVS-NE height (ft)',\n       y = 'Bias (ft)',\n       title = \"FVS-Northeast\")\n\np.ht.mixed &lt;- ggplot(tree, aes(x = HT_PRED_MIXED, y = (HT-HT_PRED_MIXED))) +\n  geom_point() +\n  stat_smooth(se = F) +\n  scale_y_continuous(limits = c(-50, 30)) +\n  labs(x = 'Predicted random-effects height (ft)',\n       y = 'Bias (ft)',\n       title = \"Mixed model (fixed + random effects)\")\n\n\nlibrary(patchwork)\np.ht.fvs + p.ht.mixed"
  },
  {
    "objectID": "post/2023-02-15-imputing-missing-tree-heights-from-a-forest-inventory/index.html#conclusion",
    "href": "post/2023-02-15-imputing-missing-tree-heights-from-a-forest-inventory/index.html#conclusion",
    "title": "Imputing missing tree heights from a forest inventory",
    "section": "Conclusion",
    "text": "Conclusion\nThere are many ways to impute missing values, and mixed models are very fitting for forest inventory data because of their hierarchical structure. If variables of interest are subsampled, mixed models can account for random variation at different levels. R’s predict() function works excellent for making mixed model predictions at these different levels.\n–\nBy Matt Russell. PS: I’ve suppressed some of the code that cleans the Penobscot data set. You can find the raw data and the full code set on GitHub."
  },
  {
    "objectID": "post/2023-04-12-bootstrapped-resampling-estimates-to-predict-tree-height/index.html#section",
    "href": "post/2023-04-12-bootstrapped-resampling-estimates-to-predict-tree-height/index.html#section",
    "title": "Bootstrapped resampling to model tree biomass",
    "section": "",
    "text": "In one of the first presentations I gave as a graduate student, I discussed a set of regression equations that fit a nonlinear model predicting a forest growth index for several species. As all graduate students do, I spent considerable time preparing my slides and practicing my talk. The presentation went well.\nI used a data splitting approach in my analysis that I presented on, a common technique that trains a model on a large portion of the data (usually around 70%) then tests it on a smaller portion of data not used in model fitting (usually around 30%). After my presentation, a faculty member came up to me and asked, “You ever considered bootstrapping?”\nUp to then, I think I learned about bootstrapping in half a lecture in one of my statistics courses. In my defense, there weren’t great tutorials on how to do bootstrapping in my own field of applied forest science, and statistical packages in software like R weren’t as common as they are today. That day, I learned that bootstrapping regression models could be a viable alternative to traditional regression approaches.\nIn a nutshell, bootstrapping is more computationally intensive but doesn’t rely on distribution assumptions (i.e., the assumption of errors that are normally distributed). It works well with data that are “messy” and in situations where only a small number of samples are available.\nThe general approach to bootstrapping a regression model is to (1) iteratively sample a subset of the data with replacement, (2) fit the regression model to each subset, and (3) output the regression coefficients from each subset so that you can visualize and interpret results.\nIn this tutorial, I use bootstrapping with with tidymodels package in R and apply it to estimating tree biomass for several species from the southern United States."
  },
  {
    "objectID": "post/2023-04-12-bootstrapped-resampling-estimates-to-predict-tree-height/index.html#tree-biomass-data",
    "href": "post/2023-04-12-bootstrapped-resampling-estimates-to-predict-tree-height/index.html#tree-biomass-data",
    "title": "Bootstrapped resampling to model tree biomass",
    "section": "Tree biomass data",
    "text": "Tree biomass data\nTo begin, we’ll use many functions from the tidyverse package in R to work with the data:\n\nlibrary(tidyverse)\n\nThe objective of this post is to fit a subsample of models that determine the aboveground biomass of trees using tree diameter as a predictor variable. I’ve gathered data from LegacyTreeData, an online repository of individual tree measurements such as volume, weight, and wood density.\nI queried the database to provide all tree measurements for pine species the US State of Georgia. (You can find the raw data here, and I’ve previously written about these data.)\nThere are 566 observations from six species that contain a value for the tree’s diameter at breast height(ST_OB_D_BH; cm) and its aboveground dry weight (AG_DW; kg). In this data set, most trees are small in diameter and do not weigh a lot:\n\nggplot(tree, aes(ST_OB_D_BH, AG_DW, col = Species)) +\n  geom_point() +\n  labs(x = \"Diameter at breast height (cm)\", \n       y = \"Aboveground dry weight (kg)\") +\n  theme(panel.background = element_rect(fill = \"NA\"),\n        axis.line = element_line(color = \"black\"))\n\n\n\n\n\n\n\n\nHere is a summary of the data we’ll use in the modeling exercise:\n\n\n\n\nSummary statistics for diameter at breast height (DBH; cm) and aboveground dry weight (weight; kg) for six pine species from the southeastern US.\n\n\nSpecies\nNum trees\nMean DBH\nMax DBH\nMin DBH\nMean weight\nMax weight\nMin weight\n\n\n\n\nLoblolly pine\n186\n7.9\n21.6\n1.8\n16.8\n191.8\n0.8\n\n\nShortleaf pine\n100\n7.6\n12.4\n2.8\n11.9\n35.6\n0.8\n\n\nLongleaf pine\n80\n7.6\n12.4\n3.0\n16.5\n54.0\n0.9\n\n\nSlash pine\n80\n7.6\n12.4\n3.0\n13.9\n48.0\n0.9\n\n\nVirginia pine\n80\n7.7\n12.4\n2.5\n17.1\n58.1\n1.0\n\n\nEastern white pine\n40\n7.5\n12.4\n2.5\n12.5\n30.7\n1.0"
  },
  {
    "objectID": "post/2023-04-12-bootstrapped-resampling-estimates-to-predict-tree-height/index.html#nonlinear-regression-model-of-tree-biomass",
    "href": "post/2023-04-12-bootstrapped-resampling-estimates-to-predict-tree-height/index.html#nonlinear-regression-model-of-tree-biomass",
    "title": "Bootstrapped resampling to model tree biomass",
    "section": "Nonlinear regression model of tree biomass",
    "text": "Nonlinear regression model of tree biomass\nFrom the previous graph and what we know about tree size-mass relationships, nonlinear equation forms work best. In this case, we’ll refit the classic Jenkins et al. tree biomass models using our the pine tree data. The model form is an exponential model which we’ll save in R as the bio_pred object.\nWith most nonlinear applications in R, we’ll also need to specify starting values for each coefficient. Here we’ll use the values for the pine species group from the Jenkins et al. publication and store them in the start_vals object:\n\nbio_pred &lt;- as.formula(AG_DW ~ exp(b0 + b1*log(ST_OB_D_BH)))\n\nstart_vals &lt;- list(b0 = -2.5356, b1 = 2.4349)\n\nA classic use of these data would be to use the nls() function in R. Here’s how we can specify that:\n\nm.bio &lt;- nls(bio_pred,\n             start = start_vals,\n             data = tree)\nsummary(m.bio)\n\n\nFormula: AG_DW ~ exp(b0 + b1 * log(ST_OB_D_BH))\n\nParameters:\n   Estimate Std. Error t value Pr(&gt;|t|)    \nb0 -3.31397    0.08806  -37.63   &lt;2e-16 ***\nb1  2.75972    0.03339   82.65   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.657 on 564 degrees of freedom\n\nNumber of iterations to convergence: 4 \nAchieved convergence tolerance: 5.075e-06\n\n\nWe can see that each coefficient has a small p-value. If we compare the size and magnitude of the coefficients to the ones presented in Jenkins et al., we see that they are similar, giving us some confidence in our analysis moving forward."
  },
  {
    "objectID": "post/2023-04-12-bootstrapped-resampling-estimates-to-predict-tree-height/index.html#bootstrapping-regressions-with-tidymodels",
    "href": "post/2023-04-12-bootstrapped-resampling-estimates-to-predict-tree-height/index.html#bootstrapping-regressions-with-tidymodels",
    "title": "Bootstrapped resampling to model tree biomass",
    "section": "Bootstrapping regressions with tidymodels",
    "text": "Bootstrapping regressions with tidymodels\nThe tidymodels package in R has a number of helpful tools for performing regressions and handling their output. The package draws from many useful functions from other packages like rsample and broom:\n\nlibrary(tidymodels)\n\nOne helpful function is tidy(), which compiles regression output into a “tibble”, or a data set that can be used in subsequent analyses. I love this function because you can use the tibble that it creates by merging it to a new data set or visualizing the output:\n\ntidy(m.bio)\n\n# A tibble: 2 × 5\n  term  estimate std.error statistic   p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 b0       -3.31    0.0881     -37.6 5.95e-156\n2 b1        2.76    0.0334      82.7 2.28e-317\n\n\nBefore we bootstrap, we’ll create a generic function to perform the subset of regressions:\n\nfit_fx &lt;- function(split){\n  nls(bio_pred, data = analysis(split), start = start_vals)\n  }\n\nThe bootstraps() function from tidymodels performs the bootstrap resampling. We’ll ask it to resample from the tree data set a total of 500 times. We set apparent = TRUE to take one additional sample in the analysis, a requirement for some estimates that are produced after the sampling.\nWe use the map() function to create a data frame of modeling results, including the coefficients. This is stored in bio_boot:\n\nset.seed(123)\n\nbio_boot &lt;-\n  bootstraps(tree, times = 500, apparent = TRUE) %&gt;%\n  mutate(models = map(splits, ~ fit_fx(.x)), \n      coef_info = map(models, tidy))\n\nbio_boot\n\n# Bootstrap sampling with apparent sample \n# A tibble: 501 × 4\n   splits            id           models coef_info       \n   &lt;list&gt;            &lt;chr&gt;        &lt;list&gt; &lt;list&gt;          \n 1 &lt;split [566/202]&gt; Bootstrap001 &lt;nls&gt;  &lt;tibble [2 × 5]&gt;\n 2 &lt;split [566/208]&gt; Bootstrap002 &lt;nls&gt;  &lt;tibble [2 × 5]&gt;\n 3 &lt;split [566/218]&gt; Bootstrap003 &lt;nls&gt;  &lt;tibble [2 × 5]&gt;\n 4 &lt;split [566/200]&gt; Bootstrap004 &lt;nls&gt;  &lt;tibble [2 × 5]&gt;\n 5 &lt;split [566/206]&gt; Bootstrap005 &lt;nls&gt;  &lt;tibble [2 × 5]&gt;\n 6 &lt;split [566/206]&gt; Bootstrap006 &lt;nls&gt;  &lt;tibble [2 × 5]&gt;\n 7 &lt;split [566/207]&gt; Bootstrap007 &lt;nls&gt;  &lt;tibble [2 × 5]&gt;\n 8 &lt;split [566/211]&gt; Bootstrap008 &lt;nls&gt;  &lt;tibble [2 × 5]&gt;\n 9 &lt;split [566/201]&gt; Bootstrap009 &lt;nls&gt;  &lt;tibble [2 × 5]&gt;\n10 &lt;split [566/220]&gt; Bootstrap010 &lt;nls&gt;  &lt;tibble [2 × 5]&gt;\n# ℹ 491 more rows\n\n\nIf we wanted to look at a specific sample (say samples 1 and 167), we could extract the output directly from bio_boot. Note the differences in the b0 and b1 coefficients between the two samples:\n\nbio_boot$models[[1]]\n\nNonlinear regression model\n  model: AG_DW ~ exp(b0 + b1 * log(ST_OB_D_BH))\n   data: analysis(split)\n    b0     b1 \n-3.505  2.848 \n residual sum-of-squares: 15302\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 2.409e-06\n\nbio_boot$models[[167]]\n\nNonlinear regression model\n  model: AG_DW ~ exp(b0 + b1 * log(ST_OB_D_BH))\n   data: analysis(split)\n    b0     b1 \n-3.139  2.682 \n residual sum-of-squares: 14350\n\nNumber of iterations to convergence: 3 \nAchieved convergence tolerance: 9.827e-06\n\n\nA more efficient way might be to extract the coefficients and store them in a data set named bio_coef:\n\nbio_coef &lt;- \n  bio_boot %&gt;%\n  select(-splits) %&gt;%\n  unnest(cols = c(coef_info)) %&gt;%\n  select(id, term, estimate) \n\nbio_coef\n\n# A tibble: 1,002 × 3\n   id           term  estimate\n   &lt;chr&gt;        &lt;chr&gt;    &lt;dbl&gt;\n 1 Bootstrap001 b0       -3.50\n 2 Bootstrap001 b1        2.85\n 3 Bootstrap002 b0       -3.27\n 4 Bootstrap002 b1        2.73\n 5 Bootstrap003 b0       -3.25\n 6 Bootstrap003 b1        2.73\n 7 Bootstrap004 b0       -3.27\n 8 Bootstrap004 b1        2.73\n 9 Bootstrap005 b0       -3.35\n10 Bootstrap005 b1        2.79\n# ℹ 992 more rows\n\n\nThen, we can visualize the distribution in the coefficients from the 500 samples in the form of a histogram:\n\np.coef &lt;- bio_coef %&gt;% \n  ggplot(aes(x = estimate)) + \n  geom_histogram(bins = 20, col = \"white\") + \n  facet_wrap(~ term, scales = \"free_x\")\n\np.coef\n\n\n\n\n\n\n\n\nWhile it’s helpful to visualize the distribution of coefficients, we also may want to quantify the key quantiles of them. The int_pctl() function calculates confidence intervals from bootstrap samples. Here are the lower and upper confidence interval values from the bootstrapped estimates:\n\npct_ints &lt;- int_pctl(bio_boot, coef_info, alpha = 0.05)\n\npct_ints\n\n# A tibble: 2 × 6\n  term  .lower .estimate .upper .alpha .method   \n  &lt;chr&gt;  &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n1 b0     -3.82     -3.21  -2.08   0.05 percentile\n2 b1      2.23      2.72   2.96   0.05 percentile\n\n\nWe can add these values to our visualization to see that the upper and lower bounds (in blue) are not uniformly distributed around the mean estimate (in orange) for each coefficient:\n\np.coef + \n  geom_vline(data = pct_ints, aes(xintercept = .estimate), \n             col = \"orange\", linewidth = 2, linetype = \"dashed\") + \n  geom_vline(data = pct_ints, aes(xintercept = .lower), \n             col = \"blue\") + \n  geom_vline(data = pct_ints, aes(xintercept = .upper), \n             col = \"blue\")\n\n\n\n\n\n\n\n\nNext, we can use the augment() function to obtain the fitted and residual values for each resampled data point. We’ll sample from 250 runs to limit some of our output:\n\nboot_aug &lt;- \n  bio_boot %&gt;% \n  sample_n(250) %&gt;% \n  mutate(augmented = map(models, augment)) %&gt;% \n  unnest(augmented)\n\nboot_aug\n\n# A tibble: 141,500 × 8\n   splits            id       models coef_info  AG_DW ST_OB_D_BH .fitted  .resid\n   &lt;list&gt;            &lt;chr&gt;    &lt;list&gt; &lt;list&gt;     &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 &lt;split [566/218]&gt; Bootstr… &lt;nls&gt;  &lt;tibble&gt;  12.1         6.86   7.32   4.74  \n 2 &lt;split [566/218]&gt; Bootstr… &lt;nls&gt;  &lt;tibble&gt;   0.862       3.05   0.763  0.0987\n 3 &lt;split [566/218]&gt; Bootstr… &lt;nls&gt;  &lt;tibble&gt;  53.8        12.2   36.4   17.4   \n 4 &lt;split [566/218]&gt; Bootstr… &lt;nls&gt;  &lt;tibble&gt;   1.18        2.54   0.459  0.720 \n 5 &lt;split [566/218]&gt; Bootstr… &lt;nls&gt;  &lt;tibble&gt;  18.3        10.9   26.8   -8.53  \n 6 &lt;split [566/218]&gt; Bootstr… &lt;nls&gt;  &lt;tibble&gt;   8.66        6.86   7.32   1.34  \n 7 &lt;split [566/218]&gt; Bootstr… &lt;nls&gt;  &lt;tibble&gt;  20.9        11.4   30.4   -9.52  \n 8 &lt;split [566/218]&gt; Bootstr… &lt;nls&gt;  &lt;tibble&gt;  11.9         7.62   9.82   2.11  \n 9 &lt;split [566/218]&gt; Bootstr… &lt;nls&gt;  &lt;tibble&gt;   5.90        5.33   3.63   2.26  \n10 &lt;split [566/218]&gt; Bootstr… &lt;nls&gt;  &lt;tibble&gt;   4.40        5.84   4.68  -0.283 \n# ℹ 141,490 more rows\n\n\nThen, we can visualize how the resampling approach with bootstrapping results in varying relationships in predicting aboveground tree biomass based on tree diameter, with each bootstrapped model shown in blue:\n\nggplot(boot_aug, aes(x = ST_OB_D_BH, y = AG_DW )) +\n  geom_line(aes(y = .fitted, group = id), alpha = .2, col = \"blue\") +\n  geom_point() +\n  labs(x = \"Diameter at breast height (cm)\", \n       y = \"Aboveground dry weight (kg)\") +\n  theme(panel.background = element_rect(fill = \"NA\"),\n        axis.line = element_line(color = \"black\"))"
  },
  {
    "objectID": "post/2023-04-12-bootstrapped-resampling-estimates-to-predict-tree-height/index.html#comparing-biomass-model-predictions",
    "href": "post/2023-04-12-bootstrapped-resampling-estimates-to-predict-tree-height/index.html#comparing-biomass-model-predictions",
    "title": "Bootstrapped resampling to model tree biomass",
    "section": "Comparing biomass model predictions",
    "text": "Comparing biomass model predictions\nFinally, we may be interested to see how the different models we’ve considered result in predictions of biomass. The tree_test object is a small data set that applies each of three predictions from the models we’ve considered:\n\nThe original Jenkins et al. 2004 model for the pine species group,\nThe nonlinear least squares model fit with parametric techniques (from the m.bio object), and\nThe NLS models fit with bootstrap estimates.\n\nThe AG_DW_pred variable stores the predicted biomass:\n\ntree_test &lt;- tibble(model = rep(c(\"Jenkins et al. 2004\", \n                                  \"NLS refit\", \n                                  \"NLS refit, with bootstrap\"),\n                                c(20, 20, 20)),\n                    dbh = rep(seq(1, 20, by = 1), 3))\n\nfx_AG_DW &lt;- function(model, ST_OB_D_BH){\n  if(model == \"Jenkins et al. 2004\")\n    {AG_DW &lt;- exp(-2.5356 + 2.4349*log(ST_OB_D_BH))}\n  else if(model == \"NLS refit\")\n    {AG_DW &lt;- exp(-3.31397 + 2.75972*log(ST_OB_D_BH))}\n  else if(model == \"NLS refit, with bootstrap\")\n    {AG_DW &lt;- exp(as.numeric(pct_ints[1,3]) + \n                    as.numeric(pct_ints[2,3])*log(ST_OB_D_BH))}\n  else(AG_DW &lt;- 0)\n  return(AG_DW = AG_DW)\n}\n\ntree_test$AG_DW_pred &lt;- mapply(fx_AG_DW, \n                               model = tree_test$model, \n                               ST_OB_D_BH = tree_test$dbh)\n\nThen, we can plot the models to observe their differences. The original Jenkins et al. model underpredicts at larger diameters relative to the models that were refit to the data:\n\nggplot(tree_test, aes(x = dbh, y = AG_DW_pred, col = model)) +\n  geom_line() +\n  geom_point() +\n  labs(x = \"Diameter at breast height (cm)\", \n       y = \"Predicted aboveground dry weight (kg)\") +\n  theme(panel.background = element_rect(fill = \"NA\"),\n        axis.line = element_line(color = \"black\"))"
  },
  {
    "objectID": "post/2023-04-12-bootstrapped-resampling-estimates-to-predict-tree-height/index.html#conclusion",
    "href": "post/2023-04-12-bootstrapped-resampling-estimates-to-predict-tree-height/index.html#conclusion",
    "title": "Bootstrapped resampling to model tree biomass",
    "section": "Conclusion",
    "text": "Conclusion\nUsing bootstrapping to estimate regression coefficients has many benefits. It works well with a small number of observations and the analyst does not need to rely on distribution assumptions about the data and the resulting error terms. The tidymodels package makes performing bootstrap methods a breeze, and a variety of functions enable the analyst to visualize and interpret output from the bootstrap samples.\n–\nSpecial thanks to Julia Silge’s excellent tutorial on tidymodels that inspired this post, and the tidymodels page from Posit for helpful code."
  },
  {
    "objectID": "post/2023-05-25-quantifying-the-shade-tolerance-of-climate-adapted-tree-species-in-the-northeast/index.html#section",
    "href": "post/2023-05-25-quantifying-the-shade-tolerance-of-climate-adapted-tree-species-in-the-northeast/index.html#section",
    "title": "Quantifying the shade tolerance of climate-adapted tree species in the Northeast",
    "section": "",
    "text": "A few weeks ago I attended Dr. Tony D’Amato’s excellent webinar “Looking at Forest Carbon from Multiple Dimensions”. The content of the webinar was excellent and I encourage anyone involved with forest management and forest carbon to view it.\nOne primary concept from the webinar stood out to me: many of the future tree species that are anticipated to be well adapted to future conditions are less shade tolerant that current trees on the landscape. Tony described climate-adapted species as being “large gap specialists”, most of which are intermediate or intolerant to shade.\nThis has tremendous implications for managing forests for carbon and climate. If climate-adapted species require more sunlight, fewer of them can grow in the same amount of area compared to our current more shade tolerant species. Hence, carbon stocks will be lower if we manage forests with climate-adapted species in mind.\nI was curious to quantify these differences in shade tolerance in my own region of the northeastern US. A few years ago, the Northern Institute of Applied Climate Science released their report “New England and Northern New York Forest Ecosystem Vulnerability Assessment and Synthesis”. The report contains valuable data on the changes in suitable habitat for tree species using information from the Climate Change Tree Atlas.\nThe report contains projected changes in potential suitable habitat for 102 tree species in the northeastern US, with forecasts through 2099. (See their Table 10 and Appendix 4). Two model scenarios were implemented: the PCM B1 and GFDL A1FI scenarios. The primary difference is that the GFDL A1FI scenario represented higher greenhouse gas emissions and greater climate warming than the PCM B1 scenario. So, both scenarios can be used to represent two different extremes of a future climate.\nFor each species, future suitable habitat is labeled for both models as:\n\nLarge decrease,\nSmall decrease,\nNo change,\nSmall increase,\nLarge increase, or\nNew habitat (species is not currently in the region but is projected to be).\n\nI also acquired species-specific data on shade tolerance values. The data on tolerance values are from a paper I authored in 2014 on plant functional traits (see supplemental data file 1 in the article.) The tolerance values were obtained from the Niinemets and Valladares study published in 2006.\nBy using boxplots, we can see the general trends in species shade tolerance by classes of future suitable habitat. Indeed, species that are projected to see increases in suitable habitat or are anticipated to see new habitat in the region display lower shade tolerance values relative to species that will see decreases in suitable habitat:\n\n\n\n\n\n\n\n\n\nThe GFDL A1F1 scenario predicts more species seeing large increases in suitable habitat compared to the PCM B1 model. Generally, the PCM B1 model is more “conservative”, with a greater number of species projected to have no change in suitable habitat through 2099.\nUnder the PCM B1 scenario, the median shade tolerance for species anticipated to see a large increase in suitable habitat is 1.90, compared to a median value of 3.13 for species projected to see a small decrease. For the GFDL A1F1 scenario, the median shade tolerance for species anticipated to see a large increase is 2.74, compared to a median value of 3.31 for species projected to see a large decrease:\n\n\n\n\nSummary of species changes in suitable habitat, US Northeast.\n\n\nScenario\nChange\nNumber of species\nMedian shade tolerance\n\n\n\n\nGFDL A1F1\nLarge decrease\n13\n3.31\n\n\nGFDL A1F1\nLarge increase\n32\n2.74\n\n\nGFDL A1F1\nNew habitat\n22\n2.23\n\n\nGFDL A1F1\nNo change\n5\n2.51\n\n\nGFDL A1F1\nSmall decrease\n12\n2.98\n\n\nGFDL A1F1\nSmall increase\n16\n2.72\n\n\nPCM B1\nLarge increase\n6\n1.90\n\n\nPCM B1\nNew habitat\n13\n2.13\n\n\nPCM B1\nNo change\n39\n3.03\n\n\nPCM B1\nSmall decrease\n10\n3.13\n\n\nPCM B1\nSmall increase\n21\n2.72\n\n\n\n\n\n\n\n\nSpecies that are projected to see decreases in suitable area in the US Northeast include lowland species that are tolerant to flooding, such as black spruce, tamarack, and black ash. Species that are projected to see increases in suitable area include several oaks and shrub species. Several southern pines and oak species are anticipated to have new habitat in the region:\n\n\n\n\nSpecies changes in suitable habitat with agreement between both PCM B1 and GFDL A1F1 model scenarios, US Northeast.\n\n\ncommon_name\nShadeTol\nChange\n\n\n\n\nblack spruce\n4.08\nLarge decrease\n\n\nblack locust\n1.72\nLarge increase\n\n\neastern redcedar\n1.28\nLarge increase\n\n\nflowering dogwood\n4.87\nLarge increase\n\n\nred mulberry\n2.34\nLarge increase\n\n\nsassafras\n1.68\nLarge increase\n\n\nyellow-poplar\n2.07\nLarge increase\n\n\nbald cypress\n2.13\nNew habitat\n\n\ncherrybark oak\n2.24\nNew habitat\n\n\ncommon persimmon\n4.21\nNew habitat\n\n\nloblolly pine\n1.99\nNew habitat\n\n\npawpaw\n3.95\nNew habitat\n\n\npond pine\n1.47\nNew habitat\n\n\nsand pine\n2.21\nNew habitat\n\n\nshortleaf pine\n1.86\nNew habitat\n\n\nsouthern red oak\n2.50\nNew habitat\n\n\nsweetgum\n1.59\nNew habitat\n\n\nVirginia pine\n1.99\nNew habitat\n\n\nAmerican chestnut\n3.06\nNo change\n\n\nAtlantic white-cedar\n3.50\nNo change\n\n\nscrub oak\n2.51\nNo change\n\n\nblack ash\n2.96\nSmall decrease\n\n\ntamarack\n0.98\nSmall decrease\n\n\neastern redbud\n3.00\nSmall increase\n\n\nnorthern red oak\n2.75\nSmall increase\n\n\nOhio buckeye\n3.49\nSmall increase\n\n\npost oak\n2.16\nSmall increase\n\n\nsourwood\n2.70\nSmall increase\n\n\nsweet birch\n2.58\nSmall increase\n\n\n\n\n\n\n\n\nBeing aware of the light requirements for species can provide insights into expectations for meeting management goals related to forest carbon stocks and sequestration. A naive carbon management strategy might be to plant as many trees as possible, but the shade tolerance of species should be considered when evaluating the tradeoffs between carbon stocking and species requirements.\n–\nBy Matt Russell. If you’re interested in the data and code behind this analysis, see this GitHub page. Email Matt with any questions or comments."
  },
  {
    "objectID": "post/2023-07-15-it-s-okay-to-look-at-solutions-for-statistics-problems/index.html",
    "href": "post/2023-07-15-it-s-okay-to-look-at-solutions-for-statistics-problems/index.html",
    "title": "It’s okay to look at the solutions for statistics and data science exercises",
    "section": "",
    "text": "The cover of a document titled “Statistics in Natural Resources: Applications with R: Exercise Soultions.”\n\n\n\n\n\n\nA few years ago I completed the RStudio Instructor training course. It was a multi-day, online training workshop and I learned less about the ins and outs of coding and analysis in R, but more about the strategies and philosophy of teaching data science principles.\nThe course prepares instructors to teach the principles of the tidyverse, especially drawing on the concepts for the excellent book R for Data Science. One of the best components of this book is the numerous questions and exercises that appear often within the chapters. Providing these formative assessments to a learner after reading about and seeing example code is an excellent way to learn new concepts.\nDuring the RStudio training, I also learned of a companion book that worked through the solutions to each exercise in R for Data Science. I wasn’t aware of this resource when I was working through R4DS a few years earlier. The instructor strongly encouraged everyone to incorporate it into their learning journey.\nAs an educator, I had mixed feelings about making the solutions for exercises available to students. The disadvantages are that students may not put in sufficient effort to solve problems. Or, students may spend too much time looking for the solutions manual, or trying to adapt a solution for a problem they have they answer for to a new problem they’re presented with.\nOn a more selfish level, as an instructor there’s a secure feeling you get by having confidence that you can reuse your questions later in the course or workshop or the next time you teach the material. Good questions take considerable time and effort to create, test, and revise. There’s a certain amount of intellectual property that comes along with creating good questions.\nUltimately, as I was working through my text Statistics in Natural Resources: Applications with R, I decided to make the solutions to all exercises available online. You can find them here on the Routledge website. In total, I think the benefits of doing this outweigh the disadvantages fom a learner’s perspective.\n\nFirst, learning happens when you watch other people work. Especially in statistics and data science, reading through someone else’s code can help you see different ways of solving a problem. The solutions might use functions and techniques that you otherwise wouldn’t have thought of.\nSecond, looking at the solutions can help you get over a roadblock. I’ve told learners in the past to “knock your head against the wall, but not so hard that it hurts.” You learn by being uncomfortable and challenged, but if you’ve spent half a day and can’t get over the hurdle, it’s okay to get a helpful hint.\nLastly, solutions allow you to check your work. This is especially beneficial to learners being introduced to a topic (as is the audience for the Statistics in Natural Resources book). Learners can gain confidence in the material knowing that their efforts arrived at the correct answer, and the solution provides that immediate feedback. This last point is especially important today as so many learners are working independently, often in an asynchronous course structure.\n\nTo summarize, there are many benefits of making solutions available for statistics and data science exercises. As I’ve done with the Statistics in Natural Resources book, providing solutions to exercises enables learners to check their work, overcome challenges, and learn from the work of others.\n–\nBy Matt Russell. Email Matt with any questions or comments."
  },
  {
    "objectID": "post/2023-08-22-implementing-adaptive-silviculture-scenarios-in-the-forest-vegetation-simulator/index.html",
    "href": "post/2023-08-22-implementing-adaptive-silviculture-scenarios-in-the-forest-vegetation-simulator/index.html",
    "title": "Implementing climate adaptive silviculture scenarios in the Forest Vegetation Simulator",
    "section": "",
    "text": "The Adaptive Silvicultre for Climate Change Project at the Chippewa National Forest, Minnesota, USA. Photo: Josh Kragthorpe."
  },
  {
    "objectID": "post/2023-08-22-implementing-adaptive-silviculture-scenarios-in-the-forest-vegetation-simulator/index.html#connecticut-fia-data",
    "href": "post/2023-08-22-implementing-adaptive-silviculture-scenarios-in-the-forest-vegetation-simulator/index.html#connecticut-fia-data",
    "title": "Implementing climate adaptive silviculture scenarios in the Forest Vegetation Simulator",
    "section": "Connecticut FIA data",
    "text": "Connecticut FIA data\nFor this analysis I’ll use three forest inventory plots measured in the state of Connecticut, obtained from the US Forest Inventory an Analysis network of plots. These plots were measured between 2018 and 2021 using a fixed radius sample design. These plots were dominated by oak trees. Two of the plots were recorded as occurring in the white oak/red oak/hickory forest type and the other in the white oak forest type.\nBefore harvests, initial basal area ranged between 155 and 194 square feet per acre. You can download the FVS-ready files I’ve compiled for these stands at this page on GitHub."
  },
  {
    "objectID": "post/2023-08-22-implementing-adaptive-silviculture-scenarios-in-the-forest-vegetation-simulator/index.html#adaptive-silviculture-treatments",
    "href": "post/2023-08-22-implementing-adaptive-silviculture-scenarios-in-the-forest-vegetation-simulator/index.html#adaptive-silviculture-treatments",
    "title": "Implementing climate adaptive silviculture scenarios in the Forest Vegetation Simulator",
    "section": "Adaptive silviculture treatments",
    "text": "Adaptive silviculture treatments\nFor the treatments to specify in FVS, we’ll follow a similar prescription that is being conducted at an Adaptive Silviculture for Climate Change experiment in New Hampshire. This treatment follows a resilience approach to climate change, where the treatment will allow some change in current forest conditions, but will also encourage its return to original conditions. The silvicultural strategy is to generally use group and single-tree selection across the stand. The subtreatments within the stand are:\n\nGaps: Small gaps that leave openings for light-dependent species (20% of stand),\nReserves: No management within small areas across the stand (20% of stand), and\nMatrix: Thinning to 80 square feet per acre (60% of the stand)\n\nThis approach seeks to maintain the diversity of native tree species while also increasing drought-adapted species like oaks."
  },
  {
    "objectID": "post/2023-08-22-implementing-adaptive-silviculture-scenarios-in-the-forest-vegetation-simulator/index.html#simulating-adaptive-silviculture-with-fvs",
    "href": "post/2023-08-22-implementing-adaptive-silviculture-scenarios-in-the-forest-vegetation-simulator/index.html#simulating-adaptive-silviculture-with-fvs",
    "title": "Implementing climate adaptive silviculture scenarios in the Forest Vegetation Simulator",
    "section": "Simulating adaptive silviculture with FVS",
    "text": "Simulating adaptive silviculture with FVS\nThe FVS model has an excellent web interface, which can be downloaded here. I’ll use this interface and specify to use the Northeast variant to the FVS model.\nThe three stands in the Connecticut FIA data are labeled with stand identification numbers 9_11_157, 9_11_67, and 9_3_389. We can add all of them by pressing Add Selected Stands and include them in the Run Contents window:"
  },
  {
    "objectID": "post/2023-08-22-implementing-adaptive-silviculture-scenarios-in-the-forest-vegetation-simulator/index.html#fvs-output",
    "href": "post/2023-08-22-implementing-adaptive-silviculture-scenarios-in-the-forest-vegetation-simulator/index.html#fvs-output",
    "title": "Implementing climate adaptive silviculture scenarios in the Forest Vegetation Simulator",
    "section": "FVS output",
    "text": "FVS output\nWe’ll run a 50-year simulation (through 2073), which can be changed under the Time window. Another keyword that I’ll set is NoTriple, which turns off the tripling of tree records early in the simulation. It can be found under the Keywords menu.\nFVS provides multiple outputs to select from under the Select Outputs tab. I’ll choose Stand Visualization, Carbon and fuels, Stand structure, and Regeneration. Choose Save and Run under the Run window to perform the simulation.\nAfter the simulation is complete for this stand, select View Outputs, Explore, and then Graphs. You can see the merchantable volume trends within the gaps, reserve, and matrix subtreatments:\n\nFor the gaps subtreatment (in red), all of the volume is removed, and by the end of the 50-year simulation our natural regeneration has not grown enough to yield any merchantable volume.\nFor the reserve subtreatment (in green), the stand continues to grow throughout the simulation.\nFor the matrix subtreatment (in blue), some volume is initially removed and growth continues throughout the simulation."
  },
  {
    "objectID": "post/2023-08-22-implementing-adaptive-silviculture-scenarios-in-the-forest-vegetation-simulator/index.html#conclusion",
    "href": "post/2023-08-22-implementing-adaptive-silviculture-scenarios-in-the-forest-vegetation-simulator/index.html#conclusion",
    "title": "Implementing climate adaptive silviculture scenarios in the Forest Vegetation Simulator",
    "section": "Conclusion",
    "text": "Conclusion\nRunning adaptive silvicultural management in models like FVS is possible and can account for diverse harvest scenarios like thinning treatments, group selection harvests, and planting climate-adapted species. It’s important to note that FVS is a distance-independent model and doesn’t rely on a tree’s spatial location to determine growth and mortality. Many climate-adaptive forest management treatments rely on spatial heterogeneity as a part of maintaining diversity and resilience in the face of future uncertain climates. Unfortunately, few growth and yield models can incorporate spatial variability in addition to having the capability to forecast a variety of forest management treatments like FVS.\nThe process of adding subtreatments within FVS to account for different management actions and viewing the composite tables allows for the assessment of management impacts at the stand scale. Still, if many stands are run within FVS with multiple subtreatments, I like to export the data to other software like R and do my own calculations of stand-level attributes. Although models like FVS were not originally designed to incorporate complex forest management scenarios like adaptive silviculture, there are strategies to forecast what stands may look like under an adaptive management framework.\n–\nBy Matt Russell. Check out the FVS-ready files on GitHub if you want to replicate this simulation, or try a another simulation using the other two stands. Email Matt with any questions or comments."
  },
  {
    "objectID": "post/2023-09-21-selecting-the-number-of-plots-to-measure-in-a-forest-carbon-inventory/index.html#proportional-allocation",
    "href": "post/2023-09-21-selecting-the-number-of-plots-to-measure-in-a-forest-carbon-inventory/index.html#proportional-allocation",
    "title": "Stratifying the number of plots to measure in a forest carbon inventory",
    "section": "Proportional allocation",
    "text": "Proportional allocation\nSay our analysis indicates we need to collect data from 100 new inventory plots to achieve a desired level of precision (e.g., to be within 5% of the mean carbon storage). Proportional allocation would distribute the 100 field plots according to their total area. That is, we can collect more field plots in larger-size strata. The following R function named allocate() can determine the appropriate number of desired plots to collect in each stratum. We’ll round up the number of plots with the ceiling() function to make sure we measure an entire plot:\n\nallocate &lt;- function(acres.stratum, total.acres, total.plots){\n  num.plots = ceiling((acres.stratum / total.acres) * total.plots)\n  return(num.plots)\n  }\n\nWe can apply the function to the stratum data frame:\n\nallocate(acres.stratum = stratum$`Stratum area (acres)`,\n         total.acres = sum(stratum$`Stratum area (acres)`),\n         total.plots = 100)\n\n[1] 33 39 29\n\n\nSo, the proportional allocation method indicates we should collect 33, 39, and 29 plots in the small-, medium-, and large-diameter stands, respectively. Because more acreage is found in medium-sized trees, we collect more plots there."
  },
  {
    "objectID": "post/2023-09-21-selecting-the-number-of-plots-to-measure-in-a-forest-carbon-inventory/index.html#optimal-allocation",
    "href": "post/2023-09-21-selecting-the-number-of-plots-to-measure-in-a-forest-carbon-inventory/index.html#optimal-allocation",
    "title": "Stratifying the number of plots to measure in a forest carbon inventory",
    "section": "Optimal allocation",
    "text": "Optimal allocation\nThe optimal allocation method distributes the 100 field plots to each stratum that provides the smallest amount of variability possible. In addition to the total area, also required in this calculation is the standard deviation of carbon found within each stratum. The total number of plots to sample in each stratum would be represented by the stratum area multiplied by the standard deviation of carbon, as seen in the table above. The Area x SD variable can be input into the allocate() function to determine the appropriate number of plots to collect with optimal allocation:\n\nallocate(acres.stratum = stratum$`Area x SD`,\n         total.acres = sum(stratum$`Area x SD`),\n         total.plots = 100)\n\n[1] 28 36 38\n\n\nSo, the optimal allocation method indicates we should collect 28, 36, and 38 plots in the small-, medium-, and large-diameter stands, respectively. In other words, because the stand deviation of carbon is lower in the small- and medium-diameter stands, we can focus our sampling in the large-diameter stands where there is more variability in carbon. The optimal strategy indicates sampling three fewer plots in the medium-diameter stands compared to the proportional allocation method, even though it’s the stand with the largest area.\n\n\n\n\nDistribution of plots to sample for proportional and optimal allocation methods.\n\n\nStand size class\nStratum area (acres)\nSD (metric tons/acre)\nArea x SD\nPlots to measure - proportional\nPlots to measure - optimal\n\n\n\n\nSmall diameter (&lt; 5 in)\n65\n7.8\n507\n33\n28\n\n\nMedium diameter (5 - 10 in)\n77\n8.3\n639\n39\n36\n\n\nLarge diameter (&gt; 10 in)\n58\n11.7\n679\n29\n38\n\n\nSUM\n200\nNA\n1825\n100\n100\n\n\n\n\n\n\n\n\nThere is a lot more that goes into planning a forest carbon inventory, but understanding how many plots to measure and where to measure them is an important first step. Using stratified random sampling is an efficient approach to determine a precise estimate of forest carbon.\n–\nSpecial thanks to Micky Allen for catching an error in the numbers of plots in Table 2.\nBy Matt Russell. Email Matt with any questions or comments."
  },
  {
    "objectID": "post/2023-10-31-calibrating-fvs-how-much-does-adding-previous-diameter-and-height-growth-change-predictions/index.html",
    "href": "post/2023-10-31-calibrating-fvs-how-much-does-adding-previous-diameter-and-height-growth-change-predictions/index.html",
    "title": "How much does adding previous diameter and height growth change FVS predictions?",
    "section": "",
    "text": "A brook in western Maine, USA."
  },
  {
    "objectID": "post/2023-10-31-calibrating-fvs-how-much-does-adding-previous-diameter-and-height-growth-change-predictions/index.html#western-maine-fia-data",
    "href": "post/2023-10-31-calibrating-fvs-how-much-does-adding-previous-diameter-and-height-growth-change-predictions/index.html#western-maine-fia-data",
    "title": "How much does adding previous diameter and height growth change FVS predictions?",
    "section": "Western Maine FIA data",
    "text": "Western Maine FIA data\nData were compiled from the FIA database from two counties in western Maine (Franklin and Oxford counties). Forests are mixed species but primarily of the maple-beech-birch or spruce-fir forest types. The most recent measurement was simulated in FVS, collected between 2017 and 2021. I used the Northeast variant of FVS and ran a simulation for 100 years.\nMaine is on a five year remeasurement cycle, so previous diameter and height measurements were collected five years prior. In total there were 294 FIA plots, each run as an individual stand. A total of 13,974 trees made up these stands. Average diameter at the start of the simulation was 6.9 inches and average height was 39.8 feet.\nFor perspective, the average 10-year diameter and height growth measured on these trees was 0.95 inches and 9.2 feet, respectively. Here is the distribution of basal area at the start of the simulation:"
  },
  {
    "objectID": "post/2023-10-31-calibrating-fvs-how-much-does-adding-previous-diameter-and-height-growth-change-predictions/index.html#calibrating-fvs-growth",
    "href": "post/2023-10-31-calibrating-fvs-how-much-does-adding-previous-diameter-and-height-growth-change-predictions/index.html#calibrating-fvs-growth",
    "title": "How much does adding previous diameter and height growth change FVS predictions?",
    "section": "Calibrating FVS growth",
    "text": "Calibrating FVS growth\nIn the FVS input file, the DG and HTG variables were completed with FIA data. These variables represent the growth occurring in the period prior to the current measurement. The two primary keywords used in FVS to represent this growth were the CalbStat and Growth keywords, all of which are described in the FVS Keyword Guide.\nFor the CalbStat keyword, I changed the minimum number of growth measurements required for calibration to three (from the default of five) because these were mixed-species stands and I wanted the model to trigger the calibration:\n\n\nFor the Growth keyword, I changed the length of the growth period to 10 years to correspond to the FVS cycle length used in the Northeast variant:\n\n\nI ran two sets of simulations: one as FVS “out-of-the-box” with no calibrations and another with the calibrations as described above. No management treatments or disturbances were applied to the simulations. It’s important to note that the Northeast variant of FVS doesn’t have an ingrowth model built in (you have to manually add trees if you want regeneration). So all of the trees that appeared in the FIA inventory were carried through in the simulation: some grew more and some died by the end of the simulation in 2123."
  },
  {
    "objectID": "post/2023-10-31-calibrating-fvs-how-much-does-adding-previous-diameter-and-height-growth-change-predictions/index.html#comparing-fvs-calibrated-and-out-of-the-box-predictions",
    "href": "post/2023-10-31-calibrating-fvs-how-much-does-adding-previous-diameter-and-height-growth-change-predictions/index.html#comparing-fvs-calibrated-and-out-of-the-box-predictions",
    "title": "How much does adding previous diameter and height growth change FVS predictions?",
    "section": "Comparing FVS calibrated and out-of-the-box predictions",
    "text": "Comparing FVS calibrated and out-of-the-box predictions\nOn average, growth calibrations to the FVS model resulted in larger aboveground carbon stocks throughout the 100 year simulation for these plots in western Maine. Throughout the simulation, carbon stocks were consistently 2 to 4% greater using calibrations compared to out-of-the-box predictions. Here are the mean carbon stocks throughout the simulation, with 95% confidence limits to show the variability:\n\n\n\n\n\n\n\n\n\nAt the individual tree level, diameter increment generally declined as the simulation reaches year 100. Calibrations showed greater growth compared to out-of-the-box predictions early in the simulation, then this growth increase declined through time:\n\n\n\n\n\n\n\n\n\nLooking at overall trends reveals important trends in this calibrations exercise. But looking at species differences can allow for more insight and allows the user to investigate model performance more closely. Comparing the eight most abundant species in this region, calibrations increased diameter increment for balsam fir, red maple, red spruce, and yellow birch. However, calibrations decreased diameter increment for eastern hemlock and paper birch.\n\n\n\n\n\n\n\n\n\nWhen looking at the small tree height model for trees with diameters less than 5.0 inches, calibrations increased height increment early in the simulation (e.g., in 2020), then generally stayed similar throughout the 100-year forecast. There was some indication that calibrations decreased height increment in later years (e.g., in years 2083 and beyond), but there were likely few trees that remained less than 5.0 inches this far into the simulation:\n\n\n\n\n\n\n\n\n\nWhen investigating the performance of the calibrations with respect to the small tree height model, there doesn’t seem to be too many apparent differences when comparing within species:"
  },
  {
    "objectID": "post/2023-10-31-calibrating-fvs-how-much-does-adding-previous-diameter-and-height-growth-change-predictions/index.html#conclusions",
    "href": "post/2023-10-31-calibrating-fvs-how-much-does-adding-previous-diameter-and-height-growth-change-predictions/index.html#conclusions",
    "title": "How much does adding previous diameter and height growth change FVS predictions?",
    "section": "Conclusions",
    "text": "Conclusions\nAs these results show, calibrating the FVS model with previous diameter and height growth measurements can change the trajectory of stands when simulated for up to 100 years. Many individual tree growth models like FVS rely on the diameter and height growth functions heavily, as they are fed into other equations such as tree volume, biomass, and carbon.\nI should note that dialing in the growth component of a model is just one type of calibration that can be made. The mortality and regeneration functions also have a considerable impact to stand level attributes. I was actually surprised to see the “growth bump” after calibrating FVS with these data, as I’ve often found FVS to already overpredict common stand attributes like volume and carbon without any calibrations applied. I suspect working more on calibrating the mortality functions would increase some of the mortality observed in these stands, providing a trade off with growth increases. In combination with growth calibrations, this could lead to model results that may be more dependable and meet forester expectations.\n–\nBy Matt Russell. Email Matt with any questions or comments."
  },
  {
    "objectID": "post/2023-11-24-a-list-of-r-packages-for-forestry-applications/index.html",
    "href": "post/2023-11-24-a-list-of-r-packages-for-forestry-applications/index.html",
    "title": "A list of R packages for forestry applications",
    "section": "",
    "text": "Image created with DALL·E 3.\n\n\n\n\n\n\nTwo years ago I wrote about the different R packages that are available to use in forestry applications. Back then, there were 16,166 packages archived on the Comprehensive R Archive Network (CRAN). Today, the CRAN repository contains 20,102 packages.\nThe power of R comes from its diversity of packages. A package is a collection of functions and data sets developed by R users. The value of using R packages is that someone else might have already written a suite of functions for you. These can include your coworkers, colleagues, and other professionals. R packages are written to be collaborative so that they can be shared with others. In turn, users can provide feedback on the functions and uses of the package to improve it.\nI recently updated my list of R packages used in forestry. It contains 68 packages that have specific applications for forest analysts. There were 31 packaged listed in 2020, so the profession has seen a 119% increase in forestry R packages in the past two years. This is great for the students and professionals that use R in the forestry community.\nInterestingly, I found many newer R packages under the following themes:\n\nTree data sets. There are several new packages that contain data sets of trees, for example pdxtrees, a package with tree data from trees in Portland, Oregon, or perutimber, a catalog of timber species found in the Peruvian Amazon. These kinds of packages make for excellent tools for teaching R concepts with forestry data, and I hope there are more of them in the future.\nPackages for analyzing tree rings. There are many new packages available for analyzing tree ring data, including DendroSync, measuRing, and xring among others.\n\nPackages for simulating tree growth. Many recent packages make it easier to integrate R with many tree growth models. Since the packages r3PG, sitree, and efdm for some examples of this.\n\nThese packages include only those that are archived on the CRAN repository. Many more R packages exist through other services such as Github. Not all R packages are available on CRAN (because it’s difficult), but Github allows users to easily see packages and the code behind them as they’re being developed.\nAny package available on CRAN has been vetted with scrutiny, so you can be sure that the forestry-specific ones are ready for a “prime time” analysis. In my searching for all forestry packages on CRAN, I’ve realized that the keywords “tree” and “forest” do not help much when searching for forestry packages. Most packages that mention “tree” or “forest” in their description are about topics such as random forests, regression trees, or decision trees:\n\n\n\n\n\n\n\n\n\nWhich R package is missing from the list? Email me with your comments and I’d love to hear which forestry packages you use.\n–\nBy Matt Russell. For more, see my monthly email newsletter for data and analytics trends in the forest products industry."
  },
  {
    "objectID": "post/2023-12-22-most-popular-blog-posts-and-podcasts-in-2023/index.html",
    "href": "post/2023-12-22-most-popular-blog-posts-and-podcasts-in-2023/index.html",
    "title": "Most popular blog posts and podcasts in 2023",
    "section": "",
    "text": "The end of the year is an excellent time to reflect on your work. One of the things I value is providing content that the forest analytics community enjoys and learns from.\nIn addition to publishing a weekly-ish newsletter on LinkedIn with 2,000+ subscribers, I published 22 posts to the Arbor Custom Analytics blog in 2023 on topics relating to data and forests. Here are the most read posts from the Arbor Custom Analytics blog in 2023:\n\nGetting your data into R from Google Sheets\nMixed models in R: a primer\nA list of R packages for forestry applications\nLorey’s height: the remote sensing way to estimate tree height\n\nFrom January through June, I also published The Forest Analytics Landing podcast every other week. Here’s a Spotify Wrapped image showing the analytics in 2023 of the Forest Analytics Landing podcast:\n\n\n\n\n\n\n\n\n\nHere are the most listened podcasts from the year:\n\n027: Forest carbon trends to watch in 2023\n034: Recent data on greenhouse gas emissions and removals from US forests\n028: Forestland on the blockchain\n035: Carbon sequestration and harvested wood products\n\nHave an idea for a future topic you’d like to learn about? Send me an email with any suggestions!\n–\nThanks to everyone for reading and listening this year. Have a blessed New Year!"
  },
  {
    "objectID": "post/2024-01-21-are-winter-temperatures-correlated-with-stumpage-prices-in-maine/index.html",
    "href": "post/2024-01-21-are-winter-temperatures-correlated-with-stumpage-prices-in-maine/index.html",
    "title": "Are winter temperatures correlated with stumpage prices in Maine?",
    "section": "",
    "text": "I was inspired after reading an article written by Lloyd Irland in The Northern Logger (January 2024) about the upswings and downswings in lumber prices in the last several decades. High prices for lumber were recently seen during the COVID-19 pandemic when everybody was remodeling and installing new wooden decks with all the free time they had. The intricacies of lumber prices are explained well in Lloyd’s article. An interesting observation is that during times of high lumber prices, the amount of money landowners received for selling their wood (termed stumpage in forestry parlance) barely nudged.\nThere are many factors that influence how much a landowners is paid for their wood. Local markets and competition in an area matter a lot, but so does the weather. Loggers are often faced with weather challenges when harvesting timber, often from it being too wet to operate on sensitive sites.\nAsk most loggers here in Maine and across the Northeast, and most will tell you the 2023-2024 winter has been mild. This presents a challenge for loggers as they rely on frozen ground conditions to harvest timber on many sites. A warm winter can limit the productivity of timber harvesters and haulers in northern states.\nI was curious to find out if there was any correlation between stumpage payments that landowners receive and how cold winters are. The idea being that if it’s been a warm winter, supply may be limited because loggers cannot harvest as much volume, and prices may be more favorable. But a cold winter might signal a timber harvesting community that is at capacity with plenty of supply.\nThe Maine Forest Service compiles annual data on stumpage prices by species and product classes, with data going back to 2000. The Maine Climate Office has data on a variety climate variables going back more than a century.\nFor this analysis, I used Maine stumpage data for all species sold as sawlogs and standardized amounts to 2021 dollars. I used the average minimum winter temperature as an indicator for how “cold” a winter was, using the average temperatures from December through February. In total, the data from 2000 through 2021 provided enough data points to see if trends exist.\nThe first finding, perhaps not unexpected, is that minimum temperatures have increased statewide across Maine since 2000. The correlation is 0.27 and is significant (i.e., a p-value of less than 0.05), indicating a slight positive relationship:\n\n\n\n\n\n\n\n\n\nThe second finding is the general decrease in stumpage prices in Maine since 2000. The correlation between Year and sawlog stumpage is weak (0.10) and not significant. These are price adjusted and in 2021 dollars, and include all 14 different sawlog species groups:\n\n\n\n\n\n\n\n\n\nThe final finding is the trends between average winter temperatures and stumpage prices trend negative for most species. I’m left head scratching with this one…:\n\n\n\n\n\n\n\n\n\nYou can see in the table below that only two of the species show significant trends. These species are spruce/fir and red pine.\n\n\n\n\nPearson correlation coefficients between stumpage price for sawlog species and minimum winter temperatures in Maine, 2000-2021.\n\n\nSawtimber species\nPearson correlation\nP-value\n\n\n\n\nSpruce & Fir\n-0.5815633\n0.0045263\n\n\nRed Pine\n-0.4400497\n0.0404173\n\n\nWhite Oak\n-0.4177549\n0.0530403\n\n\nSugar Maple\n-0.3987091\n0.0660591\n\n\nYellow Birch\n-0.3928108\n0.0705462\n\n\nHemlock\n-0.3362474\n0.1260096\n\n\nRed Oak\n-0.3258437\n0.1389088\n\n\nWhite Pine\n-0.2788256\n0.2089096\n\n\nRed/White Maple\n-0.2331360\n0.2964157\n\n\nCedar\n-0.1300809\n0.5639533\n\n\nBeech\n0.0699217\n0.7571720\n\n\nAsh\n-0.0446981\n0.8434261\n\n\nWhite Birch\n-0.0260954\n0.9082289\n\n\nAspen/Poplar\n-0.0108163\n0.9618973\n\n\n\n\n\n\n\n\nI anticipated finding a clearer story between winter temperatures and stumpage prices in a state that where winter weather determines so much about the health of an industry. But I’m no economist and markets sure are difficult to predict. Stumpage prices have decreased over the last 20 years in Maine, at least for the species and products examined here. This could be what statisticians call a “lurking” variable. But on the whole, it’s unwise to use a single variable to try to predict a market.\nDespite this, it seems like climate variables could be used more market-level analyses within the forest products industry. These data show some relationships between climate and prices. As statistician Edward Tufte once said, “Correlation does not imply causation, but it sure is a hint.”\n–\nBy Matt Russell. For more, subscribe to my monthly email newsletter to stay ahead on data and analytics trends in the forest products industry."
  },
  {
    "objectID": "post/2024-02-23-arbor-is-on-quarto/index.html",
    "href": "post/2024-02-23-arbor-is-on-quarto/index.html",
    "title": "Arbor Custom Analytics website now runs on Quarto",
    "section": "",
    "text": "I’ve recently moved the Arbor Custom Analytics website to Quarto, an open source publishing system. It’s built on top of Jupyter and Markdown, and it’s a great fit for ou work.\nMost of the content has ported over, but a few older blog posts might be missing. If you’re looking for something specific and don;t see it, feel free to reach out.\n–\nBy Matt Russell."
  },
  {
    "objectID": "post/2024-04-10-recapping-the-russell-s-2024-maple-syrup-season/index.html",
    "href": "post/2024-04-10-recapping-the-russell-s-2024-maple-syrup-season/index.html",
    "title": "Recapping the Russell 2024 maple syrup season",
    "section": "",
    "text": "A Maine maple tree at work in March.\n\n\n\n\n\n\nIf you live in the Northeastern United States, maple trees abound. And in the late winter and early spring, collecting sap from those trees to turn into syrup is a great way to enjoy the woods.\nFor the last two years my wife and I have become amateur syrup producers. We have several red and sugar maples trees on our property that we’ve tapped. Over four to six weeks, we hang buckets from trees, collect sap every day or two, then boil it to make maple syrup.\nBeginning in late February, we place taps into the maples and hang buckets to collect. Sap begins to flow soon thereafter. In 2023 we tapped 11 trees, and then added two more in 2024. This is enough trees to keep us as busy as we want to be over this time, and gives us enough syrup for ourselves and to share with family and neighbors. We generally collect enough sap that produces around a gallon of syrup.\nIf we collect over a gallon of sap on an individual tree one a single day, that’s good production. We see a large amount of sap produced shortly after tapping on many trees. Here’s the daily-ish amount of sap produced at each tree over the last two seasons:\n\n\n\n\n\n\n\n\n\nSome of our maples can provide over five gallons of sap over the course of a season. Average sap yield per tree was much greater in 2023 compared to 2024, likely because (1) we collected over a shorter time period (about four weeks in 2024 compared to six weeks in 2023) and (2) a warm winter likely impacted the sap run this year. Here are the average sap yields for each tree over the last two seasons:\n\n\n\n\n\n\n\n\n\nWe have a number of different sized maples that we tap, trying to select for trees with large crowns. The tree sizes generally range from 12 to 20 inches in diameter at breast height (DBH). I was curious to quantify the relationship between tree size and sap yield. Our results don’t indicate much of a trend between sap yield and DBH (but maybe there’s a slight correlation in 2024):\n\n\n\n\n\n\n\n\n\nAs for species, our sugar maple trees produce more sap on average than red maples. We also tap more sugar maples than reds:\n\n\n\n\nSap yields for Russell maple trees, 2023-2024.\n\n\nYear\nSpecies\nNum trees\nMean sap yield/tree (gallons)\nSD sap yield/tree (gallons)\n\n\n\n\n2023\nRed maple\n3\n4.90\n2.43\n\n\n2023\nSugar maple\n8\n5.91\n1.65\n\n\n2024\nRed maple\n3\n0.47\n0.10\n\n\n2024\nSugar maple\n10\n3.15\n1.82\n\n\n\n\n\n\n\n\nAll together, our sap yields in 2023 and 2024 were 62 and 33 gallons, respectively. Our boiling process tends to align with the “industry average” sap:syrup ratio of 40:1. We had quite a bit of syrup last year–this year not as much.\nOf course these data say nothing about the quality of syrup. That’s a more subjective analysis :)\n–\nBy Matt Russell. For more, subscribe to my monthly email newsletter to stay ahead on data and analytics trends in the forest products industry."
  },
  {
    "objectID": "post/how-many-trees-make-a-mass-timber-building/index.html",
    "href": "post/how-many-trees-make-a-mass-timber-building/index.html",
    "title": "How many trees make a mass timber building?",
    "section": "",
    "text": "The T3 in Minneapolis, Minnesota is a mass timber building built with wooden slabs, columns, and beams (photo: Ema Peter (http://www.emapeter.com/\n\n\n\nMass timber buildings are increasingly being built and have a number of environmental benefits. They are lighter than other common building materials like steel, are fire resistant, and their use reduces carbon dioxide emissions compared to other building materials. Buildings are even being constructed with beetle-killed wood to mitigate the impacts of forests disturbances on the carbon cycle. Because of these benefits, building codes are being changed or altered that support the construction of mass timber buildings.\nBut just how much “mass” of wood typically goes into making a mass timber building? Comparing how much wood is needed to build a typical residential home can help to provide insights into the volume of wood that is used in mass timber buildings.\nTraditional homes are “stick built”, typically made of narrow dimensional lumber like 2 x 4s. Each style of home varies depending on its size, number of doors and windows, and how the interior walls are oriented.\nA few estimates specifying the amount of wood required to build a home are online, but the style and size of the home are the primary drivers of how much wood is needed. From several online sources, a 2,000 square foot home requires approximately 16,000 board feet of wood in its construction. A board foot represents 144 cubic inches, or a piece of wood that is 12 inches x 12 inches x 1 inch.\nFor the same square footage, the amount of wood needed to make a mass timber building would be much greater than for a home. Instead of using dimensional lumber, mass timber buildings are typically made of materials like cross laminated timber (CLT). Finished panels are between two and ten feet wide and are up to 60 feet long and 20 inches thick. Here are five examples of mass timber building across the world:\n\n\n\n\nSome example recently-constructed mass timber buildings with their total square footage.\n\n\nBuilding\nLocation\nYearBuilt\nStories\nSquareFeet\n\n\n\n\nT3\nMinneapolis, MN\n2016\n7\n224000\n\n\nCarbon12\nPortland, OR\n2018\n8\n42000\n\n\nT3 West Midtown\nAtlanta, GA\n2018\n7\n205000\n\n\nBrock Commons Tallwood House\nVancouver, BC\n2017\n18\n162700\n\n\nThe Cube Building\nLondon, UK\n2015\n10\n72650\n\n\n\n\n\n\n\n\nHow can we use the board foot volume of wood used in a house to determine how many trees go into the construction of a mass timber building? Using the typical amounts of wood needed to build a home (i.e., 2,000 square foot home = 16,000 board feet of wood), a few key assumptions can be made to “scale up” to find out how much board feet is required for a mass timber building:\n\nFirst, mass timber buildings have no studs in their walls. We can assume that they have “continuous walls”. If typical stud widths in homes are 16 inches apart with two-inch pieces of wood, we can assume a continuous wall has eight times as much wood occupying a wall.\nSecond, mass timber walls (both interior and exterior) are thicker than home walls. If a mass timber wall is 20 inches think, five times as much wood would occupy a wall.\n\nTo summarize these assumptions, one square foot of a home requires 0.125 board feet of wood. One square foot of a mass timber building requires 5 board feet of wood.\nMass timber buildings typically use western conifers like Douglas-fir in construction material such as CLT. Growth and yield tables for a typical fully-stocked stand of Douglas-fir on an average quality site can produce 100,000 board feet of wood per acre at 100 years. This stand would typically contain 182 trees per acre that are seven inches in diameter or larger. This provides a useful reference point to quantify not only the volume of wood needed for a mass timber building, but also an approximation of how many trees and acres of forestland that wood may represent.\n\n\n\n\nEstimates of total board feet, acres harvested, and total number of trees needed to construct mass timber buildings.\n\n\nBuilding\nBoardFeet\nAcres\nTrees\n\n\n\n\nT3\n1120000\n11\n2002\n\n\nCarbon12\n210000\n2\n364\n\n\nT3 West Midtown\n1025000\n10\n1820\n\n\nBrock Commons Tallwood House\n813500\n8\n1456\n\n\nThe Cube Building\n363250\n4\n728\n\n\n\n\n\n\n\n\nAssuming the trees were harvested from fully-stocked stands, the total number of trees required to make a mass timber building ranges from as high as 2,002 trees for the T3 building in Minneapolis to 364 trees for the Carbon12 building in Portland. These numbers of trees equate to the volume of trees from 11 and 2 acres of forestland, respectively.\n\n\n\n\n\n\n\n\n\nIt’s difficult to find detailed information on how much wood is required to design mass timber buildings. There is no doubt that these values would differ substantially depending on the design of each building. The building’s square footage, interior and exterior design, and other features would influence the total volume of wood required.\nThese results suggest that some of the largest mass timber buildings in the world may have been constructed from the use of thousands of trees. The use of all of those trees has a long-lasting benefit to the health of forests by keeping all of that wood as a long-lived forest product.\nUpdated 27 September 2022. Special thanks to Lydia Link for pointing out calculation error in board foot value of buildings.\nBy Matt Russell. Leave a comment below or email Matt with any questions or comments."
  }
]